[
  {
    "objectID": "node_types.html",
    "href": "node_types.html",
    "title": "Node types",
    "section": "",
    "text": "Node types\nThe execution of most of the nodes consists of a single schedule compilation, a single measurement and a single post-processing. Although for most of the nodes this workflow suffices, there are exceptions while this workflow can become limiting in more advanced implementations.\nTo allow greater flexibilty in the node implementations the nodes are categorized:\n\nAccording to whether they compile once or multiple times:\n\nnode.type = simple_sweep: if it compiles once\nnode.type = parameterized_sweep: if it compiles multiple times\n\nAccording to whether the sweeping parameters are swept whithin the schedule or not:\n\nThere is only node.schedule_samplespace if the sweeping takes place whithin the schedule\nThere are both node.schedule_samplespace and node.external_samplespace if there are sweeping parameters outside of the scedule. For example the coupler_spectroscopy node sweeps the dc_current outside of the schedule:\n\n\n  class Coupler_Spectroscopy_Node(BaseNode):\n    measurement_obj = Two_Tones_Multidim\n    analysis_obj = CouplerSpectroscopyAnalysis\n\n    def __init__(self, name: str, all_qubits: list[str], couplers, **schedule_keywords):\n        super().__init__(name, all_qubits, **schedule_keywords)\n        self.couplers = couplers\n        self.redis_field = ['parking_current']\n        self.all_qubits = self.coupled_qubits\n\n        self.schedule_samplespace = {\n            'spec_frequencies': {\n                qubit: qubit_samples(qubit) for qubit in self.all_qubits\n            }\n        }\n\n        self.external_samplespace = {\n            'dc_currents': {\n                self.coupler: np.arange(-2.5e-3, 2.5e-3, 500e-6)\n            },\n        }\n\n    def pre_measurement_operation(self, reduced_ext_space):\n        iteration_dict = reduced_ext_space['dc_currents']\n\n        this_iteration_value = list(iteration_dict.values())[0]\n        print(f'{ this_iteration_value = }')\n        self.spi_dac.set_dac_current(self.dac, this_iteration_value)\nBy default every node every node is assigned a node.type attribute at the BaseNode class:\nself.type = simple_sweep\nThis attribute can be overwritten at the implementation of the class of each node. An example of a parameterized_sweep node type is Randomized_Benchmarking as each new iteration requires a the schedule to be recompiled with a different random seed.\nThe tergite_acl/scripts/node_supervisor.py is responcible to distinguish between each node variation.\n\nExamples of nodes requiring an external samplespace:\n\ncoupler_spectroscopy sweeps the dc_current which is set by the SPI rack not the cluster\nT1 sweeps a repetetion index to repeat the measurement many times\nrandomized_benchmarking sweeps different seeds. Although the seed is a schedule parameter, sweeping outside the schedule improves memory utilization."
  },
  {
    "objectID": "available_nodes.html",
    "href": "available_nodes.html",
    "title": "Available Nodes",
    "section": "",
    "text": "punchout\nresonator_spectroscopy\nresonator_spectroscopy_1\nresonator_spectroscopy_2\nro_frequency_two_state_optimization\nro_frequency_three_state_optimization\nro_amplitude_two_state_optimization\nro_amplitude_three_state_optimization\n\n\n\n\n\nqubit_01_spectroscopy\nqubit_01_spectroscopy_pulsed\nrabi_oscillations\nramsey_correction\nqubit_12_spectroscopy_pulsed\nqubit_12_spectroscopy_multidim\nrabi_oscillations_12\nramsey_correction_12\nadaptive_motzoi_parameter\nn_rabi_oscillations\nstate_discrimination\n\n\n\n\n\ncoupler_spectroscopy\ncoupler_resonator_spectroscopy\n\n\n\n\n\nT1\nT2\nT2_echo\nrandomized_benchmarking\nall_XY"
  },
  {
    "objectID": "available_nodes.html#readout-nodes",
    "href": "available_nodes.html#readout-nodes",
    "title": "Available Nodes",
    "section": "",
    "text": "punchout\nresonator_spectroscopy\nresonator_spectroscopy_1\nresonator_spectroscopy_2\nro_frequency_two_state_optimization\nro_frequency_three_state_optimization\nro_amplitude_two_state_optimization\nro_amplitude_three_state_optimization"
  },
  {
    "objectID": "available_nodes.html#qubit-control-nodes",
    "href": "available_nodes.html#qubit-control-nodes",
    "title": "Available Nodes",
    "section": "",
    "text": "qubit_01_spectroscopy\nqubit_01_spectroscopy_pulsed\nrabi_oscillations\nramsey_correction\nqubit_12_spectroscopy_pulsed\nqubit_12_spectroscopy_multidim\nrabi_oscillations_12\nramsey_correction_12\nadaptive_motzoi_parameter\nn_rabi_oscillations\nstate_discrimination"
  },
  {
    "objectID": "available_nodes.html#coupler-nodes",
    "href": "available_nodes.html#coupler-nodes",
    "title": "Available Nodes",
    "section": "",
    "text": "coupler_spectroscopy\ncoupler_resonator_spectroscopy"
  },
  {
    "objectID": "available_nodes.html#characterization-nodes",
    "href": "available_nodes.html#characterization-nodes",
    "title": "Available Nodes",
    "section": "",
    "text": "T1\nT2\nT2_echo\nrandomized_benchmarking\nall_XY"
  },
  {
    "objectID": "operation.html",
    "href": "operation.html",
    "title": "Tergite Automatic Calibration",
    "section": "",
    "text": "The package ships with a command line interface to solve some common tasks that appear quite often.\nIn the following there are a number of useful commands, but if you want to find out all commands use: acli --help\nTo delete all redis entries: acli node reset -a\nTo reset a particular node: acli node reset -n &lt;nodename&gt;\nFor example to reset the node rabi_oscillations run the command:\nacli node reset -n rabi_oscillations\nTo start a new calibration sequence according to the configuration files:\npython tergite_acl/scripts/calibration_supervisor.py\nor\nacli calibration start"
  },
  {
    "objectID": "operation.html#operation",
    "href": "operation.html#operation",
    "title": "Tergite Automatic Calibration",
    "section": "",
    "text": "The package ships with a command line interface to solve some common tasks that appear quite often.\nIn the following there are a number of useful commands, but if you want to find out all commands use: acli --help\nTo delete all redis entries: acli node reset -a\nTo reset a particular node: acli node reset -n &lt;nodename&gt;\nFor example to reset the node rabi_oscillations run the command:\nacli node reset -n rabi_oscillations\nTo start a new calibration sequence according to the configuration files:\npython tergite_acl/scripts/calibration_supervisor.py\nor\nacli calibration start"
  },
  {
    "objectID": "operation.html#structure",
    "href": "operation.html#structure",
    "title": "Tergite Automatic Calibration",
    "section": "Structure",
    "text": "Structure\nFor each calibration node: compilation -&gt; execution -&gt; post-processing -&gt; redis updating"
  },
  {
    "objectID": "operation.html#data-browsing",
    "href": "operation.html#data-browsing",
    "title": "Tergite Automatic Calibration",
    "section": "Data browsing",
    "text": "Data browsing\nDatasets are stored in data_directory Can be browsed with the dataset browser (coming soon)"
  },
  {
    "objectID": "operation.html#development",
    "href": "operation.html#development",
    "title": "Tergite Automatic Calibration",
    "section": "Development",
    "text": "Development\nWhen submitting contributions, please prepend your commit messages with: fix: for bug fixes feat: for introducing a new feature (e.g. a new measurement node or a new analysis class) chore: for refractoring changes or any change that doesn’t affect the functionality of the code docs: for changes in the README, docstrings etc test: or dev: for testing or development changes (e.g. profiling scripts)"
  },
  {
    "objectID": "getting_started.html",
    "href": "getting_started.html",
    "title": "Getting started",
    "section": "",
    "text": "This project contains an orchistration manager, a collection of callibration schedules and a collection of post-processing & analysis routines. It is tailored for the tune-up of the 25 qubits QPU at Chalmers, QTL. This repository utilizes redis for on memory data storage. As redis operates only on Linux systems, this repo can only work\n\neither on Linux distributions\nor WSL (Windows Subsystem for Linux) environments, installed on a Windows system.\n\nTo install WSL, it is required Windows 10 of version at least 1903.\n\n\n\n\n\ngit clone git@github.com:chalmersnextlabs-quantum/tergite-autocalibration.git\n\n\n\nhttps://redis.io/docs/getting-started/installation/install-redis-on-linux/\n\n\n\nredis-server\n\n\n\nIf for example you want to name your environment tac, you create it as\nconda create --name tac python=3.9\n\n\n\nconda activate tac\n\n\n\nsource activate tac\n\n\n\ncd tergite-autocalibration-lite/\nFrom now on, it is assumed that all commands are executed from the project root directory.\n\n\n\npip install -e .\nHere . is the root directory (i.e. the directory that contains the pyproject.toml file)\n\n\n\n\nBefore the first run of the callibration suite a number of configuration files need to be set up. These files describe our initial knowledge on the system as well as the connectivity with the measurement hardware.\n\n\nAll the settings and paths are contained in the .env file. Since this file contains user specific settings, it is git-ignored, it must be created by the user. For convinience a template file dot-env-template.txt already exists.\nFirst copy the template file to the .env (this creates the .env file if it doesn’t exist):\ncp dot-env-template .env\nThen edit the newly created .env file according to your system. For example here’s how the user lab-user may complete the .env file:\n# Copy this file to a .env file in the tergite-autocalibration folder on the root level.\n# The .env file is a simple list of keys and values. It is also known as the INI file on MS Windows.\n# Fill in the necessary values.\n\n# DEFAULT_PREFIX is added to logfiles, redis entries and in the data directory\n# Default: cal\nDEFAULT_PREFIX=calibration\n\n# Directory settings\n# ROOT_DIR defines the top-level folder of the tergite-autocalibration-lite folder\n# Default: two levels up from the config\nROOT_DIR='/home/lab-user/github/tergite-acl/'\n\n# DATA_DIR defines where plots are stored\nDATA_DIR='/home/lab-user/github/tergite-acl/data_directory/'\n\n# CONFIG_DIR defines where the configuration is stored\nCONFIG_DIR='/home/lab-user/github/tergite-acl/config_dir/'\n\n\n# Configuration settings\n# It is assumed that all these paths are relative to CONFIG_DIR\n# HARDWARE_CONFIG is what Q-BLOX needs to compile schedules on the hardware\n# It should be a file in json format, there is no default file\nHARDWARE_CONFIG='HARDWARE_CONFIGURATION.json'\n\n# DEVICE_CONFIG contains the initial values for the device configuration\nDEVICE_CONFIG='device_config.toml'\n\n# Configuration variables\n# CLUSTER_IP is the IP address of the instrument cluster to connect with\nCLUSTER_IP='162.0.2.162'\n# SPI_SERIAL_PORT is the port on which the spi rack is connected\nSPI_SERIAL_PORT='/dev/ttyACM0'\n\n# APP_SETTINGS reflect which environment the calibration is to run in.\n# Options\n#  - development\n#  - production\n#  - staging\n#  - test\n# Default: production\n# TODO: currently we are only using the calibration in the development mode\nRUN_MODE=development\n\n# REDIS_PORT is the port which to use when connecting to redis\nREDIS_PORT=6379\n# REDIS_CONNECTION will be automatically created in settings.py\n\n# PLOTTING is a boolean to indicate whether plots should be shown or whether plots should be silent in the background\n# Default: True\nPLOTTING=True"
  },
  {
    "objectID": "getting_started.html#installation",
    "href": "getting_started.html#installation",
    "title": "Getting started",
    "section": "",
    "text": "This project contains an orchistration manager, a collection of callibration schedules and a collection of post-processing & analysis routines. It is tailored for the tune-up of the 25 qubits QPU at Chalmers, QTL. This repository utilizes redis for on memory data storage. As redis operates only on Linux systems, this repo can only work\n\neither on Linux distributions\nor WSL (Windows Subsystem for Linux) environments, installed on a Windows system.\n\nTo install WSL, it is required Windows 10 of version at least 1903."
  },
  {
    "objectID": "getting_started.html#repository-installation",
    "href": "getting_started.html#repository-installation",
    "title": "Getting started",
    "section": "",
    "text": "git clone git@github.com:chalmersnextlabs-quantum/tergite-autocalibration.git\n\n\n\nhttps://redis.io/docs/getting-started/installation/install-redis-on-linux/\n\n\n\nredis-server\n\n\n\nIf for example you want to name your environment tac, you create it as\nconda create --name tac python=3.9\n\n\n\nconda activate tac\n\n\n\nsource activate tac\n\n\n\ncd tergite-autocalibration-lite/\nFrom now on, it is assumed that all commands are executed from the project root directory.\n\n\n\npip install -e .\nHere . is the root directory (i.e. the directory that contains the pyproject.toml file)"
  },
  {
    "objectID": "getting_started.html#setting-up-system-configuration-files",
    "href": "getting_started.html#setting-up-system-configuration-files",
    "title": "Getting started",
    "section": "",
    "text": "Before the first run of the callibration suite a number of configuration files need to be set up. These files describe our initial knowledge on the system as well as the connectivity with the measurement hardware.\n\n\nAll the settings and paths are contained in the .env file. Since this file contains user specific settings, it is git-ignored, it must be created by the user. For convinience a template file dot-env-template.txt already exists.\nFirst copy the template file to the .env (this creates the .env file if it doesn’t exist):\ncp dot-env-template .env\nThen edit the newly created .env file according to your system. For example here’s how the user lab-user may complete the .env file:\n# Copy this file to a .env file in the tergite-autocalibration folder on the root level.\n# The .env file is a simple list of keys and values. It is also known as the INI file on MS Windows.\n# Fill in the necessary values.\n\n# DEFAULT_PREFIX is added to logfiles, redis entries and in the data directory\n# Default: cal\nDEFAULT_PREFIX=calibration\n\n# Directory settings\n# ROOT_DIR defines the top-level folder of the tergite-autocalibration-lite folder\n# Default: two levels up from the config\nROOT_DIR='/home/lab-user/github/tergite-acl/'\n\n# DATA_DIR defines where plots are stored\nDATA_DIR='/home/lab-user/github/tergite-acl/data_directory/'\n\n# CONFIG_DIR defines where the configuration is stored\nCONFIG_DIR='/home/lab-user/github/tergite-acl/config_dir/'\n\n\n# Configuration settings\n# It is assumed that all these paths are relative to CONFIG_DIR\n# HARDWARE_CONFIG is what Q-BLOX needs to compile schedules on the hardware\n# It should be a file in json format, there is no default file\nHARDWARE_CONFIG='HARDWARE_CONFIGURATION.json'\n\n# DEVICE_CONFIG contains the initial values for the device configuration\nDEVICE_CONFIG='device_config.toml'\n\n# Configuration variables\n# CLUSTER_IP is the IP address of the instrument cluster to connect with\nCLUSTER_IP='162.0.2.162'\n# SPI_SERIAL_PORT is the port on which the spi rack is connected\nSPI_SERIAL_PORT='/dev/ttyACM0'\n\n# APP_SETTINGS reflect which environment the calibration is to run in.\n# Options\n#  - development\n#  - production\n#  - staging\n#  - test\n# Default: production\n# TODO: currently we are only using the calibration in the development mode\nRUN_MODE=development\n\n# REDIS_PORT is the port which to use when connecting to redis\nREDIS_PORT=6379\n# REDIS_CONNECTION will be automatically created in settings.py\n\n# PLOTTING is a boolean to indicate whether plots should be shown or whether plots should be silent in the background\n# Default: True\nPLOTTING=True"
  },
  {
    "objectID": "configuration_files.html",
    "href": "configuration_files.html",
    "title": "Configuration Files",
    "section": "",
    "text": "Here the connection is made between the Qblox cluster physical ports and clocks to the qubits and couplers of the QPU.\nThe file must be placed at the directory tergite-acl/config_dir/\nGiven a .csv file after a mixer calibration the function ... can create instantly the corresponding JSON file.\n\n\n\nHere we set reasonable initial parameters\n\n\n\nHere we input the initial values for the resonator and qubits frequencies, assumed that they have been acquired through VNA measurements, design targets or previous manual measurements. /tergite_acl/config/VNA_values.py"
  },
  {
    "objectID": "configuration_files.html#hardware-configuration-file-json",
    "href": "configuration_files.html#hardware-configuration-file-json",
    "title": "Configuration Files",
    "section": "",
    "text": "Here the connection is made between the Qblox cluster physical ports and clocks to the qubits and couplers of the QPU.\nThe file must be placed at the directory tergite-acl/config_dir/\nGiven a .csv file after a mixer calibration the function ... can create instantly the corresponding JSON file."
  },
  {
    "objectID": "configuration_files.html#device-configuration-file-toml",
    "href": "configuration_files.html#device-configuration-file-toml",
    "title": "Configuration Files",
    "section": "",
    "text": "Here we set reasonable initial parameters"
  },
  {
    "objectID": "configuration_files.html#vna-resonator-qubit-frequencies",
    "href": "configuration_files.html#vna-resonator-qubit-frequencies",
    "title": "Configuration Files",
    "section": "",
    "text": "Here we input the initial values for the resonator and qubits frequencies, assumed that they have been acquired through VNA measurements, design targets or previous manual measurements. /tergite_acl/config/VNA_values.py"
  },
  {
    "objectID": "new_node_creation.html",
    "href": "new_node_creation.html",
    "title": "To create a new node:",
    "section": "",
    "text": "in the file tergite_autocalibration/lib/node_factory.py expand the dictionary self.node_implementations with a new entry: The key should be a string of the node name and the value should be the object that contains the implementation details. This object should be imported from either tergite_autocalibration/lib/nodes/qubit_control_nodes.py, tergite_autocalibration/lib/nodes/coupler_nodes.py, tergite_autocalibration/lib/nodes/readout_nodes.py or tergite_autocalibration/lib/nodes/characterization_nodes.py\nIn the file tergite_autocalibration/lib/nodes/graph.py in the list graph_dependencies insert the edges that describe the position of the new node in the Directed Acyclic Graph. There are two entries required (or one entry if the new node is the last on its path):\n\n\n('previous_node','new_node')\n('new_node', 'next_node')\n\n\nIn the tergite_autocalibration/config/device_config.toml set the quantity of interest at nan value\n\n\n\nEach node implementation object should contain a reference to the measurement object, the analysis object, the list of redis fields that the analysis updates and the samplespace of the measurement. For example on the Rabi Rabi Oscillations Node:\nclass Rabi_Oscillations_Node(BaseNode):\n    measurement_obj = Rabi_Oscillations\n    analysis_obj = RabiAnalysis\n\n    def __init__(self, name: str, all_qubits: list[str], **node_dictionary):\n        super().__init__(name, all_qubits, **node_dictionary)\n        self.redis_field = ['rxy:amp180']\n        self.schedule_samplespace = {\n            'mw_amplitudes': {\n                qubit: np.linspace(0.002, 0.80, 101) for qubit in self.all_qubits\n            }\n        }\n\n\nThe measurement_obj is imported from tergite_autocalibration/lib/calibration_schedules/ and contains the class that generates the appropriate measurement schedule. To initialize we require a dicttionary of the extended transmons:\ntransmons: dict[str, ExtendedTransmon]\nIt must contain a method called schedule_function that expects the node.samplespace as input and returns the complete schedule.\n\n\n\nThe analysis_obj is imported from tergite_autocalibration/lib/analysis/ and contains the class that perform the analysis for a single qubit. It must contain a run_fitting method and a plotter method\n\n\n\nNodes are divided in two distict categories:\n\nsimple_sweep: where the Quantify Schedule is compiled only once\nparameterized_sweep: where the node requires multiple iterations and each iteration requires a new recompilation.\n\nFurthermore each node can expect two types of samplespaces:\n\nschedule_samplespace: parameter values to be input to the schedule function\nexternal_samplespace: parameter values for quantities that are not set during a schedule\n\n\n\n\nPlease create a new node in a separate folder, so that is is clearer what the new node is meant to do Add an empty init.py file to the folder, this is needed to mark the folder as part of the packege and allow imports from these folders\nTo keep the code clean, please create sub-folders following this scheme:\n\ntests: create unit tests in here, more on tests in href: unit_tests\nutils: any utility class, such as enum, errors and similar classes should be placed here\n\n\n\n\nPlease add your node to the list of available nodes in this Documentation.\nAdd ny relevant information on how to use your node, dependencies and reference to publication as needed for allowing other to use the code you developed.\nDetails on the implementation on the Node types section."
  },
  {
    "objectID": "new_node_creation.html#node-implementation-object",
    "href": "new_node_creation.html#node-implementation-object",
    "title": "To create a new node:",
    "section": "",
    "text": "Each node implementation object should contain a reference to the measurement object, the analysis object, the list of redis fields that the analysis updates and the samplespace of the measurement. For example on the Rabi Rabi Oscillations Node:\nclass Rabi_Oscillations_Node(BaseNode):\n    measurement_obj = Rabi_Oscillations\n    analysis_obj = RabiAnalysis\n\n    def __init__(self, name: str, all_qubits: list[str], **node_dictionary):\n        super().__init__(name, all_qubits, **node_dictionary)\n        self.redis_field = ['rxy:amp180']\n        self.schedule_samplespace = {\n            'mw_amplitudes': {\n                qubit: np.linspace(0.002, 0.80, 101) for qubit in self.all_qubits\n            }\n        }\n\n\nThe measurement_obj is imported from tergite_autocalibration/lib/calibration_schedules/ and contains the class that generates the appropriate measurement schedule. To initialize we require a dicttionary of the extended transmons:\ntransmons: dict[str, ExtendedTransmon]\nIt must contain a method called schedule_function that expects the node.samplespace as input and returns the complete schedule.\n\n\n\nThe analysis_obj is imported from tergite_autocalibration/lib/analysis/ and contains the class that perform the analysis for a single qubit. It must contain a run_fitting method and a plotter method\n\n\n\nNodes are divided in two distict categories:\n\nsimple_sweep: where the Quantify Schedule is compiled only once\nparameterized_sweep: where the node requires multiple iterations and each iteration requires a new recompilation.\n\nFurthermore each node can expect two types of samplespaces:\n\nschedule_samplespace: parameter values to be input to the schedule function\nexternal_samplespace: parameter values for quantities that are not set during a schedule\n\n\n\n\nPlease create a new node in a separate folder, so that is is clearer what the new node is meant to do Add an empty init.py file to the folder, this is needed to mark the folder as part of the packege and allow imports from these folders\nTo keep the code clean, please create sub-folders following this scheme:\n\ntests: create unit tests in here, more on tests in href: unit_tests\nutils: any utility class, such as enum, errors and similar classes should be placed here\n\n\n\n\nPlease add your node to the list of available nodes in this Documentation.\nAdd ny relevant information on how to use your node, dependencies and reference to publication as needed for allowing other to use the code you developed.\nDetails on the implementation on the Node types section."
  },
  {
    "objectID": "unit_tests.html",
    "href": "unit_tests.html",
    "title": "Unit tests for nodes",
    "section": "",
    "text": "The test should be created in a sub-folder of the node called tests.\nPlease organise the file using these sub-folders:\n\ndata: place here any data file that is needed to create test cases, while it is possible to mock the data. Feel free to add a text file explaing how the data was produced. Mocking data is also possible but it may be not practical for complex datasets.\nresults: create this folder to store your results, i.e. files that would be created by your analysis such as the plots. It should be empty, do not commit your results. To assure this is the case, add a file name .gitinore in the folder with this code:\n\n# Ignore everything in this directory\n*\n\n# But do not ignore this file\n!.gitignore     \n\n\n\nA good starting point, especially starting from scratch, it is to test for the class type, then that the inputs have the right formats and then move to some simple operation or trivial case. Build more complex cases from the simpler ones and exploits your tests to refector the code as needed. Try to test as many reasonable cases as possible, both successfull and not. Remeber to test for exceptions. We also suggest to develop using test drive development tecniques that will ensure a high test coverage and a good code structure. Yes, do not forget to keep refactoring to improve the code.\nYou can find some samples below taken from the cz_parametrisation node, where all objects are tested\nCurrently there is no way to differentiate from tests that require a QPU (i.e. measurments) and those that do not (i.e. analyses). Since the latter are simpler to write, start with those that, in general, are more likely to benefit from unit tests as there is much more logic in the analysis than in the measurment.\nIf you need to test some complex scenario, such as those involving sweep, it is probably easier to start writing code and tests from the lowest level and then compose those objects to hendle the more complex scenario considered.\n\n\n\n\n\ndef test_canCreateCorrectType():\n    c = CZ_Parametrisation_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q15\"])\n    assert isinstance(c, CZ_Parametrisation_Fix_Duration_Node)\n    assert isinstance(c, ParametrizedSweepNode)\nThe suggested very first test is to istantiate the class and make sure it has the correct type(s) following any inheritance.\n\n\n\ndef test_CanGetQubitsFromCouplers():\n    c = CZ_Parametrisation_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q15\"])\n    assert c.all_qubits == [\"q14\", \"q15\"]\n    assert c.couplers == ['q14_q15']\nMake sure all inputs and their manipulations are correctly initialised in the constructor, in this case the qubits are taken from the coupler pair\n\n\n\ndef test_ValidationReturnErrorWithSameQubitCoupler():\n    with pytest.raises(ValueError):\n       CZ_Parametrisation_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q14\"])\nInputs can be incorrect and should always be tested to avoid unexpected behaviour down the line which can be difficult to trace back to the origin. There are infinite number of possible errors, so it is impossible to cover them all, but at least the obvious ones should be considered. In this case a typical typing error with the couple have the same qubit twice.\n\n\n\n@pytest.fixture(autouse=True)\ndef setup_good_data():\n    os.environ[\"DATA_DIR\"] = str(Path(__file__).parent / \"results\")\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    return d14, d15, freqs, amps\nThis is an example of how to load data for testing the analysis of a node, please note that the specifics may change as we are about to change the dataset format at the time of writing this; however, the principle will be the same.\nThis data can then be used in multiple tests, for example:\ndef test_canGetMaxFromQ1(setup_good_data):\n    d14, d15, freqs, amps = setup_good_data\n    c = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    result = c.run_fitting()\n    indexBestFreq = np.where(freqs == result[0])[0]\n    indexBestAmp = np.where(amps == result[1])[0]\n    assert indexBestFreq[0] == 9\n    assert indexBestAmp[0] == 13\n\n\ndef test_canGetMinFromQ2(setup_good_data):\n    d14, d15, freqs, amps = setup_good_data\n    c = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    result = c.run_fitting()\n    indexBestFreq = np.where(freqs == result[0])[0]\n    indexBestAmp = np.where(amps == result[1])[0]\n    assert indexBestFreq[0] == 10\n    assert indexBestAmp[0] == 12\nThese two tests make sure that the return values are correct for the two qubits that are connected by the coupler.\n\n\n\ndef test_canPlotBad(setup_bad_data):\n    matplotlib.use(\"Agg\")\n    d14, d15, freqs, amps = setup_bad_data\n    c14 = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    result = c14.run_fitting()\n\n    figure_path = os.environ[\"DATA_DIR\"] + \"/Frequency_Amplitude_bad_q14.png\"\n    # Remove the file if it already exists\n    if os.path.exists(figure_path):\n        os.remove(figure_path)\n\n    fig, ax = plt.subplots(figsize=(15, 7), num=1)\n    plt.Axes\n    c14.plotter(ax)\n    fig.savefig(figure_path)\n    plt.close()\n\n    assert os.path.exists(figure_path)\n    from PIL import Image\n\n    with Image.open(figure_path) as img:\n        assert img.format == \"PNG\", \"File should be a PNG image\"\n\n    c15 = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    result = c15.run_fitting()\n\n    figure_path = os.environ[\"DATA_DIR\"] + \"/Frequency_Amplitude_bad_q15.png\"\n    # Remove the file if it already exists\n    if os.path.exists(figure_path):\n        os.remove(figure_path)\n\n    fig, ax = plt.subplots(figsize=(15, 7), num=1)\n    plt.Axes\n    c15.plotter(ax)\n    fig.savefig(figure_path)\n    plt.close()\n\n    assert os.path.exists(figure_path)\n    from PIL import Image\n\n    with Image.open(figure_path) as img:\n        assert img.format == \"PNG\", \"File should be a PNG image\"\nThis is an example on how to save files in the “results” sub-folder within the “tests” folder. The code make sure the expected file exists and has the correct format. The created files can be inspected by the developer. A possible extension would be to upload the images in the data folder and check the those produced by the test are identical.\n\n\n\n@pytest.fixture(autouse=True)\ndef setup_bad_data():\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_bad_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    q15Res = q15Ana.run_fitting()\n    return q14Res, q15Res\n\n\ndef test_combineBadResultsReturnNoValidPoint(setup_bad_data):\n    q14Res, q15Res = setup_bad_data\n    c = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n    r = c.are_frequencies_compatible()\n    assert r == False\n    r = c.are_amplitudes_compatible()\n    assert r == False\n    r = c.are_two_qubits_compatible()\n    assert r == False\nIn this example, the data produced is loaded from a file that has data that is not a good working point. Note that there two analyses are run in the setup; the combination is run in the test, so that, if needed in other tests, the data could be modified for specific cases. This is a failure test, making sure that bad inputs are not recognised as good working points.\n\n\n\ndef setup_data():\n    # It should be a single dataset, but we do not have one yet, so we loop over existing files\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    q15Res = q15Ana.run_fitting()\n    c1 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_bad_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs_bad = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps_bad = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(\n        d14, freqs_bad, amps_bad\n    )\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(\n        d15, freqs_bad, amps_bad\n    )\n    q15Res = q15Ana.run_fitting()\n    c2 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    dataset_path = (\n        Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp_2.hdf5\"\n    )\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs_2 = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps_2 = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs_2, amps_2)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs_2, amps_2)\n    q15Res = q15Ana.run_fitting()\n    c3 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    list_of_results = [(c1, 0.1), (c2, 0.2), (c3, 0.3)]\n    return list_of_results, freqs, amps, freqs_2, amps_2\nIn this case, three points are loaded and the analysis is run on each of them. The results are returned for each point and can be used in different tests, for example by removing elements in the list_results array."
  },
  {
    "objectID": "unit_tests.html#folder-structure",
    "href": "unit_tests.html#folder-structure",
    "title": "Unit tests for nodes",
    "section": "",
    "text": "The test should be created in a sub-folder of the node called tests.\nPlease organise the file using these sub-folders:\n\ndata: place here any data file that is needed to create test cases, while it is possible to mock the data. Feel free to add a text file explaing how the data was produced. Mocking data is also possible but it may be not practical for complex datasets.\nresults: create this folder to store your results, i.e. files that would be created by your analysis such as the plots. It should be empty, do not commit your results. To assure this is the case, add a file name .gitinore in the folder with this code:\n\n# Ignore everything in this directory\n*\n\n# But do not ignore this file\n!.gitignore"
  },
  {
    "objectID": "unit_tests.html#general-guideline-and-information",
    "href": "unit_tests.html#general-guideline-and-information",
    "title": "Unit tests for nodes",
    "section": "",
    "text": "A good starting point, especially starting from scratch, it is to test for the class type, then that the inputs have the right formats and then move to some simple operation or trivial case. Build more complex cases from the simpler ones and exploits your tests to refector the code as needed. Try to test as many reasonable cases as possible, both successfull and not. Remeber to test for exceptions. We also suggest to develop using test drive development tecniques that will ensure a high test coverage and a good code structure. Yes, do not forget to keep refactoring to improve the code.\nYou can find some samples below taken from the cz_parametrisation node, where all objects are tested\nCurrently there is no way to differentiate from tests that require a QPU (i.e. measurments) and those that do not (i.e. analyses). Since the latter are simpler to write, start with those that, in general, are more likely to benefit from unit tests as there is much more logic in the analysis than in the measurment.\nIf you need to test some complex scenario, such as those involving sweep, it is probably easier to start writing code and tests from the lowest level and then compose those objects to hendle the more complex scenario considered."
  },
  {
    "objectID": "unit_tests.html#example-tests",
    "href": "unit_tests.html#example-tests",
    "title": "Unit tests for nodes",
    "section": "",
    "text": "def test_canCreateCorrectType():\n    c = CZ_Parametrisation_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q15\"])\n    assert isinstance(c, CZ_Parametrisation_Fix_Duration_Node)\n    assert isinstance(c, ParametrizedSweepNode)\nThe suggested very first test is to istantiate the class and make sure it has the correct type(s) following any inheritance.\n\n\n\ndef test_CanGetQubitsFromCouplers():\n    c = CZ_Parametrisation_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q15\"])\n    assert c.all_qubits == [\"q14\", \"q15\"]\n    assert c.couplers == ['q14_q15']\nMake sure all inputs and their manipulations are correctly initialised in the constructor, in this case the qubits are taken from the coupler pair\n\n\n\ndef test_ValidationReturnErrorWithSameQubitCoupler():\n    with pytest.raises(ValueError):\n       CZ_Parametrisation_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q14\"])\nInputs can be incorrect and should always be tested to avoid unexpected behaviour down the line which can be difficult to trace back to the origin. There are infinite number of possible errors, so it is impossible to cover them all, but at least the obvious ones should be considered. In this case a typical typing error with the couple have the same qubit twice.\n\n\n\n@pytest.fixture(autouse=True)\ndef setup_good_data():\n    os.environ[\"DATA_DIR\"] = str(Path(__file__).parent / \"results\")\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    return d14, d15, freqs, amps\nThis is an example of how to load data for testing the analysis of a node, please note that the specifics may change as we are about to change the dataset format at the time of writing this; however, the principle will be the same.\nThis data can then be used in multiple tests, for example:\ndef test_canGetMaxFromQ1(setup_good_data):\n    d14, d15, freqs, amps = setup_good_data\n    c = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    result = c.run_fitting()\n    indexBestFreq = np.where(freqs == result[0])[0]\n    indexBestAmp = np.where(amps == result[1])[0]\n    assert indexBestFreq[0] == 9\n    assert indexBestAmp[0] == 13\n\n\ndef test_canGetMinFromQ2(setup_good_data):\n    d14, d15, freqs, amps = setup_good_data\n    c = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    result = c.run_fitting()\n    indexBestFreq = np.where(freqs == result[0])[0]\n    indexBestAmp = np.where(amps == result[1])[0]\n    assert indexBestFreq[0] == 10\n    assert indexBestAmp[0] == 12\nThese two tests make sure that the return values are correct for the two qubits that are connected by the coupler.\n\n\n\ndef test_canPlotBad(setup_bad_data):\n    matplotlib.use(\"Agg\")\n    d14, d15, freqs, amps = setup_bad_data\n    c14 = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    result = c14.run_fitting()\n\n    figure_path = os.environ[\"DATA_DIR\"] + \"/Frequency_Amplitude_bad_q14.png\"\n    # Remove the file if it already exists\n    if os.path.exists(figure_path):\n        os.remove(figure_path)\n\n    fig, ax = plt.subplots(figsize=(15, 7), num=1)\n    plt.Axes\n    c14.plotter(ax)\n    fig.savefig(figure_path)\n    plt.close()\n\n    assert os.path.exists(figure_path)\n    from PIL import Image\n\n    with Image.open(figure_path) as img:\n        assert img.format == \"PNG\", \"File should be a PNG image\"\n\n    c15 = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    result = c15.run_fitting()\n\n    figure_path = os.environ[\"DATA_DIR\"] + \"/Frequency_Amplitude_bad_q15.png\"\n    # Remove the file if it already exists\n    if os.path.exists(figure_path):\n        os.remove(figure_path)\n\n    fig, ax = plt.subplots(figsize=(15, 7), num=1)\n    plt.Axes\n    c15.plotter(ax)\n    fig.savefig(figure_path)\n    plt.close()\n\n    assert os.path.exists(figure_path)\n    from PIL import Image\n\n    with Image.open(figure_path) as img:\n        assert img.format == \"PNG\", \"File should be a PNG image\"\nThis is an example on how to save files in the “results” sub-folder within the “tests” folder. The code make sure the expected file exists and has the correct format. The created files can be inspected by the developer. A possible extension would be to upload the images in the data folder and check the those produced by the test are identical.\n\n\n\n@pytest.fixture(autouse=True)\ndef setup_bad_data():\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_bad_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    q15Res = q15Ana.run_fitting()\n    return q14Res, q15Res\n\n\ndef test_combineBadResultsReturnNoValidPoint(setup_bad_data):\n    q14Res, q15Res = setup_bad_data\n    c = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n    r = c.are_frequencies_compatible()\n    assert r == False\n    r = c.are_amplitudes_compatible()\n    assert r == False\n    r = c.are_two_qubits_compatible()\n    assert r == False\nIn this example, the data produced is loaded from a file that has data that is not a good working point. Note that there two analyses are run in the setup; the combination is run in the test, so that, if needed in other tests, the data could be modified for specific cases. This is a failure test, making sure that bad inputs are not recognised as good working points.\n\n\n\ndef setup_data():\n    # It should be a single dataset, but we do not have one yet, so we loop over existing files\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    q15Res = q15Ana.run_fitting()\n    c1 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_bad_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs_bad = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps_bad = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(\n        d14, freqs_bad, amps_bad\n    )\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(\n        d15, freqs_bad, amps_bad\n    )\n    q15Res = q15Ana.run_fitting()\n    c2 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    dataset_path = (\n        Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp_2.hdf5\"\n    )\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs_2 = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps_2 = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs_2, amps_2)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs_2, amps_2)\n    q15Res = q15Ana.run_fitting()\n    c3 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    list_of_results = [(c1, 0.1), (c2, 0.2), (c3, 0.3)]\n    return list_of_results, freqs, amps, freqs_2, amps_2\nIn this case, three points are loaded and the analysis is run on each of them. The results are returned for each point and can be used in different tests, for example by removing elements in the list_results array."
  }
]