[
  {
    "objectID": "unit_tests.html",
    "href": "unit_tests.html",
    "title": "Unit tests for nodes",
    "section": "",
    "text": "The test should be created in a sub-folder of the node called tests.\nPlease organise the file using these sub-folders:\n\ndata: place here any data file that is needed to create test cases, while it is possible to mock the data. Feel free to add a text file explaing how the data was produced. Mocking data is also possible but it may be not practical for complex datasets.\nresults: create this folder to store your results, i.e. files that would be created by your analysis such as the plots. It should be empty, do not commit your results. To assure this is the case, add a file name .gitinore in the folder with this code:\n\n# Ignore everything in this directory\n*\n\n# But do not ignore this file\n!.gitignore     \n\n\n\nA good starting point, especially starting from scratch, it is to test for the class type, then that the inputs have the right formats and then move to some simple operation or trivial case. Build more complex cases from the simpler ones and exploits your tests to refector the code as needed. Try to test as many reasonable cases as possible, both successfull and not. Remeber to test for exceptions. We also suggest to develop using test drive development tecniques that will ensure a high test coverage and a good code structure. Yes, do not forget to keep refactoring to improve the code.\nYou can find some samples below taken from the cz_parametrisation node, where all objects are tested\nCurrently there is no way to differentiate from tests that require a QPU (i.e. measurments) and those that do not (i.e. analyses). Since the latter are simpler to write, start with those that, in general, are more likely to benefit from unit tests as there is much more logic in the analysis than in the measurment.\nIf you need to test some complex scenario, such as those involving sweep, it is probably easier to start writing code and tests from the lowest level and then compose those objects to hendle the more complex scenario considered.\n\n\n\n\n\ndef test_canCreateCorrectType():\n    c = CZ_Parametrisation_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q15\"])\n    assert isinstance(c, CZ_Parametrisation_Fix_Duration_Node)\n    assert isinstance(c, ParametrizedSweepNode)\nThe suggested very first test is to istantiate the class and make sure it has the correct type(s) following any inheritance.\n\n\n\ndef test_CanGetQubitsFromCouplers():\n    c = CZ_Parametrisation_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q15\"])\n    assert c.all_qubits == [\"q14\", \"q15\"]\n    assert c.couplers == ['q14_q15']\nMake sure all inputs and their manipulations are correctly initialised in the constructor, in this case the qubits are taken from the coupler pair\n\n\n\ndef test_ValidationReturnErrorWithSameQubitCoupler():\n    with pytest.raises(ValueError):\n       CZ_Parametrisation_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q14\"])\nInputs can be incorrect and should always be tested to avoid unexpected behaviour down the line which can be difficult to trace back to the origin. There are infinite number of possible errors, so it is impossible to cover them all, but at least the obvious ones should be considered. In this case a typical typing error with the couple have the same qubit twice.\n\n\n\n@pytest.fixture(autouse=True)\ndef setup_good_data():\n    os.environ[\"DATA_DIR\"] = str(Path(__file__).parent / \"results\")\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    return d14, d15, freqs, amps\nThis is an example of how to load data for testing the analysis of a node, please note that the specifics may change as we are about to change the dataset format at the time of writing this; however, the principle will be the same.\nThis data can then be used in multiple tests, for example:\ndef test_canGetMaxFromQ1(setup_good_data):\n    d14, d15, freqs, amps = setup_good_data\n    c = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    result = c.run_fitting()\n    indexBestFreq = np.where(freqs == result[0])[0]\n    indexBestAmp = np.where(amps == result[1])[0]\n    assert indexBestFreq[0] == 9\n    assert indexBestAmp[0] == 13\n\n\ndef test_canGetMinFromQ2(setup_good_data):\n    d14, d15, freqs, amps = setup_good_data\n    c = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    result = c.run_fitting()\n    indexBestFreq = np.where(freqs == result[0])[0]\n    indexBestAmp = np.where(amps == result[1])[0]\n    assert indexBestFreq[0] == 10\n    assert indexBestAmp[0] == 12\nThese two tests make sure that the return values are correct for the two qubits that are connected by the coupler.\n\n\n\ndef test_canPlotBad(setup_bad_data):\n    matplotlib.use(\"Agg\")\n    d14, d15, freqs, amps = setup_bad_data\n    c14 = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    result = c14.run_fitting()\n\n    figure_path = os.environ[\"DATA_DIR\"] + \"/Frequency_Amplitude_bad_q14.png\"\n    # Remove the file if it already exists\n    if os.path.exists(figure_path):\n        os.remove(figure_path)\n\n    fig, ax = plt.subplots(figsize=(15, 7), num=1)\n    plt.Axes\n    c14.plotter(ax)\n    fig.savefig(figure_path)\n    plt.close()\n\n    assert os.path.exists(figure_path)\n    from PIL import Image\n\n    with Image.open(figure_path) as img:\n        assert img.format == \"PNG\", \"File should be a PNG image\"\n\n    c15 = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    result = c15.run_fitting()\n\n    figure_path = os.environ[\"DATA_DIR\"] + \"/Frequency_Amplitude_bad_q15.png\"\n    # Remove the file if it already exists\n    if os.path.exists(figure_path):\n        os.remove(figure_path)\n\n    fig, ax = plt.subplots(figsize=(15, 7), num=1)\n    plt.Axes\n    c15.plotter(ax)\n    fig.savefig(figure_path)\n    plt.close()\n\n    assert os.path.exists(figure_path)\n    from PIL import Image\n\n    with Image.open(figure_path) as img:\n        assert img.format == \"PNG\", \"File should be a PNG image\"\nThis is an example on how to save files in the “results” sub-folder within the “tests” folder. The code make sure the expected file exists and has the correct format. The created files can be inspected by the developer. A possible extension would be to upload the images in the data folder and check the those produced by the test are identical.\n\n\n\n@pytest.fixture(autouse=True)\ndef setup_bad_data():\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_bad_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    q15Res = q15Ana.run_fitting()\n    return q14Res, q15Res\n\n\ndef test_combineBadResultsReturnNoValidPoint(setup_bad_data):\n    q14Res, q15Res = setup_bad_data\n    c = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n    r = c.are_frequencies_compatible()\n    assert r == False\n    r = c.are_amplitudes_compatible()\n    assert r == False\n    r = c.are_two_qubits_compatible()\n    assert r == False\nIn this example, the data produced is loaded from a file that has data that is not a good working point. Note that there two analyses are run in the setup; the combination is run in the test, so that, if needed in other tests, the data could be modified for specific cases. This is a failure test, making sure that bad inputs are not recognised as good working points.\n\n\n\ndef setup_data():\n    # It should be a single dataset, but we do not have one yet, so we loop over existing files\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    q15Res = q15Ana.run_fitting()\n    c1 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_bad_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs_bad = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps_bad = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(\n        d14, freqs_bad, amps_bad\n    )\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(\n        d15, freqs_bad, amps_bad\n    )\n    q15Res = q15Ana.run_fitting()\n    c2 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    dataset_path = (\n        Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp_2.hdf5\"\n    )\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs_2 = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps_2 = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs_2, amps_2)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs_2, amps_2)\n    q15Res = q15Ana.run_fitting()\n    c3 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    list_of_results = [(c1, 0.1), (c2, 0.2), (c3, 0.3)]\n    return list_of_results, freqs, amps, freqs_2, amps_2\nIn this case, three points are loaded and the analysis is run on each of them. The results are returned for each point and can be used in different tests, for example by removing elements in the list_results array."
  },
  {
    "objectID": "unit_tests.html#folder-structure",
    "href": "unit_tests.html#folder-structure",
    "title": "Unit tests for nodes",
    "section": "",
    "text": "The test should be created in a sub-folder of the node called tests.\nPlease organise the file using these sub-folders:\n\ndata: place here any data file that is needed to create test cases, while it is possible to mock the data. Feel free to add a text file explaing how the data was produced. Mocking data is also possible but it may be not practical for complex datasets.\nresults: create this folder to store your results, i.e. files that would be created by your analysis such as the plots. It should be empty, do not commit your results. To assure this is the case, add a file name .gitinore in the folder with this code:\n\n# Ignore everything in this directory\n*\n\n# But do not ignore this file\n!.gitignore"
  },
  {
    "objectID": "unit_tests.html#general-guideline-and-information",
    "href": "unit_tests.html#general-guideline-and-information",
    "title": "Unit tests for nodes",
    "section": "",
    "text": "A good starting point, especially starting from scratch, it is to test for the class type, then that the inputs have the right formats and then move to some simple operation or trivial case. Build more complex cases from the simpler ones and exploits your tests to refector the code as needed. Try to test as many reasonable cases as possible, both successfull and not. Remeber to test for exceptions. We also suggest to develop using test drive development tecniques that will ensure a high test coverage and a good code structure. Yes, do not forget to keep refactoring to improve the code.\nYou can find some samples below taken from the cz_parametrisation node, where all objects are tested\nCurrently there is no way to differentiate from tests that require a QPU (i.e. measurments) and those that do not (i.e. analyses). Since the latter are simpler to write, start with those that, in general, are more likely to benefit from unit tests as there is much more logic in the analysis than in the measurment.\nIf you need to test some complex scenario, such as those involving sweep, it is probably easier to start writing code and tests from the lowest level and then compose those objects to hendle the more complex scenario considered."
  },
  {
    "objectID": "unit_tests.html#example-tests",
    "href": "unit_tests.html#example-tests",
    "title": "Unit tests for nodes",
    "section": "",
    "text": "def test_canCreateCorrectType():\n    c = CZ_Parametrisation_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q15\"])\n    assert isinstance(c, CZ_Parametrisation_Fix_Duration_Node)\n    assert isinstance(c, ParametrizedSweepNode)\nThe suggested very first test is to istantiate the class and make sure it has the correct type(s) following any inheritance.\n\n\n\ndef test_CanGetQubitsFromCouplers():\n    c = CZ_Parametrisation_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q15\"])\n    assert c.all_qubits == [\"q14\", \"q15\"]\n    assert c.couplers == ['q14_q15']\nMake sure all inputs and their manipulations are correctly initialised in the constructor, in this case the qubits are taken from the coupler pair\n\n\n\ndef test_ValidationReturnErrorWithSameQubitCoupler():\n    with pytest.raises(ValueError):\n       CZ_Parametrisation_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q14\"])\nInputs can be incorrect and should always be tested to avoid unexpected behaviour down the line which can be difficult to trace back to the origin. There are infinite number of possible errors, so it is impossible to cover them all, but at least the obvious ones should be considered. In this case a typical typing error with the couple have the same qubit twice.\n\n\n\n@pytest.fixture(autouse=True)\ndef setup_good_data():\n    os.environ[\"DATA_DIR\"] = str(Path(__file__).parent / \"results\")\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    return d14, d15, freqs, amps\nThis is an example of how to load data for testing the analysis of a node, please note that the specifics may change as we are about to change the dataset format at the time of writing this; however, the principle will be the same.\nThis data can then be used in multiple tests, for example:\ndef test_canGetMaxFromQ1(setup_good_data):\n    d14, d15, freqs, amps = setup_good_data\n    c = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    result = c.run_fitting()\n    indexBestFreq = np.where(freqs == result[0])[0]\n    indexBestAmp = np.where(amps == result[1])[0]\n    assert indexBestFreq[0] == 9\n    assert indexBestAmp[0] == 13\n\n\ndef test_canGetMinFromQ2(setup_good_data):\n    d14, d15, freqs, amps = setup_good_data\n    c = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    result = c.run_fitting()\n    indexBestFreq = np.where(freqs == result[0])[0]\n    indexBestAmp = np.where(amps == result[1])[0]\n    assert indexBestFreq[0] == 10\n    assert indexBestAmp[0] == 12\nThese two tests make sure that the return values are correct for the two qubits that are connected by the coupler.\n\n\n\ndef test_canPlotBad(setup_bad_data):\n    matplotlib.use(\"Agg\")\n    d14, d15, freqs, amps = setup_bad_data\n    c14 = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    result = c14.run_fitting()\n\n    figure_path = os.environ[\"DATA_DIR\"] + \"/Frequency_Amplitude_bad_q14.png\"\n    # Remove the file if it already exists\n    if os.path.exists(figure_path):\n        os.remove(figure_path)\n\n    fig, ax = plt.subplots(figsize=(15, 7), num=1)\n    plt.Axes\n    c14.plotter(ax)\n    fig.savefig(figure_path)\n    plt.close()\n\n    assert os.path.exists(figure_path)\n    from PIL import Image\n\n    with Image.open(figure_path) as img:\n        assert img.format == \"PNG\", \"File should be a PNG image\"\n\n    c15 = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    result = c15.run_fitting()\n\n    figure_path = os.environ[\"DATA_DIR\"] + \"/Frequency_Amplitude_bad_q15.png\"\n    # Remove the file if it already exists\n    if os.path.exists(figure_path):\n        os.remove(figure_path)\n\n    fig, ax = plt.subplots(figsize=(15, 7), num=1)\n    plt.Axes\n    c15.plotter(ax)\n    fig.savefig(figure_path)\n    plt.close()\n\n    assert os.path.exists(figure_path)\n    from PIL import Image\n\n    with Image.open(figure_path) as img:\n        assert img.format == \"PNG\", \"File should be a PNG image\"\nThis is an example on how to save files in the “results” sub-folder within the “tests” folder. The code make sure the expected file exists and has the correct format. The created files can be inspected by the developer. A possible extension would be to upload the images in the data folder and check the those produced by the test are identical.\n\n\n\n@pytest.fixture(autouse=True)\ndef setup_bad_data():\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_bad_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    q15Res = q15Ana.run_fitting()\n    return q14Res, q15Res\n\n\ndef test_combineBadResultsReturnNoValidPoint(setup_bad_data):\n    q14Res, q15Res = setup_bad_data\n    c = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n    r = c.are_frequencies_compatible()\n    assert r == False\n    r = c.are_amplitudes_compatible()\n    assert r == False\n    r = c.are_two_qubits_compatible()\n    assert r == False\nIn this example, the data produced is loaded from a file that has data that is not a good working point. Note that there two analyses are run in the setup; the combination is run in the test, so that, if needed in other tests, the data could be modified for specific cases. This is a failure test, making sure that bad inputs are not recognised as good working points.\n\n\n\ndef setup_data():\n    # It should be a single dataset, but we do not have one yet, so we loop over existing files\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    q15Res = q15Ana.run_fitting()\n    c1 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_bad_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs_bad = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps_bad = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(\n        d14, freqs_bad, amps_bad\n    )\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(\n        d15, freqs_bad, amps_bad\n    )\n    q15Res = q15Ana.run_fitting()\n    c2 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    dataset_path = (\n        Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp_2.hdf5\"\n    )\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs_2 = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps_2 = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs_2, amps_2)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs_2, amps_2)\n    q15Res = q15Ana.run_fitting()\n    c3 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    list_of_results = [(c1, 0.1), (c2, 0.2), (c3, 0.3)]\n    return list_of_results, freqs, amps, freqs_2, amps_2\nIn this case, three points are loaded and the analysis is run on each of them. The results are returned for each point and can be used in different tests, for example by removing elements in the list_results array."
  },
  {
    "objectID": "new_node_creation.html",
    "href": "new_node_creation.html",
    "title": "To create a new node:",
    "section": "",
    "text": "in the file tergite_autocalibration/lib/node_factory.py expand the dictionary self.node_implementations with a new entry: The key should be a string of the node name and the value should be the object that contains the implementation details. This object should be imported from either tergite_autocalibration/lib/nodes/qubit_control_nodes.py, tergite_autocalibration/lib/nodes/coupler_nodes.py, tergite_autocalibration/lib/nodes/readout_nodes.py or tergite_autocalibration/lib/nodes/characterization_nodes.py\nIn the file tergite_autocalibration/lib/nodes/graph.py in the list graph_dependencies insert the edges that describe the position of the new node in the Directed Acyclic Graph. There are two entries required (or one entry if the new node is the last on its path):\n\n\n('previous_node','new_node')\n('new_node', 'next_node')\n\n\nIn the tergite_autocalibration/config/device_config.toml set the quantity of interest at nan value\n\n\n\nEach node implementation object should contain a reference to the measurement object, the analysis object, the list of redis fields that the analysis updates and the samplespace of the measurement. For example on the Rabi Rabi Oscillations Node:\nclass Rabi_Oscillations_Node(BaseNode):\n    measurement_obj = Rabi_Oscillations\n    analysis_obj = RabiAnalysis\n\n    def __init__(self, name: str, all_qubits: list[str], **node_dictionary):\n        super().__init__(name, all_qubits, **node_dictionary)\n        self.redis_field = ['rxy:amp180']\n        self.schedule_samplespace = {\n            'mw_amplitudes': {\n                qubit: np.linspace(0.002, 0.80, 101) for qubit in self.all_qubits\n            }\n        }\n\n\nThe measurement_obj is imported from tergite_autocalibration/lib/calibration_schedules/ and contains the class that generates the appropriate measurement schedule. To initialize we require a dicttionary of the extended transmons:\ntransmons: dict[str, ExtendedTransmon]\nIt must contain a method called schedule_function that expects the node.samplespace as input and returns the complete schedule.\n\n\n\nThe analysis_obj is imported from tergite_autocalibration/lib/analysis/ and contains the class that perform the analysis for a single qubit. It must contain a run_fitting method and a plotter method\n\n\n\nNodes are divided in two distict categories:\n\nsimple_sweep: where the Quantify Schedule is compiled only once\nparameterized_sweep: where the node requires multiple iterations and each iteration requires a new recompilation.\n\nFurthermore each node can expect two types of samplespaces:\n\nschedule_samplespace: parameter values to be input to the schedule function\nexternal_samplespace: parameter values for quantities that are not set during a schedule\n\n\n\n\nPlease create a new node in a separate folder, so that is is clearer what the new node is meant to do Add an empty init.py file to the folder, this is needed to mark the folder as part of the packege and allow imports from these folders\nTo keep the code clean, please create sub-folders following this scheme:\n\ntests: create unit tests in here, more on tests in href: unit_tests\nutils: any utility class, such as enum, errors and similar classes should be placed here\n\n\n\n\nPlease add your node to the list of available nodes in this Documentation.\nAdd ny relevant information on how to use your node, dependencies and reference to publication as needed for allowing other to use the code you developed.\nDetails on the implementation on the Node types section.",
    "crumbs": [
      "Getting started",
      "Developer Guide",
      "To create a new node:"
    ]
  },
  {
    "objectID": "new_node_creation.html#node-implementation-object",
    "href": "new_node_creation.html#node-implementation-object",
    "title": "To create a new node:",
    "section": "",
    "text": "Each node implementation object should contain a reference to the measurement object, the analysis object, the list of redis fields that the analysis updates and the samplespace of the measurement. For example on the Rabi Rabi Oscillations Node:\nclass Rabi_Oscillations_Node(BaseNode):\n    measurement_obj = Rabi_Oscillations\n    analysis_obj = RabiAnalysis\n\n    def __init__(self, name: str, all_qubits: list[str], **node_dictionary):\n        super().__init__(name, all_qubits, **node_dictionary)\n        self.redis_field = ['rxy:amp180']\n        self.schedule_samplespace = {\n            'mw_amplitudes': {\n                qubit: np.linspace(0.002, 0.80, 101) for qubit in self.all_qubits\n            }\n        }\n\n\nThe measurement_obj is imported from tergite_autocalibration/lib/calibration_schedules/ and contains the class that generates the appropriate measurement schedule. To initialize we require a dicttionary of the extended transmons:\ntransmons: dict[str, ExtendedTransmon]\nIt must contain a method called schedule_function that expects the node.samplespace as input and returns the complete schedule.\n\n\n\nThe analysis_obj is imported from tergite_autocalibration/lib/analysis/ and contains the class that perform the analysis for a single qubit. It must contain a run_fitting method and a plotter method\n\n\n\nNodes are divided in two distict categories:\n\nsimple_sweep: where the Quantify Schedule is compiled only once\nparameterized_sweep: where the node requires multiple iterations and each iteration requires a new recompilation.\n\nFurthermore each node can expect two types of samplespaces:\n\nschedule_samplespace: parameter values to be input to the schedule function\nexternal_samplespace: parameter values for quantities that are not set during a schedule\n\n\n\n\nPlease create a new node in a separate folder, so that is is clearer what the new node is meant to do Add an empty init.py file to the folder, this is needed to mark the folder as part of the packege and allow imports from these folders\nTo keep the code clean, please create sub-folders following this scheme:\n\ntests: create unit tests in here, more on tests in href: unit_tests\nutils: any utility class, such as enum, errors and similar classes should be placed here\n\n\n\n\nPlease add your node to the list of available nodes in this Documentation.\nAdd ny relevant information on how to use your node, dependencies and reference to publication as needed for allowing other to use the code you developed.\nDetails on the implementation on the Node types section.",
    "crumbs": [
      "Getting started",
      "Developer Guide",
      "To create a new node:"
    ]
  },
  {
    "objectID": "available_nodes.html",
    "href": "available_nodes.html",
    "title": "Node Library",
    "section": "",
    "text": "graph TD\n    A[Resonator Spectroscopy] --&gt; B(Qubit Spectroscopy)\n    B --&gt; C[Rabi Oscillations]\n    C --&gt; D[Ramsey Correction]\n    D --&gt; E[Motzoi Parameter]\n    E --&gt; F[Resonator Spectroscopy 1]\n    F --&gt; C1[T1] --&gt; C2[T2] --&gt; C3[Randomized Benchmarking]\n    F --&gt; F1(Qubit 12 Spectroscopy) --&gt; G(Rabi 12 Oscillations)\n    F --&gt; H1(2 States Discrimination)\n    G --&gt; H2(3 States Discrimination)\n        \n    click A href \"nodes/resonator_spectroscopy_node.html\"\n\n    style A fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style B fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style C fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style D fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style E fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style F fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style C1 fill:#ffffcc,stroke:#333,stroke-width:2px\n    style C2 fill:#ffffcc,stroke:#333,stroke-width:2px\n    style C3 fill:#ffffcc,stroke:#333,stroke-width:2px\n    style F1 fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style G fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style H1 fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style H2 fill:#ffe6cc,stroke:#333,stroke-width:2px\n\n\n\n\n\n\n\n\n\npunchout\nresonator_spectroscopy\nresonator_spectroscopy_1\nresonator_spectroscopy_2\nro_frequency_two_state_optimization\nro_frequency_three_state_optimization\nro_amplitude_two_state_optimization\nro_amplitude_three_state_optimization\n\n\n\n\n\nqubit_01_spectroscopy\nqubit_01_spectroscopy_pulsed\nrabi_oscillations\nramsey_correction\nqubit_12_spectroscopy_pulsed\nqubit_12_spectroscopy_multidim\nrabi_oscillations_12\nramsey_correction_12\nadaptive_motzoi_parameter\nn_rabi_oscillations\nstate_discrimination\n\n\n\n\n\ncoupler_spectroscopy\ncoupler_resonator_spectroscopy\n\n\n\n\n\nT1\nT2\nT2_echo\nrandomized_benchmarking\nall_XY",
    "crumbs": [
      "Home",
      "Node Library",
      "Overview"
    ]
  },
  {
    "objectID": "available_nodes.html#readout-nodes",
    "href": "available_nodes.html#readout-nodes",
    "title": "Node Library",
    "section": "",
    "text": "punchout\nresonator_spectroscopy\nresonator_spectroscopy_1\nresonator_spectroscopy_2\nro_frequency_two_state_optimization\nro_frequency_three_state_optimization\nro_amplitude_two_state_optimization\nro_amplitude_three_state_optimization",
    "crumbs": [
      "Home",
      "Node Library",
      "Overview"
    ]
  },
  {
    "objectID": "available_nodes.html#qubit-control-nodes",
    "href": "available_nodes.html#qubit-control-nodes",
    "title": "Node Library",
    "section": "",
    "text": "qubit_01_spectroscopy\nqubit_01_spectroscopy_pulsed\nrabi_oscillations\nramsey_correction\nqubit_12_spectroscopy_pulsed\nqubit_12_spectroscopy_multidim\nrabi_oscillations_12\nramsey_correction_12\nadaptive_motzoi_parameter\nn_rabi_oscillations\nstate_discrimination",
    "crumbs": [
      "Home",
      "Node Library",
      "Overview"
    ]
  },
  {
    "objectID": "available_nodes.html#coupler-nodes",
    "href": "available_nodes.html#coupler-nodes",
    "title": "Node Library",
    "section": "",
    "text": "coupler_spectroscopy\ncoupler_resonator_spectroscopy",
    "crumbs": [
      "Home",
      "Node Library",
      "Overview"
    ]
  },
  {
    "objectID": "available_nodes.html#characterization-nodes",
    "href": "available_nodes.html#characterization-nodes",
    "title": "Node Library",
    "section": "",
    "text": "T1\nT2\nT2_echo\nrandomized_benchmarking\nall_XY",
    "crumbs": [
      "Home",
      "Node Library",
      "Overview"
    ]
  },
  {
    "objectID": "operation.html",
    "href": "operation.html",
    "title": "Tergite Automatic Calibration",
    "section": "",
    "text": "The package ships with a command line interface to solve some common tasks that appear quite often.\nIn the following there are a number of useful commands, but if you want to find out all commands use: acli --help\nTo delete all redis entries: acli node reset -a\nTo reset a particular node: acli node reset -n &lt;nodename&gt;\nFor example to reset the node rabi_oscillations run the command:\nacli node reset -n rabi_oscillations\nTo start a new calibration sequence according to the configuration files:\npython tergite_acl/scripts/calibration_supervisor.py\nor\nacli calibration start",
    "crumbs": [
      "Getting started",
      "Operation"
    ]
  },
  {
    "objectID": "operation.html#operation",
    "href": "operation.html#operation",
    "title": "Tergite Automatic Calibration",
    "section": "",
    "text": "The package ships with a command line interface to solve some common tasks that appear quite often.\nIn the following there are a number of useful commands, but if you want to find out all commands use: acli --help\nTo delete all redis entries: acli node reset -a\nTo reset a particular node: acli node reset -n &lt;nodename&gt;\nFor example to reset the node rabi_oscillations run the command:\nacli node reset -n rabi_oscillations\nTo start a new calibration sequence according to the configuration files:\npython tergite_acl/scripts/calibration_supervisor.py\nor\nacli calibration start",
    "crumbs": [
      "Getting started",
      "Operation"
    ]
  },
  {
    "objectID": "operation.html#cli",
    "href": "operation.html#cli",
    "title": "Tergite Automatic Calibration",
    "section": "CLI",
    "text": "CLI\nThis document provides an overview of the Command Line Interface (CLI) options and their functionalities.\n\nMain Commands\nThe autocalibration CLI is organized into several main command groups:\n\ncluster: Handle operations related to the cluster\nnode: Handle operations related to the node\ngraph: Handle operations related to the calibration graph\ncalibration: Handle operations related to the calibration supervisor\njoke: Handle operations related to the well-being of the user\n\n\n\nCluster Commands\n\ncluster reboot\nReboots the cluster.\nUsage:\nacli cluster reboot\nThis command will prompt for confirmation before rebooting the cluster, as it can interrupt ongoing measurements.\n\n\n\nNode Commands\n\nnode reset\nResets all parameters in Redis for the specified node(s).\nUsage:\nacli node reset [OPTIONS]\nOptions:\n\n-n, --name TEXT: Name of the node to be reset in Redis (e.g., resonator_spectroscopy)\n-a, --all: Reset all nodes\n-f, --from_node TEXT: Reset all nodes from the specified node in the chain\n\n\n\n\nGraph Commands\n\ngraph plot\nPlots the calibration graph to the user-specified target node in topological order.\nUsage:\nacli graph plot\nThis command visualizes the calibration graph using an arrow chart.\n\n\n\nCalibration Commands\n\ncalibration start\nStarts the calibration supervisor.\nUsage:\nacli calibration start [OPTIONS]\nOptions:\n\n-c TEXT: Cluster IP address (if not set, it will use CLUSTER_IP from the .env file)\n-r TEXT: Rerun an analysis (specify the path to the dataset folder)\n-n, --name TEXT: Specify the node type to rerun (works only with -r option)\n--push: Push a backend to an MSS specified in MSS_MACHINE_ROOT_URL in the .env file\n--browser: Will open the dataset browser in the background and plot the measurement results live\n\n\n\n\nDataset browser\n\nbrowser\nStarts the dataset browser.\nUsage:\nacli browser --datadir [OPTIONS]\nOptions:\n\n--datadir PATH: Folder to take the plot data from\n--liveplotting: Whether plots should be updated in real time (default: False)\n--log-level INT: Log-level as in the Python logging package to be used in the logs (default: 30)\n\n\n\n\nJoke Command\n\njoke\nPrints a random joke to lighten the mood.\nUsage:\nacli joke\nThis command fetches and displays a random joke, excluding potentially offensive content.\n\n\n\nNotes\n\nThe CLI uses the Click library for command-line interface creation.\nSome commands may require additional configuration or environment variables to be set.\nWhen using the -r option for rerunning analysis, make sure to also specify the node name using -n.\n\nFor more detailed information about each command, use the --help option with any command or subcommand.",
    "crumbs": [
      "Getting started",
      "Operation"
    ]
  },
  {
    "objectID": "operation.html#structure",
    "href": "operation.html#structure",
    "title": "Tergite Automatic Calibration",
    "section": "Structure",
    "text": "Structure\nFor each calibration node: compilation -&gt; execution -&gt; post-processing -&gt; redis updating",
    "crumbs": [
      "Getting started",
      "Operation"
    ]
  },
  {
    "objectID": "operation.html#data-browsing",
    "href": "operation.html#data-browsing",
    "title": "Tergite Automatic Calibration",
    "section": "Data browsing",
    "text": "Data browsing\nDatasets are stored in data_directory Can be browsed with the dataset browser (coming soon)",
    "crumbs": [
      "Getting started",
      "Operation"
    ]
  },
  {
    "objectID": "operation.html#development",
    "href": "operation.html#development",
    "title": "Tergite Automatic Calibration",
    "section": "Development",
    "text": "Development\nWhen submitting contributions, please prepend your commit messages with: fix: for bug fixes feat: for introducing a new feature (e.g. a new measurement node or a new analysis class) chore: for refactoring changes or any change that doesn’t affect the functionality of the code docs: for changes in the README, docstrings etc test: or dev: for testing or development changes (e.g. profiling scripts)",
    "crumbs": [
      "Getting started",
      "Operation"
    ]
  },
  {
    "objectID": "getting_started.html",
    "href": "getting_started.html",
    "title": "Getting started",
    "section": "",
    "text": "This guide contains some information on how to get started with the automatic calibration. Please consider also reading the README.md file in the git repository for more information.\n\n\nThe automatic calibration requires redis for on-memory data storage. As redis operates only on Linux systems, the calibration has to run either on one of:\n\nLinux distributions\nWSL (Windows Subsystem for Linux) environments, installed on a Windows system. WSL requires Windows 10 and a version of at least 1903.\n\nInstallation instructions for redis can be found here: https://redis.io/docs/getting-started/installation/install-redis-on-linux/\nThe link for the redis installation also contains instructions on how to start a redis instance. However, if you already have redis installed, you can run it using:\nredis-server --port YOUR_REDIS_PORT\nUsually, redis will run automatically on port 6379, but in a shared environment please check with others to get your redis port, since you would overwrite each other’s memory.\n\n\n\nThe first step during the installation is to clone the repository. Please note that the link below is the link to the public mirror of the repository on GitHub. If you are developing code, most likely, you have to replace it with the link to the development server.\ngit clone git@github.com:tergite/tergite-autocalibration.git\nTo manage Python packages, we are using the package manager conda. It is recommended to create an environment for your project - alternatively, you can also just use Python 3.10\nconda create -n tac python=3.10 -y\nHere, tac stands for tergite-autocalibration. We can activate and use the environment like this:\nconda activate tac\nIf you are not using conda, activate the environment with:\nsource activate tac\nNow, you should enter the repository folder, because the following commands have to be executed in there.\ncd tergite-autocalibration\nTo install the repository in editable mode. In Python the editable mode is triggerd by the parameter -e. This means that when you changed and saved your code files, they will be automatically loaded into the environment without re-installation.\npip install -e .\nHere . is the root directory (i.e. the directory that contains the pyproject.toml file)\n\n\n\n\n\nBefore the first run of the calibration suite a number of configuration files need to be set up. These files describe our initial knowledge on the system as well as the connectivity with the measurement hardware.\n\n\n\nAll the settings and paths are contained in the .env file. Since this file contains user specific settings, it is git-ignored, it must be created by the user. For convenience a template file dot-env-template.txt already exists.\nFirst copy the template file to the .env (this creates the .env file if it doesn’t exist):\ncp dot-env-template .env\nThen edit the newly created .env file according to your system. For example here’s how the user lab-user may complete the .env file:\n# Copy this file to a .env file in the tergite-autocalibration folder on the root level.\n# The .env file is a simple list of keys and values. It is also known as the INI file on MS Windows.\n# Fill in the necessary values.\n\n# DEFAULT_PREFIX is added to logfiles, redis entries and in the data directory\n# Default: cal\nDEFAULT_PREFIX = 'calibration'\n\n# Directory settings\n# ROOT_DIR defines the top-level folder of the tergite-autocalibration-lite folder\n# Default: two levels up from the config\nROOT_DIR = '/home/lab-user/github/tergite-acl/'\n\n# DATA_DIR defines where plots are stored\nDATA_DIR = '/home/lab-user/github/tergite-acl/data_directory/'\n\n# CONFIG_DIR defines where the configuration is stored\nCONFIG_DIR = '/home/lab-user/github/tergite-acl/config_dir/'\n\n# Configuration settings\n# It is assumed that all these paths are relative to CONFIG_DIR\n# HARDWARE_CONFIG is what Q-BLOX needs to compile schedules on the hardware\n# It should be a file in json format, there is no default file\nHARDWARE_CONFIG = 'HARDWARE_CONFIGURATION.json'\n\n# DEVICE_CONFIG contains the initial values for the device configuration\nDEVICE_CONFIG = 'device_config.toml'\n\n# Configuration variables\n# CLUSTER_IP is the IP address of the instrument cluster to connect with\nCLUSTER_IP = '162.0.2.162'\n# SPI_SERIAL_PORT is the port on which the spi rack is connected\nSPI_SERIAL_PORT = '/dev/ttyACM0'\n\n# APP_SETTINGS reflect which environment the calibration is to run in.\n# Options\n#  - development\n#  - production\n#  - staging\n#  - test\n# Default: production\n# TODO: currently we are only using the calibration in the development mode\nRUN_MODE = 'development'\n\n# REDIS_PORT is the port which to use when connecting to redis\nREDIS_PORT = 6379\n# REDIS_CONNECTION will be automatically created in settings.py\n\n# PLOTTING is a boolean to indicate whether plots should be shown or whether plots should be silent in the background\n# Default: True\nPLOTTING = True",
    "crumbs": [
      "Home",
      "User Guide",
      "Getting started"
    ]
  },
  {
    "objectID": "getting_started.html#installation",
    "href": "getting_started.html#installation",
    "title": "Getting started",
    "section": "",
    "text": "The first step during the installation is to clone the repository. Please note that the link below is the link to the public mirror of the repository on GitHub. If you are developing code, most likely, you have to replace it with the link to the development server.\ngit clone git@github.com:tergite/tergite-autocalibration.git\nTo manage Python packages, we are using the package manager conda. It is recommended to create an environment for your project - alternatively, you can also just use Python 3.10\nconda create -n tac python=3.10 -y\nHere, tac stands for tergite-autocalibration. We can activate and use the environment like this:\nconda activate tac\nIf you are not using conda, activate the environment with:\nsource activate tac\nNow, you should enter the repository folder, because the following commands have to be executed in there.\ncd tergite-autocalibration\nTo install the repository in editable mode. In Python the editable mode is triggerd by the parameter -e. This means that when you changed and saved your code files, they will be automatically loaded into the environment without re-installation.\npip install -e .\nHere . is the root directory (i.e. the directory that contains the pyproject.toml file)\n\n\n\n\n\nBefore the first run of the calibration suite a number of configuration files need to be set up. These files describe our initial knowledge on the system as well as the connectivity with the measurement hardware.\n\n\n\nAll the settings and paths are contained in the .env file. Since this file contains user specific settings, it is git-ignored, it must be created by the user. For convenience a template file dot-env-template.txt already exists.\nFirst copy the template file to the .env (this creates the .env file if it doesn’t exist):\ncp dot-env-template .env\nThen edit the newly created .env file according to your system. For example here’s how the user lab-user may complete the .env file:\n# Copy this file to a .env file in the tergite-autocalibration folder on the root level.\n# The .env file is a simple list of keys and values. It is also known as the INI file on MS Windows.\n# Fill in the necessary values.\n\n# DEFAULT_PREFIX is added to logfiles, redis entries and in the data directory\n# Default: cal\nDEFAULT_PREFIX = 'calibration'\n\n# Directory settings\n# ROOT_DIR defines the top-level folder of the tergite-autocalibration-lite folder\n# Default: two levels up from the config\nROOT_DIR = '/home/lab-user/github/tergite-acl/'\n\n# DATA_DIR defines where plots are stored\nDATA_DIR = '/home/lab-user/github/tergite-acl/data_directory/'\n\n# CONFIG_DIR defines where the configuration is stored\nCONFIG_DIR = '/home/lab-user/github/tergite-acl/config_dir/'\n\n# Configuration settings\n# It is assumed that all these paths are relative to CONFIG_DIR\n# HARDWARE_CONFIG is what Q-BLOX needs to compile schedules on the hardware\n# It should be a file in json format, there is no default file\nHARDWARE_CONFIG = 'HARDWARE_CONFIGURATION.json'\n\n# DEVICE_CONFIG contains the initial values for the device configuration\nDEVICE_CONFIG = 'device_config.toml'\n\n# Configuration variables\n# CLUSTER_IP is the IP address of the instrument cluster to connect with\nCLUSTER_IP = '162.0.2.162'\n# SPI_SERIAL_PORT is the port on which the spi rack is connected\nSPI_SERIAL_PORT = '/dev/ttyACM0'\n\n# APP_SETTINGS reflect which environment the calibration is to run in.\n# Options\n#  - development\n#  - production\n#  - staging\n#  - test\n# Default: production\n# TODO: currently we are only using the calibration in the development mode\nRUN_MODE = 'development'\n\n# REDIS_PORT is the port which to use when connecting to redis\nREDIS_PORT = 6379\n# REDIS_CONNECTION will be automatically created in settings.py\n\n# PLOTTING is a boolean to indicate whether plots should be shown or whether plots should be silent in the background\n# Default: True\nPLOTTING = True",
    "crumbs": [
      "Home",
      "User Guide",
      "Getting started"
    ]
  },
  {
    "objectID": "getting_started.html#repository-installation",
    "href": "getting_started.html#repository-installation",
    "title": "Getting started",
    "section": "",
    "text": "git clone git@github.com:chalmersnextlabs-quantum/tergite-autocalibration.git\n\n\n\nhttps://redis.io/docs/getting-started/installation/install-redis-on-linux/\n\n\n\nredis-server\n\n\n\nIf for example you want to name your environment tac, you create it as\nconda create --name tac python=3.9\n\n\n\nconda activate tac\n\n\n\nsource activate tac\n\n\n\ncd tergite-autocalibration-lite/\nFrom now on, it is assumed that all commands are executed from the project root directory.\n\n\n\npip install -e .\nHere . is the root directory (i.e. the directory that contains the pyproject.toml file)",
    "crumbs": [
      "Home",
      "User Guide",
      "Getting started"
    ]
  },
  {
    "objectID": "getting_started.html#setting-up-system-configuration-files",
    "href": "getting_started.html#setting-up-system-configuration-files",
    "title": "Getting started",
    "section": "",
    "text": "Before the first run of the callibration suite a number of configuration files need to be set up. These files describe our initial knowledge on the system as well as the connectivity with the measurement hardware.\n\n\nAll the settings and paths are contained in the .env file. Since this file contains user specific settings, it is git-ignored, it must be created by the user. For convinience a template file dot-env-template.txt already exists.\nFirst copy the template file to the .env (this creates the .env file if it doesn’t exist):\ncp dot-env-template .env\nThen edit the newly created .env file according to your system. For example here’s how the user lab-user may complete the .env file:\n# Copy this file to a .env file in the tergite-autocalibration folder on the root level.\n# The .env file is a simple list of keys and values. It is also known as the INI file on MS Windows.\n# Fill in the necessary values.\n\n# DEFAULT_PREFIX is added to logfiles, redis entries and in the data directory\n# Default: cal\nDEFAULT_PREFIX=calibration\n\n# Directory settings\n# ROOT_DIR defines the top-level folder of the tergite-autocalibration-lite folder\n# Default: two levels up from the config\nROOT_DIR='/home/lab-user/github/tergite-acl/'\n\n# DATA_DIR defines where plots are stored\nDATA_DIR='/home/lab-user/github/tergite-acl/data_directory/'\n\n# CONFIG_DIR defines where the configuration is stored\nCONFIG_DIR='/home/lab-user/github/tergite-acl/config_dir/'\n\n\n# Configuration settings\n# It is assumed that all these paths are relative to CONFIG_DIR\n# HARDWARE_CONFIG is what Q-BLOX needs to compile schedules on the hardware\n# It should be a file in json format, there is no default file\nHARDWARE_CONFIG='HARDWARE_CONFIGURATION.json'\n\n# DEVICE_CONFIG contains the initial values for the device configuration\nDEVICE_CONFIG='device_config.toml'\n\n# Configuration variables\n# CLUSTER_IP is the IP address of the instrument cluster to connect with\nCLUSTER_IP='162.0.2.162'\n# SPI_SERIAL_PORT is the port on which the spi rack is connected\nSPI_SERIAL_PORT='/dev/ttyACM0'\n\n# APP_SETTINGS reflect which environment the calibration is to run in.\n# Options\n#  - development\n#  - production\n#  - staging\n#  - test\n# Default: production\n# TODO: currently we are only using the calibration in the development mode\nRUN_MODE=development\n\n# REDIS_PORT is the port which to use when connecting to redis\nREDIS_PORT=6379\n# REDIS_CONNECTION will be automatically created in settings.py\n\n# PLOTTING is a boolean to indicate whether plots should be shown or whether plots should be silent in the background\n# Default: True\nPLOTTING=True",
    "crumbs": [
      "Home",
      "User Guide",
      "Getting started"
    ]
  },
  {
    "objectID": "configuration_files.html",
    "href": "configuration_files.html",
    "title": "Configuration Files",
    "section": "",
    "text": "Here the connection is made between the Qblox cluster physical ports and clocks to the qubits and couplers of the QPU.\nThe file must be placed at the directory configs/\nGiven a .csv file after a mixer calibration the function ... can create instantly the corresponding JSON file.\n\n\nWith quantify-scheduler 0.18.0 there is been introduced a new way on how to structure the hardware configuration file. If you are having a hardware configuration file, that is structured using the old way, you can use the following script to migrate it to the new structure.\npython tergite_autocalibration/scripts/migrate_blox_hardware_configuration.py &lt;PATH_TO_HW_CONFIG&gt;\n\n\n\n\nHere we set reasonable initial parameters\n\n\n\nHere we input the initial values for the resonator and qubits frequencies, assumed that they have been acquired through VNA measurements, design targets or previous manual measurements. /tergite_acl/config/VNA_values.py"
  },
  {
    "objectID": "configuration_files.html#hardware-configuration-file-json",
    "href": "configuration_files.html#hardware-configuration-file-json",
    "title": "Configuration Files",
    "section": "",
    "text": "Here the connection is made between the Qblox cluster physical ports and clocks to the qubits and couplers of the QPU.\nThe file must be placed at the directory configs/\nGiven a .csv file after a mixer calibration the function ... can create instantly the corresponding JSON file.\n\n\nWith quantify-scheduler 0.18.0 there is been introduced a new way on how to structure the hardware configuration file. If you are having a hardware configuration file, that is structured using the old way, you can use the following script to migrate it to the new structure.\npython tergite_autocalibration/scripts/migrate_blox_hardware_configuration.py &lt;PATH_TO_HW_CONFIG&gt;"
  },
  {
    "objectID": "configuration_files.html#device-configuration-file-toml",
    "href": "configuration_files.html#device-configuration-file-toml",
    "title": "Configuration Files",
    "section": "",
    "text": "Here we set reasonable initial parameters"
  },
  {
    "objectID": "configuration_files.html#vna-resonator-qubit-frequencies",
    "href": "configuration_files.html#vna-resonator-qubit-frequencies",
    "title": "Configuration Files",
    "section": "",
    "text": "Here we input the initial values for the resonator and qubits frequencies, assumed that they have been acquired through VNA measurements, design targets or previous manual measurements. /tergite_acl/config/VNA_values.py"
  },
  {
    "objectID": "developer_guide.html",
    "href": "developer_guide.html",
    "title": "Developer Guide",
    "section": "",
    "text": "This and the following sections provide information on how to develop code in tergite-autocalibration.\n\n\nConsider installing Quarto and other packages before you create your coda environment to have the path correctly initialised in your environment. This is not necessary, but it can simplify operations later, especially using VSCode.\nAfter you install tergite-autocalibration with:\npip install -e .\nrun\npip install poetry\npoetry install --with dev,test\nthis will install the additional packages for developing code and running tests\n\n\n\nWe use American English, please set any spell checker to this language.\n\nThe file names should be written using snake_case, with words written in lowercase and separated by underscores.\nClass names should be in PascalCase (where all words are capitalized). Many class do not follow this rule and use CamelCase with underscore, they will be changed.\nMethods should be in snake_case\nVariables should be in snake_case\n\n\n\n\nThere are settings available in the repo for IDEs, recommending extensions and settings. Please discuss with the team before modifying these default settings, they should be changed only with the consensus of the team.\n\n\nBlack, quarto and python extensions are recommended. Some settings are recommended too. You can find the settings for VSCode in the repository in the folder .vscode.\n\n\n\n\nPlease read carefully below, what should be done before doing a commit to a merge request. Most of the points are going to be checked automatically in GitLab, so, you would receive an error when running the pipeline.\n\n\nWhen submitting contributions, please prepend your commit messages with:\n\nfix: for bug fixes\nfeat: for introducing and working on a new feature (e.g. a new measurement node or a new analysis class)\nchore: for refactoring changes or any change that doesn’t affect the functionality of the code\ndocs: for changes in the README, docstrings etc\ntest: or dev: for testing or development changes (e.g. profiling scripts)\n\n\n\n\nWhen you create or modify a file, make sure that add the following copyright text is present at the top of the file. Remember to add your name and do not delete previous contributors. If you add the statement to a file that does not have one, please check on git the names of contributors.\n# This code is part of Tergite Autocalibration\n#\n# (C) Copyright WRITE YOUR NAME HERE, 2024\n#\n# This code is licensed under the Apache License, Version 2.0. You may\n# obtain a copy of this license in the LICENSE.txt file in the root directory\n# of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.\n#\n# Any modifications or derivative works of this code must retain this\n# copyright notice, and modified files need to carry a notice indicating\n# that they have been altered from the originals.\n\n\n\nUpdate the changelog in the section called [Unreleased]. Please note that there are several sections titled “Added”, ” Change” and “Fixed”; place your text in the correct category.\n\n\n\nThe code analyzer used in the project is black, which is installed as part of the dev dependencies. To use black, open a shell and run\nblack .\nPlease make sure to run it before committing to a merge request.\nIf your pipeline is still showing an error when you are running black, please make sure that you have installed the right version of black. You can check that by running\nblack --version\nIf your black version differs, it might be possible that you have had installed a version of black either in your base environment or via apt. To remove these versions, please deactivate your conda environment with conda deactivate and then run:\nsudo apt remote black\nor\npip uninstall black\nNext, please activate again your Python environment and install the correct version black as defined in the pyproject.toml file.\npip install black==VERSION_FROM_PYPROJECT_TOML\n\n\n\n\nAs you noticed, most of the above advice contains the formatting and other formal steps during development. Consider reading about:\n\nUnit testing to find out more about how to write test cases for the code.\nNodes on how to create a new calibration node.\nNode Classes to learn about the different types of nodes that the framework supports.\n\nIf you are having any feedback about the documentation or want to work on documentation, please continue with this developer guide about how to contribute with documentation.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Introduction"
    ]
  },
  {
    "objectID": "developer_guide.html#additional-installations",
    "href": "developer_guide.html#additional-installations",
    "title": "Developer Guide",
    "section": "",
    "text": "Consider installing Quarto and other packages before you create your coda environment to have the path correctly initialised in your environment. This is not necessary, but it can simplify operations later, especially using VSCode.\nAfter you install tergite-autocalibration with:\npip install -e .\nrun\npip install poetry\npoetry install --with dev,test\nthis will install the additional packages for developing code and running tests",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Introduction"
    ]
  },
  {
    "objectID": "developer_guide.html#naming-convention-and-style",
    "href": "developer_guide.html#naming-convention-and-style",
    "title": "Developer Guide",
    "section": "",
    "text": "We use American English, please set any spell checker to this language.\n\nThe file names should be written using snake_case, with words written in lowercase and separated by underscores.\nClass names should be in PascalCase (where all words are capitalized). Many class do not follow this rule and use CamelCase with underscore, they will be changed.\nMethods should be in snake_case\nVariables should be in snake_case",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Introduction"
    ]
  },
  {
    "objectID": "developer_guide.html#ide-preloaded-settings",
    "href": "developer_guide.html#ide-preloaded-settings",
    "title": "Developer Guide",
    "section": "",
    "text": "There are settings available in the repo for IDEs, recommending extensions and settings. Please discuss with the team before modifying these default settings, they should be changed only with the consensus of the team.\n\n\nBlack, quarto and python extensions are recommended. Some settings are recommended too. You can find the settings for VSCode in the repository in the folder .vscode.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Introduction"
    ]
  },
  {
    "objectID": "developer_guide.html#things-to-do-before-a-commit",
    "href": "developer_guide.html#things-to-do-before-a-commit",
    "title": "Developer Guide",
    "section": "",
    "text": "Please read carefully below, what should be done before doing a commit to a merge request. Most of the points are going to be checked automatically in GitLab, so, you would receive an error when running the pipeline.\n\n\nWhen submitting contributions, please prepend your commit messages with:\n\nfix: for bug fixes\nfeat: for introducing and working on a new feature (e.g. a new measurement node or a new analysis class)\nchore: for refactoring changes or any change that doesn’t affect the functionality of the code\ndocs: for changes in the README, docstrings etc\ntest: or dev: for testing or development changes (e.g. profiling scripts)\n\n\n\n\nWhen you create or modify a file, make sure that add the following copyright text is present at the top of the file. Remember to add your name and do not delete previous contributors. If you add the statement to a file that does not have one, please check on git the names of contributors.\n# This code is part of Tergite Autocalibration\n#\n# (C) Copyright WRITE YOUR NAME HERE, 2024\n#\n# This code is licensed under the Apache License, Version 2.0. You may\n# obtain a copy of this license in the LICENSE.txt file in the root directory\n# of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.\n#\n# Any modifications or derivative works of this code must retain this\n# copyright notice, and modified files need to carry a notice indicating\n# that they have been altered from the originals.\n\n\n\nUpdate the changelog in the section called [Unreleased]. Please note that there are several sections titled “Added”, ” Change” and “Fixed”; place your text in the correct category.\n\n\n\nThe code analyzer used in the project is black, which is installed as part of the dev dependencies. To use black, open a shell and run\nblack .\nPlease make sure to run it before committing to a merge request.\nIf your pipeline is still showing an error when you are running black, please make sure that you have installed the right version of black. You can check that by running\nblack --version\nIf your black version differs, it might be possible that you have had installed a version of black either in your base environment or via apt. To remove these versions, please deactivate your conda environment with conda deactivate and then run:\nsudo apt remote black\nor\npip uninstall black\nNext, please activate again your Python environment and install the correct version black as defined in the pyproject.toml file.\npip install black==VERSION_FROM_PYPROJECT_TOML",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Introduction"
    ]
  },
  {
    "objectID": "node_types.html",
    "href": "node_types.html",
    "title": "Node types",
    "section": "",
    "text": "Node types\nThe execution of most of the nodes consists of a single schedule compilation, a single measurement and a single post-processing. Although for most of the nodes this workflow suffices, there are exceptions while this workflow can become limiting in more advanced implementations.\nTo allow greater flexibilty in the node implementations the nodes are categorized:\n\nAccording to whether they compile once or multiple times:\n\nnode.type = simple_sweep: if it compiles once\nnode.type = parameterized_sweep: if it compiles multiple times\n\nAccording to whether the sweeping parameters are swept whithin the schedule or not:\n\nThere is only node.schedule_samplespace if the sweeping takes place whithin the schedule\nThere are both node.schedule_samplespace and node.external_samplespace if there are sweeping parameters outside of the scedule. For example the coupler_spectroscopy node sweeps the dc_current outside of the schedule:\n\n\n  class Coupler_Spectroscopy_Node(BaseNode):\n    measurement_obj = Two_Tones_Multidim\n    analysis_obj = CouplerSpectroscopyAnalysis\n\n    def __init__(self, name: str, all_qubits: list[str], couplers, **schedule_keywords):\n        super().__init__(name, all_qubits, **schedule_keywords)\n        self.couplers = couplers\n        self.redis_field = ['parking_current']\n        self.all_qubits = self.coupled_qubits\n\n        self.schedule_samplespace = {\n            'spec_frequencies': {\n                qubit: qubit_samples(qubit) for qubit in self.all_qubits\n            }\n        }\n\n        self.external_samplespace = {\n            'dc_currents': {\n                self.coupler: np.arange(-2.5e-3, 2.5e-3, 500e-6)\n            },\n        }\n\n    def pre_measurement_operation(self, reduced_ext_space):\n        iteration_dict = reduced_ext_space['dc_currents']\n\n        this_iteration_value = list(iteration_dict.values())[0]\n        print(f'{ this_iteration_value = }')\n        self.spi_dac.set_dac_current(self.dac, this_iteration_value)\nBy default every node every node is assigned a node.type attribute at the BaseNode class:\nself.type = simple_sweep\nThis attribute can be overwritten at the implementation of the class of each node. An example of a parameterized_sweep node type is Randomized_Benchmarking as each new iteration requires a the schedule to be recompiled with a different random seed.\nThe tergite_acl/scripts/node_supervisor.py is responcible to distinguish between each node variation.\n\nExamples of nodes requiring an external samplespace:\n\ncoupler_spectroscopy sweeps the dc_current which is set by the SPI rack not the cluster\nT1 sweeps a repetetion index to repeat the measurement many times\nrandomized_benchmarking sweeps different seeds. Although the seed is a schedule parameter, sweeping outside the schedule improves memory utilization."
  },
  {
    "objectID": "developer_guide/unit_tests.html",
    "href": "developer_guide/unit_tests.html",
    "title": "Unit tests for nodes",
    "section": "",
    "text": "The test should be created in a sub-folder of the node called tests.\nPlease organise the file using these sub-folders:\n\ndata: place here any data file that is needed to create test cases, while it is possible to mock the data. Feel free to add a text file explaing how the data was produced. Mocking data is also possible but it may be not practical for complex datasets.\nresults: create this folder to store your results, i.e. files that would be created by your analysis such as the plots. It should be empty, do not commit your results. To assure this is the case, add a file name .gitinore in the folder with this code:\n\n# Ignore everything in this directory\n*\n\n# But do not ignore this file\n!.gitignore     \n\n\n\nA good starting point, especially starting from scratch, it is to test for the class type, then that the inputs have the right formats and then move to some simple operation or trivial case. Build more complex cases from the simpler ones and exploits your tests to refector the code as needed. Try to test as many reasonable cases as possible, both successfull and not. Remeber to test for exceptions. We also suggest to develop using test drive development tecniques that will ensure a high test coverage and a good code structure. Yes, do not forget to keep refactoring to improve the code.\nYou can find some samples below taken from the cz_parametrisation node, where all objects are tested\nCurrently there is no way to differentiate from tests that require a QPU (i.e. measurments) and those that do not (i.e. analyses). Since the latter are simpler to write, start with those that, in general, are more likely to benefit from unit tests as there is much more logic in the analysis than in the measurment.\nIf you need to test some complex scenario, such as those involving sweep, it is probably easier to start writing code and tests from the lowest level and then compose those objects to hendle the more complex scenario considered.\n\n\n\n\n\ndef test_canCreateCorrectType():\n    c = CZ_Parametrization_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q15\"])\n    assert isinstance(c, CZ_Parametrization_Fix_Duration_Node)\n    assert isinstance(c, ScheduleNode)\nThe suggested very first test is to istantiate the class and make sure it has the correct type(s) following any inheritance.\n\n\n\ndef test_CanGetQubitsFromCouplers():\n    c = CZ_Parametrization_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q15\"])\n    assert c.all_qubits == [\"q14\", \"q15\"]\n    assert c.couplers == ['q14_q15']\nMake sure all inputs and their manipulations are correctly initialised in the constructor, in this case the qubits are taken from the coupler pair\n\n\n\ndef test_ValidationReturnErrorWithSameQubitCoupler():\n    with pytest.raises(ValueError):\n       CZ_Parametrization_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q14\"])\nInputs can be incorrect and should always be tested to avoid unexpected behaviour down the line which can be difficult to trace back to the origin. There are infinite number of possible errors, so it is impossible to cover them all, but at least the obvious ones should be considered. In this case a typical typing error with the couple have the same qubit twice.\n\n\n\n@pytest.fixture(autouse=True)\ndef setup_good_data():\n    os.environ[\"DATA_DIR\"] = str(Path(__file__).parent / \"results\")\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    return d14, d15, freqs, amps\nThis is an example of how to load data for testing the analysis of a node, please note that the specifics may change as we are about to change the dataset format at the time of writing this; however, the principle will be the same.\nThis data can then be used in multiple tests, for example:\ndef test_canGetMaxFromQ1(setup_good_data):\n    d14, d15, freqs, amps = setup_good_data\n    c = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    result = c.run_fitting()\n    indexBestFreq = np.where(freqs == result[0])[0]\n    indexBestAmp = np.where(amps == result[1])[0]\n    assert indexBestFreq[0] == 9\n    assert indexBestAmp[0] == 13\n\n\ndef test_canGetMinFromQ2(setup_good_data):\n    d14, d15, freqs, amps = setup_good_data\n    c = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    result = c.run_fitting()\n    indexBestFreq = np.where(freqs == result[0])[0]\n    indexBestAmp = np.where(amps == result[1])[0]\n    assert indexBestFreq[0] == 10\n    assert indexBestAmp[0] == 12\nThese two tests make sure that the return values are correct for the two qubits that are connected by the coupler.\n\n\n\ndef test_canPlotBad(setup_bad_data):\n    matplotlib.use(\"Agg\")\n    d14, d15, freqs, amps = setup_bad_data\n    c14 = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    result = c14.run_fitting()\n\n    figure_path = os.environ[\"DATA_DIR\"] + \"/Frequency_Amplitude_bad_q14.png\"\n    # Remove the file if it already exists\n    if os.path.exists(figure_path):\n        os.remove(figure_path)\n\n    fig, ax = plt.subplots(figsize=(15, 7), num=1)\n    plt.Axes\n    c14.plotter(ax)\n    fig.savefig(figure_path)\n    plt.close()\n\n    assert os.path.exists(figure_path)\n    from PIL import Image\n\n    with Image.open(figure_path) as img:\n        assert img.format == \"PNG\", \"File should be a PNG image\"\n\n    c15 = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    result = c15.run_fitting()\n\n    figure_path = os.environ[\"DATA_DIR\"] + \"/Frequency_Amplitude_bad_q15.png\"\n    # Remove the file if it already exists\n    if os.path.exists(figure_path):\n        os.remove(figure_path)\n\n    fig, ax = plt.subplots(figsize=(15, 7), num=1)\n    plt.Axes\n    c15.plotter(ax)\n    fig.savefig(figure_path)\n    plt.close()\n\n    assert os.path.exists(figure_path)\n    from PIL import Image\n\n    with Image.open(figure_path) as img:\n        assert img.format == \"PNG\", \"File should be a PNG image\"\nThis is an example on how to save files in the “results” sub-folder within the “tests” folder. The code make sure the expected file exists and has the correct format. The created files can be inspected by the developer. A possible extension would be to upload the images in the data folder and check the those produced by the test are identical.\n\n\n\n@pytest.fixture(autouse=True)\ndef setup_bad_data():\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_bad_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    q15Res = q15Ana.run_fitting()\n    return q14Res, q15Res\n\n\ndef test_combineBadResultsReturnNoValidPoint(setup_bad_data):\n    q14Res, q15Res = setup_bad_data\n    c = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n    r = c.are_frequencies_compatible()\n    assert r == False\n    r = c.are_amplitudes_compatible()\n    assert r == False\n    r = c.are_two_qubits_compatible()\n    assert r == False\nIn this example, the data produced is loaded from a file that has data that is not a good working point. Note that there two analyses are run in the setup; the combination is run in the test, so that, if needed in other tests, the data could be modified for specific cases. This is a failure test, making sure that bad inputs are not recognised as good working points.\n\n\n\ndef setup_data():\n    # It should be a single dataset, but we do not have one yet, so we loop over existing files\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    q15Res = q15Ana.run_fitting()\n    c1 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_bad_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs_bad = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps_bad = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(\n        d14, freqs_bad, amps_bad\n    )\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(\n        d15, freqs_bad, amps_bad\n    )\n    q15Res = q15Ana.run_fitting()\n    c2 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    dataset_path = (\n        Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp_2.hdf5\"\n    )\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs_2 = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps_2 = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs_2, amps_2)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs_2, amps_2)\n    q15Res = q15Ana.run_fitting()\n    c3 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    list_of_results = [(c1, 0.1), (c2, 0.2), (c3, 0.3)]\n    return list_of_results, freqs, amps, freqs_2, amps_2\nIn this case, three points are loaded and the analysis is run on each of them. The results are returned for each point and can be used in different tests, for example by removing elements in the list_results array."
  },
  {
    "objectID": "developer_guide/unit_tests.html#folder-structure",
    "href": "developer_guide/unit_tests.html#folder-structure",
    "title": "Unit tests for nodes",
    "section": "",
    "text": "The test should be created in a sub-folder of the node called tests.\nPlease organise the file using these sub-folders:\n\ndata: place here any data file that is needed to create test cases, while it is possible to mock the data. Feel free to add a text file explaing how the data was produced. Mocking data is also possible but it may be not practical for complex datasets.\nresults: create this folder to store your results, i.e. files that would be created by your analysis such as the plots. It should be empty, do not commit your results. To assure this is the case, add a file name .gitinore in the folder with this code:\n\n# Ignore everything in this directory\n*\n\n# But do not ignore this file\n!.gitignore"
  },
  {
    "objectID": "developer_guide/unit_tests.html#general-guideline-and-information",
    "href": "developer_guide/unit_tests.html#general-guideline-and-information",
    "title": "Unit tests for nodes",
    "section": "",
    "text": "A good starting point, especially starting from scratch, it is to test for the class type, then that the inputs have the right formats and then move to some simple operation or trivial case. Build more complex cases from the simpler ones and exploits your tests to refector the code as needed. Try to test as many reasonable cases as possible, both successfull and not. Remeber to test for exceptions. We also suggest to develop using test drive development tecniques that will ensure a high test coverage and a good code structure. Yes, do not forget to keep refactoring to improve the code.\nYou can find some samples below taken from the cz_parametrisation node, where all objects are tested\nCurrently there is no way to differentiate from tests that require a QPU (i.e. measurments) and those that do not (i.e. analyses). Since the latter are simpler to write, start with those that, in general, are more likely to benefit from unit tests as there is much more logic in the analysis than in the measurment.\nIf you need to test some complex scenario, such as those involving sweep, it is probably easier to start writing code and tests from the lowest level and then compose those objects to hendle the more complex scenario considered."
  },
  {
    "objectID": "developer_guide/unit_tests.html#example-tests",
    "href": "developer_guide/unit_tests.html#example-tests",
    "title": "Unit tests for nodes",
    "section": "",
    "text": "def test_canCreateCorrectType():\n    c = CZ_Parametrization_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q15\"])\n    assert isinstance(c, CZ_Parametrization_Fix_Duration_Node)\n    assert isinstance(c, ScheduleNode)\nThe suggested very first test is to istantiate the class and make sure it has the correct type(s) following any inheritance.\n\n\n\ndef test_CanGetQubitsFromCouplers():\n    c = CZ_Parametrization_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q15\"])\n    assert c.all_qubits == [\"q14\", \"q15\"]\n    assert c.couplers == ['q14_q15']\nMake sure all inputs and their manipulations are correctly initialised in the constructor, in this case the qubits are taken from the coupler pair\n\n\n\ndef test_ValidationReturnErrorWithSameQubitCoupler():\n    with pytest.raises(ValueError):\n       CZ_Parametrization_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q14\"])\nInputs can be incorrect and should always be tested to avoid unexpected behaviour down the line which can be difficult to trace back to the origin. There are infinite number of possible errors, so it is impossible to cover them all, but at least the obvious ones should be considered. In this case a typical typing error with the couple have the same qubit twice.\n\n\n\n@pytest.fixture(autouse=True)\ndef setup_good_data():\n    os.environ[\"DATA_DIR\"] = str(Path(__file__).parent / \"results\")\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    return d14, d15, freqs, amps\nThis is an example of how to load data for testing the analysis of a node, please note that the specifics may change as we are about to change the dataset format at the time of writing this; however, the principle will be the same.\nThis data can then be used in multiple tests, for example:\ndef test_canGetMaxFromQ1(setup_good_data):\n    d14, d15, freqs, amps = setup_good_data\n    c = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    result = c.run_fitting()\n    indexBestFreq = np.where(freqs == result[0])[0]\n    indexBestAmp = np.where(amps == result[1])[0]\n    assert indexBestFreq[0] == 9\n    assert indexBestAmp[0] == 13\n\n\ndef test_canGetMinFromQ2(setup_good_data):\n    d14, d15, freqs, amps = setup_good_data\n    c = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    result = c.run_fitting()\n    indexBestFreq = np.where(freqs == result[0])[0]\n    indexBestAmp = np.where(amps == result[1])[0]\n    assert indexBestFreq[0] == 10\n    assert indexBestAmp[0] == 12\nThese two tests make sure that the return values are correct for the two qubits that are connected by the coupler.\n\n\n\ndef test_canPlotBad(setup_bad_data):\n    matplotlib.use(\"Agg\")\n    d14, d15, freqs, amps = setup_bad_data\n    c14 = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    result = c14.run_fitting()\n\n    figure_path = os.environ[\"DATA_DIR\"] + \"/Frequency_Amplitude_bad_q14.png\"\n    # Remove the file if it already exists\n    if os.path.exists(figure_path):\n        os.remove(figure_path)\n\n    fig, ax = plt.subplots(figsize=(15, 7), num=1)\n    plt.Axes\n    c14.plotter(ax)\n    fig.savefig(figure_path)\n    plt.close()\n\n    assert os.path.exists(figure_path)\n    from PIL import Image\n\n    with Image.open(figure_path) as img:\n        assert img.format == \"PNG\", \"File should be a PNG image\"\n\n    c15 = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    result = c15.run_fitting()\n\n    figure_path = os.environ[\"DATA_DIR\"] + \"/Frequency_Amplitude_bad_q15.png\"\n    # Remove the file if it already exists\n    if os.path.exists(figure_path):\n        os.remove(figure_path)\n\n    fig, ax = plt.subplots(figsize=(15, 7), num=1)\n    plt.Axes\n    c15.plotter(ax)\n    fig.savefig(figure_path)\n    plt.close()\n\n    assert os.path.exists(figure_path)\n    from PIL import Image\n\n    with Image.open(figure_path) as img:\n        assert img.format == \"PNG\", \"File should be a PNG image\"\nThis is an example on how to save files in the “results” sub-folder within the “tests” folder. The code make sure the expected file exists and has the correct format. The created files can be inspected by the developer. A possible extension would be to upload the images in the data folder and check the those produced by the test are identical.\n\n\n\n@pytest.fixture(autouse=True)\ndef setup_bad_data():\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_bad_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    q15Res = q15Ana.run_fitting()\n    return q14Res, q15Res\n\n\ndef test_combineBadResultsReturnNoValidPoint(setup_bad_data):\n    q14Res, q15Res = setup_bad_data\n    c = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n    r = c.are_frequencies_compatible()\n    assert r == False\n    r = c.are_amplitudes_compatible()\n    assert r == False\n    r = c.are_two_qubits_compatible()\n    assert r == False\nIn this example, the data produced is loaded from a file that has data that is not a good working point. Note that there two analyses are run in the setup; the combination is run in the test, so that, if needed in other tests, the data could be modified for specific cases. This is a failure test, making sure that bad inputs are not recognised as good working points.\n\n\n\ndef setup_data():\n    # It should be a single dataset, but we do not have one yet, so we loop over existing files\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    q15Res = q15Ana.run_fitting()\n    c1 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_bad_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs_bad = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps_bad = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(\n        d14, freqs_bad, amps_bad\n    )\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(\n        d15, freqs_bad, amps_bad\n    )\n    q15Res = q15Ana.run_fitting()\n    c2 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    dataset_path = (\n        Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp_2.hdf5\"\n    )\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs_2 = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps_2 = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs_2, amps_2)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs_2, amps_2)\n    q15Res = q15Ana.run_fitting()\n    c3 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    list_of_results = [(c1, 0.1), (c2, 0.2), (c3, 0.3)]\n    return list_of_results, freqs, amps, freqs_2, amps_2\nIn this case, three points are loaded and the analysis is run on each of them. The results are returned for each point and can be used in different tests, for example by removing elements in the list_results array."
  },
  {
    "objectID": "developer_guide/node_types.html",
    "href": "developer_guide/node_types.html",
    "title": "Node types",
    "section": "",
    "text": "Node types\nThe execution of most of the nodes consists of a single schedule compilation, a single measurement and a single post-processing. Although for most of the nodes this workflow suffices, there are exceptions while this workflow can become limiting in more advanced implementations.\nTo allow greater flexibilty in the node implementations the nodes are categorized:\n\nAccording to whether they compile once or multiple times:\n\nnode.type = simple_sweep: if it compiles once\nnode.type = parameterized_sweep: if it compiles multiple times\n\nAccording to whether the sweeping parameters are swept whithin the schedule or not:\n\nThere is only node.schedule_samplespace if the sweeping takes place whithin the schedule\nThere are both node.schedule_samplespace and node.external_samplespace if there are sweeping parameters outside of the scedule. For example the coupler_spectroscopy node sweeps the dc_current outside of the schedule:\n\n\n  class Coupler_Spectroscopy_Node(BaseNode):\n    measurement_obj = Two_Tones_Multidim\n    analysis_obj = CouplerSpectroscopyAnalysis\n\n    def __init__(self, name: str, all_qubits: list[str], couplers, **schedule_keywords):\n        super().__init__(name, all_qubits, **schedule_keywords)\n        self.couplers = couplers\n        self.redis_field = ['parking_current']\n        self.all_qubits = self.coupled_qubits\n\n        self.schedule_samplespace = {\n            'spec_frequencies': {\n                qubit: qubit_samples(qubit) for qubit in self.all_qubits\n            }\n        }\n\n        self.external_samplespace = {\n            'dc_currents': {\n                self.coupler: np.arange(-2.5e-3, 2.5e-3, 500e-6)\n            },\n        }\n\n    def pre_measurement_operation(self, reduced_ext_space):\n        iteration_dict = reduced_ext_space['dc_currents']\n\n        this_iteration_value = list(iteration_dict.values())[0]\n        print(f'{ this_iteration_value = }')\n        self.spi_dac.set_dac_current(self.dac, this_iteration_value)\nBy default every node every node is assigned a node.type attribute at the BaseNode class:\nself.type = simple_sweep\nThis attribute can be overwritten at the implementation of the class of each node. An example of a parameterized_sweep node type is Randomized_Benchmarking as each new iteration requires a the schedule to be recompiled with a different random seed.\nThe tergite_acl/scripts/node_supervisor.py is responcible to distinguish between each node variation.\n\nExamples of nodes requiring an external samplespace:\n\ncoupler_spectroscopy sweeps the dc_current which is set by the SPI rack not the cluster\nT1 sweeps a repetetion index to repeat the measurement many times\nrandomized_benchmarking sweeps different seeds. Although the seed is a schedule parameter, sweeping outside the schedule improves memory utilization."
  },
  {
    "objectID": "user_guide/configuration_files.html",
    "href": "user_guide/configuration_files.html",
    "title": "Configuration Files",
    "section": "",
    "text": "Here the connection is made between the Qblox cluster physical ports and clocks to the qubits and couplers of the QPU.\nThe file must be placed at the directory configs/\nGiven a .csv file after a mixer calibration the function ... can create instantly the corresponding JSON file.\n\n\nWith quantify-scheduler 0.18.0 there is been introduced a new way on how to structure the hardware configuration file. If you are having a hardware configuration file, that is structured using the old way, you can use the following script to migrate it to the new structure.\npython tergite_autocalibration/scripts/migrate_blox_hardware_configuration.py &lt;PATH_TO_HW_CONFIG&gt;\n\n\n\n\nHere we set reasonable initial parameters\n\n\n\nHere we input the initial values for the resonator and qubits frequencies, assumed that they have been acquired through VNA measurements, design targets or previous manual measurements. /tergite_acl/config/VNA_values.py",
    "crumbs": [
      "Home",
      "User Guide",
      "Configuration Files"
    ]
  },
  {
    "objectID": "user_guide/configuration_files.html#hardware-configuration-file-json",
    "href": "user_guide/configuration_files.html#hardware-configuration-file-json",
    "title": "Configuration Files",
    "section": "",
    "text": "Here the connection is made between the Qblox cluster physical ports and clocks to the qubits and couplers of the QPU.\nThe file must be placed at the directory configs/\nGiven a .csv file after a mixer calibration the function ... can create instantly the corresponding JSON file.\n\n\nWith quantify-scheduler 0.18.0 there is been introduced a new way on how to structure the hardware configuration file. If you are having a hardware configuration file, that is structured using the old way, you can use the following script to migrate it to the new structure.\npython tergite_autocalibration/scripts/migrate_blox_hardware_configuration.py &lt;PATH_TO_HW_CONFIG&gt;",
    "crumbs": [
      "Home",
      "User Guide",
      "Configuration Files"
    ]
  },
  {
    "objectID": "user_guide/configuration_files.html#device-configuration-file-toml",
    "href": "user_guide/configuration_files.html#device-configuration-file-toml",
    "title": "Configuration Files",
    "section": "",
    "text": "Here we set reasonable initial parameters",
    "crumbs": [
      "Home",
      "User Guide",
      "Configuration Files"
    ]
  },
  {
    "objectID": "user_guide/configuration_files.html#vna-resonator-qubit-frequencies",
    "href": "user_guide/configuration_files.html#vna-resonator-qubit-frequencies",
    "title": "Configuration Files",
    "section": "",
    "text": "Here we input the initial values for the resonator and qubits frequencies, assumed that they have been acquired through VNA measurements, design targets or previous manual measurements. /tergite_acl/config/VNA_values.py",
    "crumbs": [
      "Home",
      "User Guide",
      "Configuration Files"
    ]
  },
  {
    "objectID": "nodes/resonator_spectroscopy_node.html",
    "href": "nodes/resonator_spectroscopy_node.html",
    "title": "Resonator Spectroscopy in Qubit-Resonator Interaction Analysis",
    "section": "",
    "text": "Resonator Spectroscopy in Qubit-Resonator Interaction Analysis\nResonator spectroscopy is a powerful tool for studying the interaction between qubits and resonators. By probing the resonator’s response across various frequencies, we can extract critical parameters, such as the resonant frequency and Q-factor, which are essential for optimizing qubit-resonator interactions.\n\nClass: Resonator_Spectroscopy\nThe Resonator_Spectroscopy class is designed to conduct resonator spectroscopy for transmon qubits. It takes as input a dictionary of transmon qubits and their respective states.\n\nMethod: schedule\nThe schedule method generates a schedule to perform resonator spectroscopy. This process includes:\n\nClock Initialization: Initializes the clocks for each qubit based on the specified qubit state.\nQubit Reset: Resets the qubit to a known state.\nFrequency Probing: Applies a square pulse at various frequencies.\nSignal Measurement: Measures the response signal, capturing data on the resonator’s behavior.\n\n\n\n\nClass: ResonatorSpectroscopyQubitAnalysis\nThe ResonatorSpectroscopyQubitAnalysis class is used for analyzing resonator spectroscopy data, enabling the extraction of the resonant frequency and Q-factor. This class takes as parameters:\n\nqubit_name: A string representing the qubit under measurement.\nredis_fields: The directory for data storage.\n\n\nMethod: analyse_qubit\nThe analyse_qubit method processes and fits the data to determine the resonator’s resonant frequency and loaded Q-factor. The analyse_qubitfit should resemble a negative Gaussian distribution.\n\n\n\nOutput: xarray.Dataset\nThe dataset returned by this analysis is an xarray.Dataset, which includes: - Frequency Sweep Data: The set of frequencies used during the sweep. - Transmission Response: The measured response of the resonator at each frequency.\nThis dataset provides essential insights into the resonator’s properties, allowing for precise tuning of qubit-resonator interactions.",
    "crumbs": [
      "Home",
      "Node Library",
      "Resonator spectroscopy"
    ]
  },
  {
    "objectID": "nodes/resonator_spectroscopy_node.html#resonator-spectroscopy-in-qubit-resonator-interaction-analysis",
    "href": "nodes/resonator_spectroscopy_node.html#resonator-spectroscopy-in-qubit-resonator-interaction-analysis",
    "title": "Tergite Automatic Calibration",
    "section": "",
    "text": "Resonator spectroscopy is a powerful tool for studying the interaction between qubits and resonators. By probing the resonator’s response across various frequencies, we can extract critical parameters, such as the resonant frequency and Q-factor, which are essential for optimizing qubit-resonator interactions.\n\n\nThe Resonator_Spectroscopy class is designed to conduct resonator spectroscopy for transmon qubits. It takes as input a dictionary of transmon qubits and their respective states.\n\n\nThe schedule method generates a schedule to perform resonator spectroscopy. This process includes:\n\nClock Initialization: Initializes the clocks for each qubit based on the specified qubit state.\nQubit Reset: Resets the qubit to a known state.\nFrequency Probing: Applies a square pulse at various frequencies.\nSignal Measurement: Measures the response signal, capturing data on the resonator’s behavior.\n\n\n\n\n\nThe ResonatorSpectroscopyQubitAnalysis class is used for analyzing resonator spectroscopy data, enabling the extraction of the resonant frequency and Q-factor. This class takes as parameters:\n\nqubit_name: A string representing the qubit under measurement.\nredis_fields: The directory for data storage.\n\n\n\nThe analyse_qubit method processes and fits the data to determine the resonator’s resonant frequency and loaded Q-factor. The analyse_qubitfit should resamble a negative Gaussian distribution.\n\n\n\n\nThe dataset returned by this analysis is an xarray.Dataset, which includes: - Frequency Sweep Data: The set of frequencies used during the sweep. - Transmission Response: The measured response of the resonator at each frequency.\nThis dataset provides essential insights into the resonator’s properties, allowing for precise tuning of qubit-resonator interactions.",
    "crumbs": [
      "Home",
      "Node Library",
      "Resonator spectroscopy"
    ]
  },
  {
    "objectID": "user_guide/operation.html",
    "href": "user_guide/operation.html",
    "title": "Operation",
    "section": "",
    "text": "The package ships with a command line interface to solve some common tasks that appear quite often.\nIn the following there are a number of useful commands, but if you want to find out all commands use: acli --help\nTo delete all redis entries: acli node reset -a\nTo reset a particular node: acli node reset -n &lt;nodename&gt;\nFor example to reset the node rabi_oscillations run the command:\nacli node reset -n rabi_oscillations\nTo start a new calibration sequence according to the configuration files:\npython tergite_acl/scripts/calibration_supervisor.py\nor\nacli calibration start\n\n\nThis document provides an overview of the Command Line Interface (CLI) options and their functionalities.\n\n\nThe autocalibration CLI is organized into several main command groups:\n\ncluster: Handle operations related to the cluster\nnode: Handle operations related to the node\ngraph: Handle operations related to the calibration graph\ncalibration: Handle operations related to the calibration supervisor\njoke: Handle operations related to the well-being of the user\n\n\n\n\n\n\nReboots the cluster.\nUsage:\nacli cluster reboot\nThis command will prompt for confirmation before rebooting the cluster, as it can interrupt ongoing measurements.\n\n\n\n\n\n\nResets all parameters in Redis for the specified node(s).\nUsage:\nacli node reset [OPTIONS]\nOptions:\n\n-n, --name TEXT: Name of the node to be reset in Redis (e.g., resonator_spectroscopy)\n-a, --all: Reset all nodes\n-f, --from_node TEXT: Reset all nodes from the specified node in the chain\n\n\n\n\n\n\n\nPlots the calibration graph to the user-specified target node in topological order.\nUsage:\nacli graph plot\nThis command visualizes the calibration graph using an arrow chart.\n\n\n\n\n\n\nStarts the calibration supervisor.\nUsage:\nacli calibration start [OPTIONS]\nOptions:\n\n-c TEXT: Cluster IP address (if not set, it will use CLUSTER_IP from the .env file)\n-r TEXT: Rerun an analysis (specify the path to the dataset folder)\n-n, --name TEXT: Specify the node type to rerun (works only with -r option)\n--push: Push a backend to an MSS specified in MSS_MACHINE_ROOT_URL in the .env file\n--browser: Will open the dataset browser in the background and plot the measurement results live\n\n\n\n\n\n\n\nStarts the dataset browser.\nUsage:\nacli browser --datadir [OPTIONS]\nOptions:\n\n--datadir PATH: Folder to take the plot data from\n--liveplotting: Whether plots should be updated in real time (default: False)\n--log-level INT: Log-level as in the Python logging package to be used in the logs (default: 30)\n\n\n\n\n\n\n\nPrints a random joke to lighten the mood.\nUsage:\nacli joke\nThis command fetches and displays a random joke, excluding potentially offensive content.\n\n\n\n\n\nThe CLI uses the Click library for command-line interface creation.\nSome commands may require additional configuration or environment variables to be set.\nWhen using the -r option for rerunning analysis, make sure to also specify the node name using -n.\n\nFor more detailed information about each command, use the --help option with any command or subcommand.\n\n\n\n\nFor each calibration node: compilation -&gt; execution -&gt; post-processing -&gt; redis updating\n\n\n\nDatasets are stored in data_directory Can be browsed with the dataset browser (coming soon)\n\n\n\nWhen submitting contributions, please prepend your commit messages with: fix: for bug fixes feat: for introducing a new feature (e.g. a new measurement node or a new analysis class) chore: for refactoring changes or any change that doesn’t affect the functionality of the code docs: for changes in the README, docstrings etc test: or dev: for testing or development changes (e.g. profiling scripts)",
    "crumbs": [
      "Home",
      "User Guide",
      "Operation"
    ]
  },
  {
    "objectID": "user_guide/operation.html#cli",
    "href": "user_guide/operation.html#cli",
    "title": "Operation",
    "section": "",
    "text": "This document provides an overview of the Command Line Interface (CLI) options and their functionalities.\n\n\nThe autocalibration CLI is organized into several main command groups:\n\ncluster: Handle operations related to the cluster\nnode: Handle operations related to the node\ngraph: Handle operations related to the calibration graph\ncalibration: Handle operations related to the calibration supervisor\njoke: Handle operations related to the well-being of the user\n\n\n\n\n\n\nReboots the cluster.\nUsage:\nacli cluster reboot\nThis command will prompt for confirmation before rebooting the cluster, as it can interrupt ongoing measurements.\n\n\n\n\n\n\nResets all parameters in Redis for the specified node(s).\nUsage:\nacli node reset [OPTIONS]\nOptions:\n\n-n, --name TEXT: Name of the node to be reset in Redis (e.g., resonator_spectroscopy)\n-a, --all: Reset all nodes\n-f, --from_node TEXT: Reset all nodes from the specified node in the chain\n\n\n\n\n\n\n\nPlots the calibration graph to the user-specified target node in topological order.\nUsage:\nacli graph plot\nThis command visualizes the calibration graph using an arrow chart.\n\n\n\n\n\n\nStarts the calibration supervisor.\nUsage:\nacli calibration start [OPTIONS]\nOptions:\n\n-c TEXT: Cluster IP address (if not set, it will use CLUSTER_IP from the .env file)\n-r TEXT: Rerun an analysis (specify the path to the dataset folder)\n-n, --name TEXT: Specify the node type to rerun (works only with -r option)\n--push: Push a backend to an MSS specified in MSS_MACHINE_ROOT_URL in the .env file\n--browser: Will open the dataset browser in the background and plot the measurement results live\n\n\n\n\n\n\n\nStarts the dataset browser.\nUsage:\nacli browser --datadir [OPTIONS]\nOptions:\n\n--datadir PATH: Folder to take the plot data from\n--liveplotting: Whether plots should be updated in real time (default: False)\n--log-level INT: Log-level as in the Python logging package to be used in the logs (default: 30)\n\n\n\n\n\n\n\nPrints a random joke to lighten the mood.\nUsage:\nacli joke\nThis command fetches and displays a random joke, excluding potentially offensive content.\n\n\n\n\n\nThe CLI uses the Click library for command-line interface creation.\nSome commands may require additional configuration or environment variables to be set.\nWhen using the -r option for rerunning analysis, make sure to also specify the node name using -n.\n\nFor more detailed information about each command, use the --help option with any command or subcommand.",
    "crumbs": [
      "Home",
      "User Guide",
      "Operation"
    ]
  },
  {
    "objectID": "user_guide/operation.html#structure",
    "href": "user_guide/operation.html#structure",
    "title": "Operation",
    "section": "",
    "text": "For each calibration node: compilation -&gt; execution -&gt; post-processing -&gt; redis updating",
    "crumbs": [
      "Home",
      "User Guide",
      "Operation"
    ]
  },
  {
    "objectID": "user_guide/operation.html#data-browsing",
    "href": "user_guide/operation.html#data-browsing",
    "title": "Operation",
    "section": "",
    "text": "Datasets are stored in data_directory Can be browsed with the dataset browser (coming soon)",
    "crumbs": [
      "Home",
      "User Guide",
      "Operation"
    ]
  },
  {
    "objectID": "user_guide/operation.html#development",
    "href": "user_guide/operation.html#development",
    "title": "Operation",
    "section": "",
    "text": "When submitting contributions, please prepend your commit messages with: fix: for bug fixes feat: for introducing a new feature (e.g. a new measurement node or a new analysis class) chore: for refactoring changes or any change that doesn’t affect the functionality of the code docs: for changes in the README, docstrings etc test: or dev: for testing or development changes (e.g. profiling scripts)",
    "crumbs": [
      "Home",
      "User Guide",
      "Operation"
    ]
  },
  {
    "objectID": "developer_guide/new_node_creation.html",
    "href": "developer_guide/new_node_creation.html",
    "title": "To create a new node:",
    "section": "",
    "text": "in the file tergite_autocalibration/lib/node_factory.py expand the dictionary self.node_implementations with a new entry: The key should be a string of the node name and the value should be the object that contains the implementation details. This object should be imported from either tergite_autocalibration/lib/nodes/qubit_control_nodes.py, tergite_autocalibration/lib/nodes/coupler_nodes.py, tergite_autocalibration/lib/nodes/readout_nodes.py or tergite_autocalibration/lib/nodes/characterization_nodes.py\nIn the file tergite_autocalibration/lib/nodes/graph.py in the list graph_dependencies insert the edges that describe the position of the new node in the Directed Acyclic Graph. There are two entries required (or one entry if the new node is the last on its path):\n\n\n('previous_node','new_node')\n('new_node', 'next_node')\n\n\nIn the tergite_autocalibration/config/device_config.toml set the quantity of interest at nan value\n\n\n\nEach node implementation object should contain a reference to the measurement object, the analysis object, the list of redis fields that the analysis updates and the samplespace of the measurement. For example on the Rabi Rabi Oscillations Node:\nclass Rabi_Oscillations_Node(BaseNode):\n    measurement_obj = Rabi_Oscillations\n    analysis_obj = RabiAnalysis\n\n    def __init__(self, name: str, all_qubits: list[str], **node_dictionary):\n        super().__init__(name, all_qubits, **node_dictionary)\n        self.redis_field = ['rxy:amp180']\n        self.schedule_samplespace = {\n            'mw_amplitudes': {\n                qubit: np.linspace(0.002, 0.80, 101) for qubit in self.all_qubits\n            }\n        }\n\n\nThe measurement_obj is imported from tergite_autocalibration/lib/calibration_schedules/ and contains the class that generates the appropriate measurement schedule. To initialize we require a dicttionary of the extended transmons:\ntransmons: dict[str, ExtendedTransmon]\nIt must contain a method called schedule_function that expects the node.samplespace as input and returns the complete schedule.\n\n\n\nThe analysis_obj is imported from tergite_autocalibration/lib/analysis/ and contains the class that perform the analysis for a single qubit. It must contain a run_fitting method and a plotter method\n\n\n\nNodes are divided in two distict categories:\n\nsimple_sweep: where the Quantify Schedule is compiled only once\nparameterized_sweep: where the node requires multiple iterations and each iteration requires a new recompilation.\n\nFurthermore each node can expect two types of samplespaces:\n\nschedule_samplespace: parameter values to be input to the schedule function\nexternal_samplespace: parameter values for quantities that are not set during a schedule\n\n\n\n\nPlease create a new node in a separate folder, so that is is clearer what the new node is meant to do Add an empty init.py file to the folder, this is needed to mark the folder as part of the packege and allow imports from these folders\nTo keep the code clean, please create sub-folders following this scheme:\n\ntests: create unit tests in here, more on tests in href: unit_tests\nutils: any utility class, such as enum, errors and similar classes should be placed here\n\n\n\n\nPlease add your node to the list of available nodes in this Documentation.\nAdd ny relevant information on how to use your node, dependencies and reference to publication as needed for allowing other to use the code you developed.\nDetails on the implementation on the Node types section."
  },
  {
    "objectID": "developer_guide/new_node_creation.html#node-implementation-object",
    "href": "developer_guide/new_node_creation.html#node-implementation-object",
    "title": "To create a new node:",
    "section": "",
    "text": "Each node implementation object should contain a reference to the measurement object, the analysis object, the list of redis fields that the analysis updates and the samplespace of the measurement. For example on the Rabi Rabi Oscillations Node:\nclass Rabi_Oscillations_Node(BaseNode):\n    measurement_obj = Rabi_Oscillations\n    analysis_obj = RabiAnalysis\n\n    def __init__(self, name: str, all_qubits: list[str], **node_dictionary):\n        super().__init__(name, all_qubits, **node_dictionary)\n        self.redis_field = ['rxy:amp180']\n        self.schedule_samplespace = {\n            'mw_amplitudes': {\n                qubit: np.linspace(0.002, 0.80, 101) for qubit in self.all_qubits\n            }\n        }\n\n\nThe measurement_obj is imported from tergite_autocalibration/lib/calibration_schedules/ and contains the class that generates the appropriate measurement schedule. To initialize we require a dicttionary of the extended transmons:\ntransmons: dict[str, ExtendedTransmon]\nIt must contain a method called schedule_function that expects the node.samplespace as input and returns the complete schedule.\n\n\n\nThe analysis_obj is imported from tergite_autocalibration/lib/analysis/ and contains the class that perform the analysis for a single qubit. It must contain a run_fitting method and a plotter method\n\n\n\nNodes are divided in two distict categories:\n\nsimple_sweep: where the Quantify Schedule is compiled only once\nparameterized_sweep: where the node requires multiple iterations and each iteration requires a new recompilation.\n\nFurthermore each node can expect two types of samplespaces:\n\nschedule_samplespace: parameter values to be input to the schedule function\nexternal_samplespace: parameter values for quantities that are not set during a schedule\n\n\n\n\nPlease create a new node in a separate folder, so that is is clearer what the new node is meant to do Add an empty init.py file to the folder, this is needed to mark the folder as part of the packege and allow imports from these folders\nTo keep the code clean, please create sub-folders following this scheme:\n\ntests: create unit tests in here, more on tests in href: unit_tests\nutils: any utility class, such as enum, errors and similar classes should be placed here\n\n\n\n\nPlease add your node to the list of available nodes in this Documentation.\nAdd ny relevant information on how to use your node, dependencies and reference to publication as needed for allowing other to use the code you developed.\nDetails on the implementation on the Node types section."
  },
  {
    "objectID": "developer-guide/unit_tests.html",
    "href": "developer-guide/unit_tests.html",
    "title": "Unit tests",
    "section": "",
    "text": "When testing software, there are several ways to test whether it is working. Amongst others such as system integration tests or usability scores, there are unit tests. A unit test is meant to confirm whether a small unit of the code is working such as a function or a class. The idea behind unit tests though, is to create tiny little tests for each function, that checks how it handles:\n\nNormal case: The things that you would expect a function to do e.g. if you have addition, and you add natural numbers.\nEdge cases: In the example with the addition this would be e.g. whether it correctly adds zero or would subtract if there is a negative number.\nFail cases: Let us say you have addition, and you are adding a number such as 20 with the string “hello”. This should fail.\n\nHaving thought about the test cases and possible scenarios will help to get a better understanding what the code does, also, it will make the code more robust. In our code base we are having a pipeline that will automatically run all tests as soon as someone wants to merge to the common branches. There are two locations where tests are stored:\n\nIn the folder for tests: This is a folder on the main level of the repository where more general tests for the whole framework go.\nIn the folder of each node: This is to test only the node itself. The tests are added directly to the node module.\n\nSince it happens more often that one will write tests for the node itself, in the following, there will be a section explaining how to do it on an example node.\n\n\nThese instructions will go step-by-step through how to create meaningful test cases for a node.\n\nOverview about the folder structure\nSpecific advices on how to test nodes\nExamples on how to test the analysis function of a node\n\nIf you are more a person that learns from the code rather than from a tutorial, please take a look at an easy node e.g. resonator spectroscopy and try to run and understand the test cases.\n\n\nThe test should be created in a sub-folder of the node called tests.\nPlease organise the file using these sub-folders:\n\ndata: place here any data file that is needed to create test cases, while it is possible to mock the data. Feel free to add a text file explaining how the data was produced.Mocking data is also possible, but it may be not practical for complex datasets.\nresults: create this folder to store your results, i.e. files that would be created by your analysis such as the plots. It should be empty, do not commit your results. To assure this is the case, add a file name .gitignore in the folder with this code:\n\n# Ignore everything in this directory\n*\n\n# But do not ignore this file\n!.gitignore     \n\n\n\nA good starting point, especially starting from scratch, it is to test for the class type, then that the inputs have the right formats and then move to some simple operation or trivial case. Build more complex cases from the simpler ones and exploits your tests to refactor the code as needed. Try to test as many reasonable cases as possible, both successfully and not. Remember to test for exceptions. We also suggest to develop using test drive development techniques that will ensure a high test coverage and a good code structure. Yes, do not forget to keep refactoring to improve the code.\nYou can find some samples below taken from the cz_parametrisation node, where all objects are tested\nCurrently, there is no way to differentiate from tests that require a QPU (i.e. measurements) and those that do not ( i.e. analyses). Since the latter are simpler to write, start with those that, in general, are more likely to benefit from unit tests as there is much more logic in the analysis than in the measurement.\nIf you need to test some complex scenario, such as those involving sweep, it is probably easier to start writing code and tests from the lowest level and then compose those objects to handle the more complex scenario considered.\n\n\n\nTest class type\ndef test_canCreateCorrectType():\n    c = CZ_Parametrization_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q15\"])\n    assert isinstance(c, CZ_Parametrization_Fix_Duration_Node)\n    assert isinstance(c, ScheduleNode)\nThe suggested very first test is to instantiate the class and make sure it has the correct type(s) following any inheritance.\nTest input parameters\ndef test_CanGetQubitsFromCouplers():\n    c = CZ_Parametrization_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q15\"])\n    assert c.all_qubits == [\"q14\", \"q15\"]\n    assert c.couplers == ['q14_q15']\nMake sure all inputs and their manipulations are correctly initialised in the constructor, in this case the qubits are taken from the coupler pair\nTest exception\ndef test_ValidationReturnErrorWithSameQubitCoupler():\n    with pytest.raises(ValueError):\n       CZ_Parametrization_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q14\"])\nInputs can be incorrect and should always be tested to avoid unexpected behaviour down the line which can be difficult to trace back to the origin. There are infinite number of possible errors, so it is impossible to cover them all, but at least the obvious ones should be considered. In this case a typical typing error with the couple have the same qubit twice.\nTest with data\n@pytest.fixture(autouse=True)\ndef setup_good_data():\n    os.environ[\"DATA_DIR\"] = str(Path(__file__).parent / \"results\")\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    return d14, d15, freqs, amps\nThis is an example of how to load data for testing the analysis of a node, please note that the specifics may change as we are about to change the dataset format at the time of writing this; however, the principle will be the same.\nThis data can then be used in multiple tests, for example:\ndef test_canGetMaxFromQ1(setup_good_data):\n    d14, d15, freqs, amps = setup_good_data\n    c = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    result = c.run_fitting()\n    indexBestFreq = np.where(freqs == result[0])[0]\n    indexBestAmp = np.where(amps == result[1])[0]\n    assert indexBestFreq[0] == 9\n    assert indexBestAmp[0] == 13\n\n\ndef test_canGetMinFromQ2(setup_good_data):\n    d14, d15, freqs, amps = setup_good_data\n    c = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    result = c.run_fitting()\n    indexBestFreq = np.where(freqs == result[0])[0]\n    indexBestAmp = np.where(amps == result[1])[0]\n    assert indexBestFreq[0] == 10\n    assert indexBestAmp[0] == 12\nThese two tests make sure that the return values are correct for the two qubits that are connected by the coupler.\nTest creating of images from plotter\ndef test_canPlotBad(setup_bad_data):\n    matplotlib.use(\"Agg\")\n    d14, d15, freqs, amps = setup_bad_data\n    c14 = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    result = c14.run_fitting()\n\n    figure_path = os.environ[\"DATA_DIR\"] + \"/Frequency_Amplitude_bad_q14.png\"\n    # Remove the file if it already exists\n    if os.path.exists(figure_path):\n        os.remove(figure_path)\n\n    fig, ax = plt.subplots(figsize=(15, 7), num=1)\n    plt.Axes\n    c14.plotter(ax)\n    fig.savefig(figure_path)\n    plt.close()\n\n    assert os.path.exists(figure_path)\n    from PIL import Image\n\n    with Image.open(figure_path) as img:\n        assert img.format == \"PNG\", \"File should be a PNG image\"\n\n    c15 = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    result = c15.run_fitting()\n\n    figure_path = os.environ[\"DATA_DIR\"] + \"/Frequency_Amplitude_bad_q15.png\"\n    # Remove the file if it already exists\n    if os.path.exists(figure_path):\n        os.remove(figure_path)\n\n    fig, ax = plt.subplots(figsize=(15, 7), num=1)\n    plt.Axes\n    c15.plotter(ax)\n    fig.savefig(figure_path)\n    plt.close()\n\n    assert os.path.exists(figure_path)\n    from PIL import Image\n\n    with Image.open(figure_path) as img:\n        assert img.format == \"PNG\", \"File should be a PNG image\"\nThis is an example on how to save files in the “results” sub-folder within the “tests” folder. The code make sure the expected file exists and has the correct format. The created files can be inspected by the developer. A possible extension would be to upload the images in the data folder and check the those produced by the test are identical.\nComplex dataset for comparing results\n@pytest.fixture(autouse=True)\ndef setup_bad_data():\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_bad_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    q15Res = q15Ana.run_fitting()\n    return q14Res, q15Res\n\n\ndef test_combineBadResultsReturnNoValidPoint(setup_bad_data):\n    q14Res, q15Res = setup_bad_data\n    c = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n    r = c.are_frequencies_compatible()\n    assert r == False\n    r = c.are_amplitudes_compatible()\n    assert r == False\n    r = c.are_two_qubits_compatible()\n    assert r == False\nIn this example, the data produced is loaded from a file that has data that is not a good working point. Note that there two analyses are run in the setup; the combination is run in the test, so that, if needed in other tests, the data could be modified for specific cases. This is a failure test, making sure that bad inputs are not recognised as good working points.\nEven more complex setup\ndef setup_data():\n    # It should be a single dataset, but we do not have one yet, so we loop over existing files\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    q15Res = q15Ana.run_fitting()\n    c1 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_bad_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs_bad = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps_bad = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(\n        d14, freqs_bad, amps_bad\n    )\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(\n        d15, freqs_bad, amps_bad\n    )\n    q15Res = q15Ana.run_fitting()\n    c2 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    dataset_path = (\n        Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp_2.hdf5\"\n    )\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs_2 = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps_2 = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs_2, amps_2)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs_2, amps_2)\n    q15Res = q15Ana.run_fitting()\n    c3 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    list_of_results = [(c1, 0.1), (c2, 0.2), (c3, 0.3)]\n    return list_of_results, freqs, amps, freqs_2, amps_2\nIn this case, three points are loaded and the analysis is run on each of them. The results are returned for each point and can be used in different tests, for example by removing elements in the list_results array.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Unit tests"
    ]
  },
  {
    "objectID": "developer-guide/unit_tests.html#folder-structure",
    "href": "developer-guide/unit_tests.html#folder-structure",
    "title": "Unit tests for nodes",
    "section": "",
    "text": "The test should be created in a sub-folder of the node called tests.\nPlease organise the file using these sub-folders:\n\ndata: place here any data file that is needed to create test cases, while it is possible to mock the data. Feel free to add a text file explaing how the data was produced. Mocking data is also possible but it may be not practical for complex datasets.\nresults: create this folder to store your results, i.e. files that would be created by your analysis such as the plots. It should be empty, do not commit your results. To assure this is the case, add a file name .gitinore in the folder with this code:\n\n# Ignore everything in this directory\n*\n\n# But do not ignore this file\n!.gitignore",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Unit tests"
    ]
  },
  {
    "objectID": "developer-guide/unit_tests.html#general-guideline-and-information",
    "href": "developer-guide/unit_tests.html#general-guideline-and-information",
    "title": "Unit tests for nodes",
    "section": "",
    "text": "A good starting point, especially starting from scratch, it is to test for the class type, then that the inputs have the right formats and then move to some simple operation or trivial case. Build more complex cases from the simpler ones and exploits your tests to refector the code as needed. Try to test as many reasonable cases as possible, both successfull and not. Remeber to test for exceptions. We also suggest to develop using test drive development tecniques that will ensure a high test coverage and a good code structure. Yes, do not forget to keep refactoring to improve the code.\nYou can find some samples below taken from the cz_parametrisation node, where all objects are tested\nCurrently there is no way to differentiate from tests that require a QPU (i.e. measurments) and those that do not (i.e. analyses). Since the latter are simpler to write, start with those that, in general, are more likely to benefit from unit tests as there is much more logic in the analysis than in the measurment.\nIf you need to test some complex scenario, such as those involving sweep, it is probably easier to start writing code and tests from the lowest level and then compose those objects to hendle the more complex scenario considered.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Unit tests"
    ]
  },
  {
    "objectID": "developer-guide/unit_tests.html#example-tests",
    "href": "developer-guide/unit_tests.html#example-tests",
    "title": "Unit tests for nodes",
    "section": "",
    "text": "def test_canCreateCorrectType():\n    c = CZ_Parametrization_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q15\"])\n    assert isinstance(c, CZ_Parametrization_Fix_Duration_Node)\n    assert isinstance(c, ScheduleNode)\nThe suggested very first test is to istantiate the class and make sure it has the correct type(s) following any inheritance.\n\n\n\ndef test_CanGetQubitsFromCouplers():\n    c = CZ_Parametrization_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q15\"])\n    assert c.all_qubits == [\"q14\", \"q15\"]\n    assert c.couplers == ['q14_q15']\nMake sure all inputs and their manipulations are correctly initialised in the constructor, in this case the qubits are taken from the coupler pair\n\n\n\ndef test_ValidationReturnErrorWithSameQubitCoupler():\n    with pytest.raises(ValueError):\n       CZ_Parametrization_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q14\"])\nInputs can be incorrect and should always be tested to avoid unexpected behaviour down the line which can be difficult to trace back to the origin. There are infinite number of possible errors, so it is impossible to cover them all, but at least the obvious ones should be considered. In this case a typical typing error with the couple have the same qubit twice.\n\n\n\n@pytest.fixture(autouse=True)\ndef setup_good_data():\n    os.environ[\"DATA_DIR\"] = str(Path(__file__).parent / \"results\")\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    return d14, d15, freqs, amps\nThis is an example of how to load data for testing the analysis of a node, please note that the specifics may change as we are about to change the dataset format at the time of writing this; however, the principle will be the same.\nThis data can then be used in multiple tests, for example:\ndef test_canGetMaxFromQ1(setup_good_data):\n    d14, d15, freqs, amps = setup_good_data\n    c = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    result = c.run_fitting()\n    indexBestFreq = np.where(freqs == result[0])[0]\n    indexBestAmp = np.where(amps == result[1])[0]\n    assert indexBestFreq[0] == 9\n    assert indexBestAmp[0] == 13\n\n\ndef test_canGetMinFromQ2(setup_good_data):\n    d14, d15, freqs, amps = setup_good_data\n    c = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    result = c.run_fitting()\n    indexBestFreq = np.where(freqs == result[0])[0]\n    indexBestAmp = np.where(amps == result[1])[0]\n    assert indexBestFreq[0] == 10\n    assert indexBestAmp[0] == 12\nThese two tests make sure that the return values are correct for the two qubits that are connected by the coupler.\n\n\n\ndef test_canPlotBad(setup_bad_data):\n    matplotlib.use(\"Agg\")\n    d14, d15, freqs, amps = setup_bad_data\n    c14 = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    result = c14.run_fitting()\n\n    figure_path = os.environ[\"DATA_DIR\"] + \"/Frequency_Amplitude_bad_q14.png\"\n    # Remove the file if it already exists\n    if os.path.exists(figure_path):\n        os.remove(figure_path)\n\n    fig, ax = plt.subplots(figsize=(15, 7), num=1)\n    plt.Axes\n    c14.plotter(ax)\n    fig.savefig(figure_path)\n    plt.close()\n\n    assert os.path.exists(figure_path)\n    from PIL import Image\n\n    with Image.open(figure_path) as img:\n        assert img.format == \"PNG\", \"File should be a PNG image\"\n\n    c15 = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    result = c15.run_fitting()\n\n    figure_path = os.environ[\"DATA_DIR\"] + \"/Frequency_Amplitude_bad_q15.png\"\n    # Remove the file if it already exists\n    if os.path.exists(figure_path):\n        os.remove(figure_path)\n\n    fig, ax = plt.subplots(figsize=(15, 7), num=1)\n    plt.Axes\n    c15.plotter(ax)\n    fig.savefig(figure_path)\n    plt.close()\n\n    assert os.path.exists(figure_path)\n    from PIL import Image\n\n    with Image.open(figure_path) as img:\n        assert img.format == \"PNG\", \"File should be a PNG image\"\nThis is an example on how to save files in the “results” sub-folder within the “tests” folder. The code make sure the expected file exists and has the correct format. The created files can be inspected by the developer. A possible extension would be to upload the images in the data folder and check the those produced by the test are identical.\n\n\n\n@pytest.fixture(autouse=True)\ndef setup_bad_data():\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_bad_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    q15Res = q15Ana.run_fitting()\n    return q14Res, q15Res\n\n\ndef test_combineBadResultsReturnNoValidPoint(setup_bad_data):\n    q14Res, q15Res = setup_bad_data\n    c = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n    r = c.are_frequencies_compatible()\n    assert r == False\n    r = c.are_amplitudes_compatible()\n    assert r == False\n    r = c.are_two_qubits_compatible()\n    assert r == False\nIn this example, the data produced is loaded from a file that has data that is not a good working point. Note that there two analyses are run in the setup; the combination is run in the test, so that, if needed in other tests, the data could be modified for specific cases. This is a failure test, making sure that bad inputs are not recognised as good working points.\n\n\n\ndef setup_data():\n    # It should be a single dataset, but we do not have one yet, so we loop over existing files\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    q15Res = q15Ana.run_fitting()\n    c1 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_bad_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs_bad = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps_bad = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(\n        d14, freqs_bad, amps_bad\n    )\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(\n        d15, freqs_bad, amps_bad\n    )\n    q15Res = q15Ana.run_fitting()\n    c2 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    dataset_path = (\n        Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp_2.hdf5\"\n    )\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs_2 = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps_2 = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs_2, amps_2)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs_2, amps_2)\n    q15Res = q15Ana.run_fitting()\n    c3 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    list_of_results = [(c1, 0.1), (c2, 0.2), (c3, 0.3)]\n    return list_of_results, freqs, amps, freqs_2, amps_2\nIn this case, three points are loaded and the analysis is run on each of them. The results are returned for each point and can be used in different tests, for example by removing elements in the list_results array.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Unit tests"
    ]
  },
  {
    "objectID": "developer-guide/node_types.html",
    "href": "developer-guide/node_types.html",
    "title": "Node types",
    "section": "",
    "text": "Node types\nThe execution of most of the nodes consists of a single schedule compilation, a single measurement and a single post-processing. Although for most of the nodes this workflow suffices, there are exceptions while this workflow can become limiting in more advanced implementations.\nTo allow greater flexibilty in the node implementations the nodes are categorized:\n\nAccording to whether they compile once or multiple times:\n\nnode.type = simple_sweep: if it compiles once\nnode.type = parameterized_sweep: if it compiles multiple times\n\nAccording to whether the sweeping parameters are swept whithin the schedule or not:\n\nThere is only node.schedule_samplespace if the sweeping takes place whithin the schedule\nThere are both node.schedule_samplespace and node.external_samplespace if there are sweeping parameters outside of the scedule. For example the coupler_spectroscopy node sweeps the dc_current outside of the schedule:\n\n\n  class Coupler_Spectroscopy_Node(BaseNode):\n    measurement_obj = Two_Tones_Multidim\n    analysis_obj = CouplerSpectroscopyAnalysis\n\n    def __init__(self, name: str, all_qubits: list[str], couplers, **schedule_keywords):\n        super().__init__(name, all_qubits, **schedule_keywords)\n        self.couplers = couplers\n        self.redis_field = ['parking_current']\n        self.all_qubits = self.coupled_qubits\n\n        self.schedule_samplespace = {\n            'spec_frequencies': {\n                qubit: qubit_samples(qubit) for qubit in self.all_qubits\n            }\n        }\n\n        self.external_samplespace = {\n            'dc_currents': {\n                self.coupler: np.arange(-2.5e-3, 2.5e-3, 500e-6)\n            },\n        }\n\n    def pre_measurement_operation(self, reduced_ext_space):\n        iteration_dict = reduced_ext_space['dc_currents']\n\n        this_iteration_value = list(iteration_dict.values())[0]\n        print(f'{ this_iteration_value = }')\n        self.spi_dac.set_dac_current(self.dac, this_iteration_value)\nBy default every node every node is assigned a node.type attribute at the BaseNode class:\nself.type = simple_sweep\nThis attribute can be overwritten at the implementation of the class of each node. An example of a parameterized_sweep node type is Randomized_Benchmarking as each new iteration requires a the schedule to be recompiled with a different random seed.\nThe tergite_acl/scripts/node_supervisor.py is responcible to distinguish between each node variation.\n\nExamples of nodes requiring an external samplespace:\n\ncoupler_spectroscopy sweeps the dc_current which is set by the SPI rack not the cluster\nT1 sweeps a repetetion index to repeat the measurement many times\nrandomized_benchmarking sweeps different seeds. Although the seed is a schedule parameter, sweeping outside the schedule improves memory utilization."
  },
  {
    "objectID": "user-guide/configuration_files.html",
    "href": "user-guide/configuration_files.html",
    "title": "Configuration Files",
    "section": "",
    "text": "Here the connection is made between the Qblox cluster physical ports and clocks to the qubits and couplers of the QPU.\nThe file must be placed at the directory configs/\nGiven a .csv file after a mixer calibration the function ... can create instantly the corresponding JSON file.\n\n\nWith quantify-scheduler 0.18.0 there has been introduced a new way on how to structure the hardware configuration file. If you are having a hardware configuration file, that is structured using the old way, you can use the following script to migrate it to the new structure.\npython tergite_autocalibration/scripts/migrate_blox_hardware_configuration.py &lt;PATH_TO_HW_CONFIG&gt;\n\n\n\n\nHere we set reasonable initial parameters\n\n\n\nHere we input the initial values for the resonator and qubits frequencies, assumed that they have been acquired through VNA measurements, design targets or previous manual measurements. /tergite_acl/config/VNA_values.py",
    "crumbs": [
      "Home",
      "User Guide",
      "Configuration Files"
    ]
  },
  {
    "objectID": "user-guide/configuration_files.html#hardware-configuration-file-json",
    "href": "user-guide/configuration_files.html#hardware-configuration-file-json",
    "title": "Configuration Files",
    "section": "",
    "text": "Here the connection is made between the Qblox cluster physical ports and clocks to the qubits and couplers of the QPU.\nThe file must be placed at the directory configs/\nGiven a .csv file after a mixer calibration the function ... can create instantly the corresponding JSON file.\n\n\nWith quantify-scheduler 0.18.0 there has been introduced a new way on how to structure the hardware configuration file. If you are having a hardware configuration file, that is structured using the old way, you can use the following script to migrate it to the new structure.\npython tergite_autocalibration/scripts/migrate_blox_hardware_configuration.py &lt;PATH_TO_HW_CONFIG&gt;",
    "crumbs": [
      "Home",
      "User Guide",
      "Configuration Files"
    ]
  },
  {
    "objectID": "user-guide/configuration_files.html#device-configuration-file-toml",
    "href": "user-guide/configuration_files.html#device-configuration-file-toml",
    "title": "Configuration Files",
    "section": "",
    "text": "Here we set reasonable initial parameters",
    "crumbs": [
      "Home",
      "User Guide",
      "Configuration Files"
    ]
  },
  {
    "objectID": "user-guide/configuration_files.html#vna-resonator-qubit-frequencies",
    "href": "user-guide/configuration_files.html#vna-resonator-qubit-frequencies",
    "title": "Configuration Files",
    "section": "",
    "text": "Here we input the initial values for the resonator and qubits frequencies, assumed that they have been acquired through VNA measurements, design targets or previous manual measurements. /tergite_acl/config/VNA_values.py",
    "crumbs": [
      "Home",
      "User Guide",
      "Configuration Files"
    ]
  },
  {
    "objectID": "user-guide/operation.html",
    "href": "user-guide/operation.html",
    "title": "Operation",
    "section": "",
    "text": "The package ships with a CLI called acli (autocalibration command line interface) to solve some common tasks that appear quite often. In the following there are a number of useful commands, but if you want to find out information about commands in your shell use acli --help.\nSince some of the commands below are using autocompleting in most cases you would have to enable that feature in your shell by running:\nacli --install-completion\nUsually the shell would just suggest you file paths when pressing the tabulator, but with autocompletion enabled, you would be even get suggestions for node names or other inputs depending on your configuration.\n\n\nThis section provides an overview of the Command Line Interface (CLI) options and their functionalities.\n\n\nThe autocalibration CLI is organized into several main command groups:\n\ncluster: Handle operations related to the cluster\nnode: Handle operations related to the node\ngraph: Handle operations related to the calibration graph\ncalibration: Handle operations related to the calibration supervisor\njoke: Handle operations related to the well-being of the user\n\n\n\n\n\n\nReboots the cluster.\nUsage:\nacli cluster reboot\nThis command will prompt for confirmation before rebooting the cluster, as it can interrupt ongoing measurements.\n\n\n\n\n\n\nResets all parameters in Redis for the specified node(s).\nUsage:\nacli node reset [OPTIONS]\nOptions:\n\n-n, --name TEXT: Name of the node to be reset in Redis (e.g., resonator_spectroscopy)\n-a, --all: Reset all nodes\n-f, --from_node TEXT: Reset all nodes from the specified node in the chain\n\n\n\n\n\n\n\nPlots the calibration graph to the user-specified target node in topological order.\nUsage:\nacli graph plot\nThis command visualizes the calibration graph using an arrow chart.\n\n\n\n\n\n\nStarts the calibration supervisor.\nUsage:\nacli calibration start [OPTIONS]\nOptions:\n\n-c TEXT: Cluster IP address (if not set, it will use CLUSTER_IP from the .env file)\n-r TEXT: Rerun an analysis (specify the path to the dataset folder)\n-n, --name TEXT: Specify the node type to rerun (works only with -r option)\n--push: Push a backend to an MSS specified in MSS_MACHINE_ROOT_URL in the .env file\n--browser: Will open the dataset browser in the background and plot the measurement results live\n\n\n\n\n\n\n\nStarts the dataset browser.\nUsage:\nacli browser --datadir [OPTIONS]\nOptions:\n\n--datadir PATH: Folder to take the plot data from\n--liveplotting: Whether plots should be updated in real time (default: False)\n--log-level INT: Log-level as in the Python logging package to be used in the logs (default: 30)\n\n\n\n\n\n\n\nPrints a random joke to lighten the mood.\nUsage:\nacli joke\nThis command fetches and displays a random joke, excluding potentially offensive content.\n\n\n\n\n\nThe CLI uses the Python library typer for command-line interface creation.\nSome commands may require additional configuration or environment variables to be set.\nWhen using the -r option for rerunning analysis, make sure to also specify the node name using -n.\n\nFor more detailed information about each command, use the --help option with any command or subcommand.",
    "crumbs": [
      "Home",
      "User Guide",
      "Operation"
    ]
  },
  {
    "objectID": "user-guide/operation.html#cli",
    "href": "user-guide/operation.html#cli",
    "title": "Operation",
    "section": "",
    "text": "This section provides an overview of the Command Line Interface (CLI) options and their functionalities.\n\n\nThe autocalibration CLI is organized into several main command groups:\n\ncluster: Handle operations related to the cluster\nnode: Handle operations related to the node\ngraph: Handle operations related to the calibration graph\ncalibration: Handle operations related to the calibration supervisor\njoke: Handle operations related to the well-being of the user\n\n\n\n\n\n\nReboots the cluster.\nUsage:\nacli cluster reboot\nThis command will prompt for confirmation before rebooting the cluster, as it can interrupt ongoing measurements.\n\n\n\n\n\n\nResets all parameters in Redis for the specified node(s).\nUsage:\nacli node reset [OPTIONS]\nOptions:\n\n-n, --name TEXT: Name of the node to be reset in Redis (e.g., resonator_spectroscopy)\n-a, --all: Reset all nodes\n-f, --from_node TEXT: Reset all nodes from the specified node in the chain\n\n\n\n\n\n\n\nPlots the calibration graph to the user-specified target node in topological order.\nUsage:\nacli graph plot\nThis command visualizes the calibration graph using an arrow chart.\n\n\n\n\n\n\nStarts the calibration supervisor.\nUsage:\nacli calibration start [OPTIONS]\nOptions:\n\n-c TEXT: Cluster IP address (if not set, it will use CLUSTER_IP from the .env file)\n-r TEXT: Rerun an analysis (specify the path to the dataset folder)\n-n, --name TEXT: Specify the node type to rerun (works only with -r option)\n--push: Push a backend to an MSS specified in MSS_MACHINE_ROOT_URL in the .env file\n--browser: Will open the dataset browser in the background and plot the measurement results live\n\n\n\n\n\n\n\nStarts the dataset browser.\nUsage:\nacli browser --datadir [OPTIONS]\nOptions:\n\n--datadir PATH: Folder to take the plot data from\n--liveplotting: Whether plots should be updated in real time (default: False)\n--log-level INT: Log-level as in the Python logging package to be used in the logs (default: 30)\n\n\n\n\n\n\n\nPrints a random joke to lighten the mood.\nUsage:\nacli joke\nThis command fetches and displays a random joke, excluding potentially offensive content.\n\n\n\n\n\nThe CLI uses the Python library typer for command-line interface creation.\nSome commands may require additional configuration or environment variables to be set.\nWhen using the -r option for rerunning analysis, make sure to also specify the node name using -n.\n\nFor more detailed information about each command, use the --help option with any command or subcommand.",
    "crumbs": [
      "Home",
      "User Guide",
      "Operation"
    ]
  },
  {
    "objectID": "user-guide/operation.html#structure",
    "href": "user-guide/operation.html#structure",
    "title": "Operation",
    "section": "",
    "text": "For each calibration node: compilation -&gt; execution -&gt; post-processing -&gt; redis updating",
    "crumbs": [
      "Home",
      "User Guide",
      "Operation"
    ]
  },
  {
    "objectID": "user-guide/operation.html#data-browsing",
    "href": "user-guide/operation.html#data-browsing",
    "title": "Operation",
    "section": "",
    "text": "Datasets are stored in data_directory Can be browsed with the dataset browser (coming soon)",
    "crumbs": [
      "Home",
      "User Guide",
      "Operation"
    ]
  },
  {
    "objectID": "user-guide/operation.html#development",
    "href": "user-guide/operation.html#development",
    "title": "Operation",
    "section": "",
    "text": "When submitting contributions, please prepend your commit messages with: fix: for bug fixes feat: for introducing a new feature (e.g. a new measurement node or a new analysis class) chore: for refactoring changes or any change that doesn’t affect the functionality of the code docs: for changes in the README, docstrings etc test: or dev: for testing or development changes (e.g. profiling scripts)",
    "crumbs": [
      "Home",
      "User Guide",
      "Operation"
    ]
  },
  {
    "objectID": "developer-guide/new_node_creation.html",
    "href": "developer-guide/new_node_creation.html",
    "title": "To create a new node:",
    "section": "",
    "text": "in the file tergite_autocalibration/lib/node_factory.py expand the dictionary self.node_implementations with a new entry: The key should be a string of the node name and the value should be the object that contains the implementation details. This object should be imported from either tergite_autocalibration/lib/nodes/qubit_control_nodes.py, tergite_autocalibration/lib/nodes/coupler_nodes.py, tergite_autocalibration/lib/nodes/readout_nodes.py or tergite_autocalibration/lib/nodes/characterization_nodes.py\nIn the file tergite_autocalibration/lib/nodes/graph.py in the list graph_dependencies insert the edges that describe the position of the new node in the Directed Acyclic Graph. There are two entries required (or one entry if the new node is the last on its path):\n\n\n('previous_node','new_node')\n('new_node', 'next_node')\n\n\nIn the tergite_autocalibration/config/device_config.toml set the quantity of interest at nan value\n\n\n\nEach node implementation object should contain a reference to the measurement object, the analysis object, the list of redis fields that the analysis updates and the samplespace of the measurement. For example on the Rabi Oscillations Node:\nclass Rabi_Oscillations_Node(BaseNode):\n    measurement_obj = Rabi_Oscillations\n    analysis_obj = RabiAnalysis\n\n    def __init__(self, name: str, all_qubits: list[str], **node_dictionary):\n        super().__init__(name, all_qubits, **node_dictionary)\n        self.redis_field = ['rxy:amp180']\n        self.schedule_samplespace = {\n            'mw_amplitudes': {\n                qubit: np.linspace(0.002, 0.80, 101) for qubit in self.all_qubits\n            }\n        }\n\n\nThe measurement_obj is imported from tergite_autocalibration/lib/calibration_schedules/ and contains the class that generates the appropriate measurement schedule. To initialize we require a dictionary of the extended transmons:\ntransmons: dict[str, ExtendedTransmon]\nIt must contain a method called schedule_function that expects the node.samplespace as input and returns the complete schedule.\n\n\n\nThe analysis_obj is imported from tergite_autocalibration/lib/analysis/ and contains the class that perform the analysis for a single qubit. It must contain a run_fitting method and a plotter method\n\n\n\nNodes are divided in two distinct categories:\n\nsimple_sweep: where the Quantify Schedule is compiled only once\nparameterized_sweep: where the node requires multiple iterations and each iteration requires a new recompilation.\n\nFurthermore, each node can expect two types of samplespaces:\n\nschedule_samplespace: parameter values to be input to the schedule function\nexternal_samplespace: parameter values for quantities that are not set during a schedule\n\n\n\n\nPlease create a new node in a separate folder, so that it is clearer what the new node is meant to do. Add an empty init.py file to the folder, this is needed to mark the folder as part of the packege and allow imports from these folders\nTo keep the code clean, please create sub-folders following this scheme:\n\ntests: create unit tests in here, more on tests in href: unit_tests\nutils: any utility class, such as enum, errors and similar classes should be placed here\n\n\n\n\nPlease add your node to the list of available nodes in this Documentation.\nAdd ny relevant information on how to use your node, dependencies and reference to publication as needed for allowing other to use the code you developed.\nDetails on the implementation on the Node types section.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Create a new node"
    ]
  },
  {
    "objectID": "developer-guide/new_node_creation.html#node-implementation-object",
    "href": "developer-guide/new_node_creation.html#node-implementation-object",
    "title": "To create a new node:",
    "section": "",
    "text": "Each node implementation object should contain a reference to the measurement object, the analysis object, the list of redis fields that the analysis updates and the samplespace of the measurement. For example on the Rabi Oscillations Node:\nclass Rabi_Oscillations_Node(BaseNode):\n    measurement_obj = Rabi_Oscillations\n    analysis_obj = RabiAnalysis\n\n    def __init__(self, name: str, all_qubits: list[str], **node_dictionary):\n        super().__init__(name, all_qubits, **node_dictionary)\n        self.redis_field = ['rxy:amp180']\n        self.schedule_samplespace = {\n            'mw_amplitudes': {\n                qubit: np.linspace(0.002, 0.80, 101) for qubit in self.all_qubits\n            }\n        }\n\n\nThe measurement_obj is imported from tergite_autocalibration/lib/calibration_schedules/ and contains the class that generates the appropriate measurement schedule. To initialize we require a dictionary of the extended transmons:\ntransmons: dict[str, ExtendedTransmon]\nIt must contain a method called schedule_function that expects the node.samplespace as input and returns the complete schedule.\n\n\n\nThe analysis_obj is imported from tergite_autocalibration/lib/analysis/ and contains the class that perform the analysis for a single qubit. It must contain a run_fitting method and a plotter method\n\n\n\nNodes are divided in two distinct categories:\n\nsimple_sweep: where the Quantify Schedule is compiled only once\nparameterized_sweep: where the node requires multiple iterations and each iteration requires a new recompilation.\n\nFurthermore, each node can expect two types of samplespaces:\n\nschedule_samplespace: parameter values to be input to the schedule function\nexternal_samplespace: parameter values for quantities that are not set during a schedule\n\n\n\n\nPlease create a new node in a separate folder, so that it is clearer what the new node is meant to do. Add an empty init.py file to the folder, this is needed to mark the folder as part of the packege and allow imports from these folders\nTo keep the code clean, please create sub-folders following this scheme:\n\ntests: create unit tests in here, more on tests in href: unit_tests\nutils: any utility class, such as enum, errors and similar classes should be placed here\n\n\n\n\nPlease add your node to the list of available nodes in this Documentation.\nAdd ny relevant information on how to use your node, dependencies and reference to publication as needed for allowing other to use the code you developed.\nDetails on the implementation on the Node types section.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Create a new node"
    ]
  },
  {
    "objectID": "developer-guide/node_classes.html",
    "href": "developer-guide/node_classes.html",
    "title": "Node types",
    "section": "",
    "text": "Node types\nThe execution of most of the nodes consists of a single schedule compilation, a single measurement and a single post-processing. Although for most of the nodes this workflow suffices, there are exceptions while this workflow can become limiting in more advanced implementations.\nTo allow greater flexibility in the node implementations the nodes are categorized:\n\nAccording to whether they compile once or multiple times:\n\nnode.type = simple_sweep: if it compiles once\nnode.type = parameterized_sweep: if it compiles multiple times\n\nAccording to whether the sweeping parameters are swept within the schedule or not:\n\nThere is only node.schedule_samplespace if the sweeping takes place within the schedule\nThere are both node.schedule_samplespace and node.external_samplespace if there are sweeping parameters outside the schedule. For example the coupler_spectroscopy node sweeps the dc_current outside of the schedule:\n\n\n  class Coupler_Spectroscopy_Node(BaseNode):\n    measurement_obj = Two_Tones_Multidim\n    analysis_obj = CouplerSpectroscopyAnalysis\n\n    def __init__(self, name: str, all_qubits: list[str], couplers, **schedule_keywords):\n        super().__init__(name, all_qubits, **schedule_keywords)\n        self.couplers = couplers\n        self.redis_field = ['parking_current']\n        self.all_qubits = self.coupled_qubits\n\n        self.schedule_samplespace = {\n            'spec_frequencies': {\n                qubit: qubit_samples(qubit) for qubit in self.all_qubits\n            }\n        }\n\n        self.external_samplespace = {\n            'dc_currents': {\n                self.coupler: np.arange(-2.5e-3, 2.5e-3, 500e-6)\n            },\n        }\n\n    def pre_measurement_operation(self, reduced_ext_space):\n        iteration_dict = reduced_ext_space['dc_currents']\n\n        this_iteration_value = list(iteration_dict.values())[0]\n        print(f'{ this_iteration_value = }')\n        self.spi_dac.set_dac_current(self.dac, this_iteration_value)\nBy default, every node is assigned a node.type attribute at the BaseNode class:\nself.type = simple_sweep\nThis attribute can be overwritten at the implementation of the class of each node. An example of a parameterized_sweep node type is Randomized_Benchmarking as each new iteration requires the schedule to be recompiled with a different random seed.\nThe tergite_acl/scripts/node_supervisor.py is responsible to distinguish between each node variation.\n\nExamples of nodes requiring an external samplespace:\n\ncoupler_spectroscopy sweeps the dc_current which is set by the SPI rack not the cluster\nT1 sweeps a repetition index to repeat the measurement many times\nrandomized_benchmarking sweeps different seeds. Although the seed is a schedule parameter, sweeping outside the schedule improves memory utilization.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Node Classes"
    ]
  },
  {
    "objectID": "home.html",
    "href": "home.html",
    "title": "Documentation of the Tergite Automatic Calibration",
    "section": "",
    "text": "Documentation of the Tergite Automatic Calibration\nThe tergite-autocalibration is a toolkit to ease calibrating quantum devices for superconducting platforms. The project contains an orchestration manager, a collection of calibration schedules and a collection of post-processing and analysis routines. It is tailored for the tune-up of the 25 qubits QPU at Chalmers, which is receiving generous funding by the Wallenberg Centre for Quantum Technology (WACQT) for research, development and operation.\n\nUser Guide\nA tutorial on how to get started with the automatic calibration. This tutorial contains a guide on how to use the commandline interface with quick commands. Further, there is an introduction into configuration files.\n\n\nNode Library\nThe main principle behind the automatic calibration is based on calibrating nodes in a graph structure. A node contains all the measurement and analysis classes to find the quantity of interest - for qubits and couplers. If you are interested in implementing a new node, it might be worth it to check whether there are existing nodes that you can use to find the qubit properties you are looking to calibrate.\n\n\nDeveloper Guide\nThis repository is an actively developed open-source project and also part of the Tergite full-stack software ecosystem to operate a quantum computer. Hence, you are more than welcome to contribute to the project with your own ideas that fit into the framework. To familiarize yourself with the existing classes and the architecture of the automatic calibration, please take a look into the development guide. Here, you can also find best practices and information about our design philosophy.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "getting_started.html#prerequisites",
    "href": "getting_started.html#prerequisites",
    "title": "Getting started",
    "section": "",
    "text": "The automatic calibration requires redis for on-memory data storage. As redis operates only on Linux systems, the calibration has to run either on one of:\n\nLinux distributions\nWSL (Windows Subsystem for Linux) environments, installed on a Windows system. WSL requires Windows 10 and a version of at least 1903.\n\nInstallation instructions for redis can be found here: https://redis.io/docs/getting-started/installation/install-redis-on-linux/\nThe link for the redis installation also contains instructions on how to start a redis instance. However, if you already have redis installed, you can run it using:\nredis-server --port YOUR_REDIS_PORT\nUsually, redis will run automatically on port 6379, but in a shared environment please check with others to get your redis port, since you would overwrite each other’s memory.",
    "crumbs": [
      "Home",
      "User Guide",
      "Getting started"
    ]
  },
  {
    "objectID": "developer_guide.html#next-steps",
    "href": "developer_guide.html#next-steps",
    "title": "Developer Guide",
    "section": "",
    "text": "As you noticed, most of the above advice contains the formatting and other formal steps during development. Consider reading about:\n\nUnit testing to find out more about how to write test cases for the code.\nNodes on how to create a new calibration node.\nNode Classes to learn about the different types of nodes that the framework supports.\n\nIf you are having any feedback about the documentation or want to work on documentation, please continue with this developer guide about how to contribute with documentation.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Introduction"
    ]
  },
  {
    "objectID": "developer-guide/writing_documentation.html",
    "href": "developer-guide/writing_documentation.html",
    "title": "Writing documentation",
    "section": "",
    "text": "Writing good documentation is very important for new people to understand how to use things. Often, we do not have time to write good guides, because we are having deadlines or other things to do that are more interesting and less boring than writing down what we already seem to know so well.\nThis guide is a small guide on how to write a good guide. It does not aim to provide an explanation on how to write the perfect documentation, because these resources already exist on the internet. However, it should make at least curious on how to write documentation with some short general rules:\n\nThink like the reader\nGo step by step\nBe precise\nFollow standards\nDiscuss\n\nIn the second part of the tutorial, we will focus on how to write documentation especially for this project.\nSo, scroll down if you want to start reading there.\n\n\nWhen you explain how things work, it is often better to take it step by step. Also, do not hesitate to make small steps. Tiny ones.\nKeep in mind to think like the reader:\n\nWhat is the previous knowledge about a topic\n\nMost of the people reading our docs are probably students, so maybe they are not familiar with all git commands and Linux tricks.\n\nProvide a structure/agenda for your guide\n\nLet us say you want to write a guide about the installation of Miniconda, which contains a lot of steps and the installation process looks like this example.\n\n\ncd ~\nmkdir tmp\ncd tmp\nwget SomeInstallationFileForMiniconda\n\nchmod u+x SomeInstallationFileForMiniconda\n./SomeInstallationFileForMiniconda\n\nconda init bash\nexport PATH=/home/&lt;YOUR\\_USERNAME&gt;/miniconda3/bin:$PATH\n\nconda create -n new-environment python=3.9\nconda activate new-environment\n\ncd ~\nmkdir repos\ncd repos\ngit clone git@my-repo-url\ncd my-repo\npip install -e .\nHow can this guide for an installation look better? Well, while writing think of yourself explaining the same installation procedure to a colleague. You would probably tell some of the commands, but in between you would also briefly discuss what they are doing. Maybe you are experiencing an error at some point. Maybe you want to give some more background knowledge in case the reader does not have it. In the case of our team it very important to give these kind of hints, because we are working in an interdisciplinary environment and followed different educational paths.\nNow, let’s see how the installation could look like. Let us assume we already wrote an introduction, the prerequisites are clear, and we are just explaining the installation itself.\n\n\n\nNow this is the better version, because it breaks down the process into:\n\nDownloading the installation file\nInstalling conda\nTroubleshooting some common problems that can happen during the installation\nShowing what would be the next step after the installation\n\ncd ~\nmkdir tmp\ncd tmp\nwget SomeInstallationFileForMiniconda\nFirst, we are navigating to our home directory and create a temporary directory where we store the Miniconda installation file. With the wget command, we download the file. Note that there are different versions of the installer, depending on which operating system you use. A full list of installers can be found on the Miniconda webpage . The reason why we are using Miniconda and not Anaconda is because it takes less disk space. As soon as we have downloaded the installer file, we can continue with the installation.\nchmod u+x SomeInstallationFileForMiniconda\n./SomeInstallationFileForMiniconda\nWe have to make the installer executable and run it. During the installation process itself, just follow the standard recommendations and paths that the installer selects during the installation. After the installation it can be possible that you have to do some tiny adjustments. With\nconda init bash\nyou can modify your shell to show the conda path. And with\nexport PATH=/home/&lt;YOUR\\_USERNAME&gt;/miniconda3/bin:$PATH\nyou are adding conda to the PATH variable. That should be done automatically during the installation, but sometimes it does not work properly. You can verify the installation by typing:\nconda --version\nAfter the installation is complete, we can create our conda environment.\nconda create -n new-environment python=3.9\nconda activate new-environment\nNote that we are using Python 3.9 in our environment. This is because we are having some dependencies with another library, which will be explained in more detail in the remarks section. If you are running the installation on macOS, please find additional resources in the respective guide how to use conda environments on macOS. We can now start to clone our repository and install the dependencies for the project.\ncd ~\ncd repos\ngit clone git@my-repo-url\nIf you have not done yet, you can create the repos directory using mkdir repos. Please read this other guide that explains how to use Git. After you cloned the repository, we can navigate in there and install our dependencies.\ncd my-repo\npip install -e .\nThis can take up to three minutes. If you are running into any problems during the installation please contact one of the other team members and verify whether it works for them or try to find a fix if you know how to approach it. If you have fixed errors that happened during the installation, please put a note into this installation guide, so, other people with the same issue have an easier time to solve it.\nWhat is now better in this second example are the explanations for every step. For your brain it is way easier to come back to one of these steps, and you learn way more about what you are doing when there are small explanations than when just copy and pasting console commands.\n\nTry to group commands or tiny steps that belong together, otherwise it looks a bit scattered. But also do not have to many loose blocks flying around.\nDo not write too much, but take some time to explain some of the backgrounds even if you think it is clear. If there is a lot of background, you can also link to some page in the internet which already has done the work and provides a tutorial. Finally, make sure that you are writing in a consistent style and provide enough examples.\nWhenever your guide is finished, share it with others and discuss. It is probably not perfect (yet) and other people might have valuable feedback from their own experiences with the problem you are describing.\n\n\n\n\n\nIn this section, we will go through the specific processes that are important when writing documentation:\n\nInstalling quarto\nStructure of the files\nAdding a navigation entry\nAdvanced features of quarto\n\n\n\nIn the tergite-autocalibration repository we are using Quarto to render documentation. Quarto is very versatile and can - among other document types - render markdown and Jupyter notebooks. If you have been following the steps in the developer guide introduction, you should have quarto already installed. Otherwise, you can do it by running:\npip install quarto\nTo render a simple preview of your documentation, please open a terminal inside the documentation folder and run:\ncd documentation\nquarto preview\nThis will open a browser window with the rendered quarto documentation pages.\n\n\n\nMaybe you noticed that on the top-level of the repository there are two folders, one called docs and another one called documentation. This is because they have two different purposes:\n\ndocumentation: Contains the markdown files and Jupyter notebooks to create the documentation from. These are the files that you edit.\ndocs: Is the output HTML after running quarto render, which is displayed on the website. You do not edit these files. They will always be generated from the files in the documentation folder.\n\nNow, let us have a look at the documentation folder, because this is the one we are working with the most. It is structured:\n\n.assets: There you put images and style/formatting material.\n.quarto: Do not touch this folder and do not commit it to git, because it contains temporary files during the rendering process.\ndeveloper-guide, nodes and user-guide: Contains the respective content for the pages.\nThen there are a couple of pages from the top-level of the documentation.\nAnd a file called _quarto.yml. This file is important, because it defines how things are rendered.\n\n\n\n\nIn here, the most relevant to be touched during adding documentation is the sidebar section. Imagine you are adding a new page e.g. about a calibration node, and you want to add it to the navigation. Then, you would add an entry at the correct position in the _quarto.yml file for the sidebar.\n  sidebar:\n    style: \"docked\"\n    search: true\n    contents:\n      - section: \"Node Library\"\n        contents:\n          - text: \"Overview\"\n            href: available_nodes.qmd\n          - text: \"Resonator spectroscopy\"\n            href: nodes/resonator_spectroscopy_node.qmd\n          - text: \"My new node\"\n            href: nodes/my_new_node.qmd\nIt is pretty self-explaining where to put the node when you see the rendered version in your browser.\n\n\n\nAs you noticed, quarto does not render from normal .md markdown files, but from .qmd quarto markdown files. These are extending the markdown functionality with some special features. Here, we will show them along with some normal useful feature from markdown.\nCode highlighting\nImagine you want to have a block to show code. What you write inside your markdown file would be:\n```python\nvariable = 123\nprint(\"Hello world\")\n```\nAnd the output would look like:\nvariable = 123\nprint(\"Hello world\")\nGraphs\nFor the calibration nodes, we are using a graph to chain them. This graph is rendered with mermaid.\n```{mermaid}\ngraph TD\n    A[Resonator Spectroscopy] --&gt; B(Qubit Spectroscopy)\n    B --&gt; C[Rabi Oscillations]\n        \n    click A href \"nodes/resonator_spectroscopy_node.html\"\n\n    style A fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style B fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style C fill:#ffe6cc,stroke:#333,stroke-width:2px\n```\nWith the style attribute, you can define the colour of the node. With the click attribute, add a link on a node inside the graph.\n\n\n\n\nWhen you reached the point that you are already writing the perfect documentation, you are probably also done reading the documentation. So, no next steps to read up upon. You can now write even more documentation or just code and explore quantum physics :)",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Writing documentation"
    ]
  },
  {
    "objectID": "developer-guide/writing_documentation.html#for-example",
    "href": "developer-guide/writing_documentation.html#for-example",
    "title": "Writing documentation",
    "section": "",
    "text": "When you explain how things work, it is often better to take it step by step. Also, do not hesitate to make small steps. And it cannot be small enough. Really.\nKeep in mind to think like the reader: * What is the previous knowledge about a topic * A big bunch of people who read our docs are probably student, so maybe they have used git once in a lab course and do not know all Linux commands * Provide a structure/agenda\nLet us say you want to write a guide about an installation, which contains a lot of steps and the installation process looks like this example.\n\n\ncd ~\nmkdir tmp\ncd tmp\nwget SomeInstallationFileForMiniconda\n\nchmod u+x SomeInstallationFileForMiniconda\n./SomeInstallationFileForMiniconda\n\nconda init bash\nexport PATH=/home/&lt;YOUR\\_USERNAME&gt;/miniconda3/bin:$PATH\n\nconda create -n new-environment python=3.9\nconda activate new-environment\n\ncd ~\nmkdir repos\ncd repos\ngit clone git@my-repo-url\ncd my-repo\npip install -e .\nHow can this guide for an installation look better? Well, while writing think of yourself explaining the same installation procedure to a colleague. You would probably tell some of the commands, but in between you would also briefly discuss what they are doing. Maybe you are experiencing an error at some point. Maybe you want to give some more background knowledge in case the reader does not have it. In the case of our team it very important to give these kind of hints, because we are working in an interdisciplinary environment and followed different educational paths.\nNow, let’s see how the installation could look like. Let us assume we already wrote an introduction, the prerequisites are clear and we are just explaining the installation itself.\n\n\n\ncd ~\nmkdir tmp\ncd tmp\nwget SomeInstallationFileForMiniconda\nFirst, we are navigating to our home directory and create a temporary directory where we store the Miniconda installation file. With the wget command, we download the file. Note that there are different versions of the installer, depending on which operating system you use. A full list of installers can be found on the Miniconda webpage . The reason why we are using Miniconda and not Anaconda is because it takes less disk space. As soon as we have downloaded the installer file, we can continue with the installation.\nchmod u+x SomeInstallationFileForMiniconda\n./SomeInstallationFileForMiniconda\nWe have to make the installer executable and run it. During the installation process itself, just follow the standard recommendations and paths that the installer selects during the installation. After the installation it can be possible that you have to do some tiny adjustments. With\nconda init bash\nyou can modify your shell to show the conda path. And with\nexport PATH=/home/&lt;YOUR\\_USERNAME&gt;/miniconda3/bin:$PATH\nyou are adding conda to the PATH variable. That should be done automatically during the installation, but sometimes it does not work properly. You can verify the installation by typing:\nconda --version\nAfter the installation is complete, we can create our conda environment.\nconda create -n new-environment python=3.9\nconda activate new-environment\nNote that we are using Python 3.9 in our environment. This is because we are having some dependencies with another library, which will be explained in more detail in the remarks section. If you are running the installation on MacOS, please find additional resources in the respective guide how to use conda environments on MacOS. We can now start to clone our repository and install the dependencies for the project.\ncd ~\ncd repos\ngit clone git@my-repo-url\nIf you have not done yet, you can create the repos directory using mkdir repos. Please read this other guide that explains you how to use Git. After you cloned the repository, we can navigate in there and install our dependencies.\ncd my-repo\npip install -e .\nThis can take up to three minutes. If you are running into any problems during the installation please contact one of the other team members and verify whether it works for them or try to find a fix if you know how to approach it. If you have fixed errors that happened during the installation, please put a note into this installation guide, so, other people with the same issue have an easier time to solve it.\nWhat is now better in this second example are the explanations for every step. For your brain it is way easier to come back to one of these steps and you learn way more about what you are doing when there are small explanations than when just copy pasting console commands.\nFurthermore, when you are running into any issue, you already have an idea of what could be wrong. And let’s say you search for a solution on how to solve a specific issue and have a lot of open tabs, it is way easier to come back to the guide and see in which step of the guide you are stuck currently.\nTry to group commands or tiny steps that belong together, otherwise it looks a bit scattered. But also do not have to many loose blocks flying around.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Writing documentation"
    ]
  },
  {
    "objectID": "developer-guide/writing_documentation.html#concluding-words",
    "href": "developer-guide/writing_documentation.html#concluding-words",
    "title": "Writing documentation",
    "section": "",
    "text": "When you are writing a guide, the most important is to write it such that everybody with a bit of technical background can understand what to do. Be precise and provide: - Version numbers of libraries - Branch names\nDo not write too much, but take some time to explain some of the backgrounds even if you think it is clear. If there is a lot of background, you can also link to some page in the internet which already has done the work and provides a tutorial. Finally, make sure that you are writing in a consistent style and provide enough examples.\nWhenever your guide is finished, share it with others and discuss. It is probably not perfect (yet) and other people might have valuable feedback from their own experiences with the problem you are describing.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Writing documentation"
    ]
  },
  {
    "objectID": "developer-guide/writing_documentation.html#example-on-writing-documentation-in-general",
    "href": "developer-guide/writing_documentation.html#example-on-writing-documentation-in-general",
    "title": "Writing documentation",
    "section": "",
    "text": "When you explain how things work, it is often better to take it step by step. Also, do not hesitate to make small steps. Tiny ones.\nKeep in mind to think like the reader:\n\nWhat is the previous knowledge about a topic\n\nMost of the people reading our docs are probably students, so maybe they are not familiar with all git commands and Linux tricks.\n\nProvide a structure/agenda for your guide\n\nLet us say you want to write a guide about the installation of Miniconda, which contains a lot of steps and the installation process looks like this example.\n\n\ncd ~\nmkdir tmp\ncd tmp\nwget SomeInstallationFileForMiniconda\n\nchmod u+x SomeInstallationFileForMiniconda\n./SomeInstallationFileForMiniconda\n\nconda init bash\nexport PATH=/home/&lt;YOUR\\_USERNAME&gt;/miniconda3/bin:$PATH\n\nconda create -n new-environment python=3.9\nconda activate new-environment\n\ncd ~\nmkdir repos\ncd repos\ngit clone git@my-repo-url\ncd my-repo\npip install -e .\nHow can this guide for an installation look better? Well, while writing think of yourself explaining the same installation procedure to a colleague. You would probably tell some of the commands, but in between you would also briefly discuss what they are doing. Maybe you are experiencing an error at some point. Maybe you want to give some more background knowledge in case the reader does not have it. In the case of our team it very important to give these kind of hints, because we are working in an interdisciplinary environment and followed different educational paths.\nNow, let’s see how the installation could look like. Let us assume we already wrote an introduction, the prerequisites are clear, and we are just explaining the installation itself.\n\n\n\nNow this is the better version, because it breaks down the process into:\n\nDownloading the installation file\nInstalling conda\nTroubleshooting some common problems that can happen during the installation\nShowing what would be the next step after the installation\n\ncd ~\nmkdir tmp\ncd tmp\nwget SomeInstallationFileForMiniconda\nFirst, we are navigating to our home directory and create a temporary directory where we store the Miniconda installation file. With the wget command, we download the file. Note that there are different versions of the installer, depending on which operating system you use. A full list of installers can be found on the Miniconda webpage . The reason why we are using Miniconda and not Anaconda is because it takes less disk space. As soon as we have downloaded the installer file, we can continue with the installation.\nchmod u+x SomeInstallationFileForMiniconda\n./SomeInstallationFileForMiniconda\nWe have to make the installer executable and run it. During the installation process itself, just follow the standard recommendations and paths that the installer selects during the installation. After the installation it can be possible that you have to do some tiny adjustments. With\nconda init bash\nyou can modify your shell to show the conda path. And with\nexport PATH=/home/&lt;YOUR\\_USERNAME&gt;/miniconda3/bin:$PATH\nyou are adding conda to the PATH variable. That should be done automatically during the installation, but sometimes it does not work properly. You can verify the installation by typing:\nconda --version\nAfter the installation is complete, we can create our conda environment.\nconda create -n new-environment python=3.9\nconda activate new-environment\nNote that we are using Python 3.9 in our environment. This is because we are having some dependencies with another library, which will be explained in more detail in the remarks section. If you are running the installation on macOS, please find additional resources in the respective guide how to use conda environments on macOS. We can now start to clone our repository and install the dependencies for the project.\ncd ~\ncd repos\ngit clone git@my-repo-url\nIf you have not done yet, you can create the repos directory using mkdir repos. Please read this other guide that explains how to use Git. After you cloned the repository, we can navigate in there and install our dependencies.\ncd my-repo\npip install -e .\nThis can take up to three minutes. If you are running into any problems during the installation please contact one of the other team members and verify whether it works for them or try to find a fix if you know how to approach it. If you have fixed errors that happened during the installation, please put a note into this installation guide, so, other people with the same issue have an easier time to solve it.\nWhat is now better in this second example are the explanations for every step. For your brain it is way easier to come back to one of these steps, and you learn way more about what you are doing when there are small explanations than when just copy and pasting console commands.\n\nTry to group commands or tiny steps that belong together, otherwise it looks a bit scattered. But also do not have to many loose blocks flying around.\nDo not write too much, but take some time to explain some of the backgrounds even if you think it is clear. If there is a lot of background, you can also link to some page in the internet which already has done the work and provides a tutorial. Finally, make sure that you are writing in a consistent style and provide enough examples.\nWhenever your guide is finished, share it with others and discuss. It is probably not perfect (yet) and other people might have valuable feedback from their own experiences with the problem you are describing.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Writing documentation"
    ]
  },
  {
    "objectID": "developer-guide/writing_documentation.html#documentation-for-the-automatic-calibration",
    "href": "developer-guide/writing_documentation.html#documentation-for-the-automatic-calibration",
    "title": "Writing documentation",
    "section": "",
    "text": "For the documentation in the automatic calibration we are using quarto",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Writing documentation"
    ]
  },
  {
    "objectID": "developer-guide/writing_documentation.html#writing-documentation-for-the-automatic-calibration",
    "href": "developer-guide/writing_documentation.html#writing-documentation-for-the-automatic-calibration",
    "title": "Writing documentation",
    "section": "",
    "text": "In this section, we will go through the specific processes that are important when writing documentation:\n\nInstalling quarto\nStructure of the files\nAdding a navigation entry\nAdvanced features of quarto\n\n\n\nIn the tergite-autocalibration repository we are using Quarto to render documentation. Quarto is very versatile and can - among other document types - render markdown and Jupyter notebooks. If you have been following the steps in the developer guide introduction, you should have quarto already installed. Otherwise, you can do it by running:\npip install quarto\nTo render a simple preview of your documentation, please open a terminal inside the documentation folder and run:\ncd documentation\nquarto preview\nThis will open a browser window with the rendered quarto documentation pages.\n\n\n\nMaybe you noticed that on the top-level of the repository there are two folders, one called docs and another one called documentation. This is because they have two different purposes:\n\ndocumentation: Contains the markdown files and Jupyter notebooks to create the documentation from. These are the files that you edit.\ndocs: Is the output HTML after running quarto render, which is displayed on the website. You do not edit these files. They will always be generated from the files in the documentation folder.\n\nNow, let us have a look at the documentation folder, because this is the one we are working with the most. It is structured:\n\n.assets: There you put images and style/formatting material.\n.quarto: Do not touch this folder and do not commit it to git, because it contains temporary files during the rendering process.\ndeveloper-guide, nodes and user-guide: Contains the respective content for the pages.\nThen there are a couple of pages from the top-level of the documentation.\nAnd a file called _quarto.yml. This file is important, because it defines how things are rendered.\n\n\n\n\nIn here, the most relevant to be touched during adding documentation is the sidebar section. Imagine you are adding a new page e.g. about a calibration node, and you want to add it to the navigation. Then, you would add an entry at the correct position in the _quarto.yml file for the sidebar.\n  sidebar:\n    style: \"docked\"\n    search: true\n    contents:\n      - section: \"Node Library\"\n        contents:\n          - text: \"Overview\"\n            href: available_nodes.qmd\n          - text: \"Resonator spectroscopy\"\n            href: nodes/resonator_spectroscopy_node.qmd\n          - text: \"My new node\"\n            href: nodes/my_new_node.qmd\nIt is pretty self-explaining where to put the node when you see the rendered version in your browser.\n\n\n\nAs you noticed, quarto does not render from normal .md markdown files, but from .qmd quarto markdown files. These are extending the markdown functionality with some special features. Here, we will show them along with some normal useful feature from markdown.\nCode highlighting\nImagine you want to have a block to show code. What you write inside your markdown file would be:\n```python\nvariable = 123\nprint(\"Hello world\")\n```\nAnd the output would look like:\nvariable = 123\nprint(\"Hello world\")\nGraphs\nFor the calibration nodes, we are using a graph to chain them. This graph is rendered with mermaid.\n```{mermaid}\ngraph TD\n    A[Resonator Spectroscopy] --&gt; B(Qubit Spectroscopy)\n    B --&gt; C[Rabi Oscillations]\n        \n    click A href \"nodes/resonator_spectroscopy_node.html\"\n\n    style A fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style B fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style C fill:#ffe6cc,stroke:#333,stroke-width:2px\n```\nWith the style attribute, you can define the colour of the node. With the click attribute, add a link on a node inside the graph.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Writing documentation"
    ]
  },
  {
    "objectID": "developer-guide/writing_documentation.html#next-steps",
    "href": "developer-guide/writing_documentation.html#next-steps",
    "title": "Writing documentation",
    "section": "",
    "text": "When you reached the point that you are already writing the perfect documentation, you are probably also done reading the documentation. So, no next steps to read up upon. You can now write even more documentation or just code and explore quantum physics :)",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Writing documentation"
    ]
  },
  {
    "objectID": "developer-guide/unit_tests.html#unit-tests-for-a-node",
    "href": "developer-guide/unit_tests.html#unit-tests-for-a-node",
    "title": "Unit tests",
    "section": "",
    "text": "These instructions will go step-by-step through how to create meaningful test cases for a node.\n\nOverview about the folder structure\nSpecific advices on how to test nodes\nExamples on how to test the analysis function of a node\n\nIf you are more a person that learns from the code rather than from a tutorial, please take a look at an easy node e.g. resonator spectroscopy and try to run and understand the test cases.\n\n\nThe test should be created in a sub-folder of the node called tests.\nPlease organise the file using these sub-folders:\n\ndata: place here any data file that is needed to create test cases, while it is possible to mock the data. Feel free to add a text file explaining how the data was produced.Mocking data is also possible, but it may be not practical for complex datasets.\nresults: create this folder to store your results, i.e. files that would be created by your analysis such as the plots. It should be empty, do not commit your results. To assure this is the case, add a file name .gitignore in the folder with this code:\n\n# Ignore everything in this directory\n*\n\n# But do not ignore this file\n!.gitignore     \n\n\n\nA good starting point, especially starting from scratch, it is to test for the class type, then that the inputs have the right formats and then move to some simple operation or trivial case. Build more complex cases from the simpler ones and exploits your tests to refactor the code as needed. Try to test as many reasonable cases as possible, both successfully and not. Remember to test for exceptions. We also suggest to develop using test drive development techniques that will ensure a high test coverage and a good code structure. Yes, do not forget to keep refactoring to improve the code.\nYou can find some samples below taken from the cz_parametrisation node, where all objects are tested\nCurrently, there is no way to differentiate from tests that require a QPU (i.e. measurements) and those that do not ( i.e. analyses). Since the latter are simpler to write, start with those that, in general, are more likely to benefit from unit tests as there is much more logic in the analysis than in the measurement.\nIf you need to test some complex scenario, such as those involving sweep, it is probably easier to start writing code and tests from the lowest level and then compose those objects to handle the more complex scenario considered.\n\n\n\nTest class type\ndef test_canCreateCorrectType():\n    c = CZ_Parametrization_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q15\"])\n    assert isinstance(c, CZ_Parametrization_Fix_Duration_Node)\n    assert isinstance(c, ScheduleNode)\nThe suggested very first test is to instantiate the class and make sure it has the correct type(s) following any inheritance.\nTest input parameters\ndef test_CanGetQubitsFromCouplers():\n    c = CZ_Parametrization_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q15\"])\n    assert c.all_qubits == [\"q14\", \"q15\"]\n    assert c.couplers == ['q14_q15']\nMake sure all inputs and their manipulations are correctly initialised in the constructor, in this case the qubits are taken from the coupler pair\nTest exception\ndef test_ValidationReturnErrorWithSameQubitCoupler():\n    with pytest.raises(ValueError):\n       CZ_Parametrization_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q14\"])\nInputs can be incorrect and should always be tested to avoid unexpected behaviour down the line which can be difficult to trace back to the origin. There are infinite number of possible errors, so it is impossible to cover them all, but at least the obvious ones should be considered. In this case a typical typing error with the couple have the same qubit twice.\nTest with data\n@pytest.fixture(autouse=True)\ndef setup_good_data():\n    os.environ[\"DATA_DIR\"] = str(Path(__file__).parent / \"results\")\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    return d14, d15, freqs, amps\nThis is an example of how to load data for testing the analysis of a node, please note that the specifics may change as we are about to change the dataset format at the time of writing this; however, the principle will be the same.\nThis data can then be used in multiple tests, for example:\ndef test_canGetMaxFromQ1(setup_good_data):\n    d14, d15, freqs, amps = setup_good_data\n    c = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    result = c.run_fitting()\n    indexBestFreq = np.where(freqs == result[0])[0]\n    indexBestAmp = np.where(amps == result[1])[0]\n    assert indexBestFreq[0] == 9\n    assert indexBestAmp[0] == 13\n\n\ndef test_canGetMinFromQ2(setup_good_data):\n    d14, d15, freqs, amps = setup_good_data\n    c = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    result = c.run_fitting()\n    indexBestFreq = np.where(freqs == result[0])[0]\n    indexBestAmp = np.where(amps == result[1])[0]\n    assert indexBestFreq[0] == 10\n    assert indexBestAmp[0] == 12\nThese two tests make sure that the return values are correct for the two qubits that are connected by the coupler.\nTest creating of images from plotter\ndef test_canPlotBad(setup_bad_data):\n    matplotlib.use(\"Agg\")\n    d14, d15, freqs, amps = setup_bad_data\n    c14 = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    result = c14.run_fitting()\n\n    figure_path = os.environ[\"DATA_DIR\"] + \"/Frequency_Amplitude_bad_q14.png\"\n    # Remove the file if it already exists\n    if os.path.exists(figure_path):\n        os.remove(figure_path)\n\n    fig, ax = plt.subplots(figsize=(15, 7), num=1)\n    plt.Axes\n    c14.plotter(ax)\n    fig.savefig(figure_path)\n    plt.close()\n\n    assert os.path.exists(figure_path)\n    from PIL import Image\n\n    with Image.open(figure_path) as img:\n        assert img.format == \"PNG\", \"File should be a PNG image\"\n\n    c15 = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    result = c15.run_fitting()\n\n    figure_path = os.environ[\"DATA_DIR\"] + \"/Frequency_Amplitude_bad_q15.png\"\n    # Remove the file if it already exists\n    if os.path.exists(figure_path):\n        os.remove(figure_path)\n\n    fig, ax = plt.subplots(figsize=(15, 7), num=1)\n    plt.Axes\n    c15.plotter(ax)\n    fig.savefig(figure_path)\n    plt.close()\n\n    assert os.path.exists(figure_path)\n    from PIL import Image\n\n    with Image.open(figure_path) as img:\n        assert img.format == \"PNG\", \"File should be a PNG image\"\nThis is an example on how to save files in the “results” sub-folder within the “tests” folder. The code make sure the expected file exists and has the correct format. The created files can be inspected by the developer. A possible extension would be to upload the images in the data folder and check the those produced by the test are identical.\nComplex dataset for comparing results\n@pytest.fixture(autouse=True)\ndef setup_bad_data():\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_bad_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    q15Res = q15Ana.run_fitting()\n    return q14Res, q15Res\n\n\ndef test_combineBadResultsReturnNoValidPoint(setup_bad_data):\n    q14Res, q15Res = setup_bad_data\n    c = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n    r = c.are_frequencies_compatible()\n    assert r == False\n    r = c.are_amplitudes_compatible()\n    assert r == False\n    r = c.are_two_qubits_compatible()\n    assert r == False\nIn this example, the data produced is loaded from a file that has data that is not a good working point. Note that there two analyses are run in the setup; the combination is run in the test, so that, if needed in other tests, the data could be modified for specific cases. This is a failure test, making sure that bad inputs are not recognised as good working points.\nEven more complex setup\ndef setup_data():\n    # It should be a single dataset, but we do not have one yet, so we loop over existing files\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    q15Res = q15Ana.run_fitting()\n    c1 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_bad_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs_bad = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps_bad = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(\n        d14, freqs_bad, amps_bad\n    )\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(\n        d15, freqs_bad, amps_bad\n    )\n    q15Res = q15Ana.run_fitting()\n    c2 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    dataset_path = (\n        Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp_2.hdf5\"\n    )\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs_2 = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps_2 = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs_2, amps_2)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs_2, amps_2)\n    q15Res = q15Ana.run_fitting()\n    c3 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    list_of_results = [(c1, 0.1), (c2, 0.2), (c3, 0.3)]\n    return list_of_results, freqs, amps, freqs_2, amps_2\nIn this case, three points are loaded and the analysis is run on each of them. The results are returned for each point and can be used in different tests, for example by removing elements in the list_results array.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Unit tests"
    ]
  }
]