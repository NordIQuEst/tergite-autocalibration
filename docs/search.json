[
  {
    "objectID": "unit_tests.html",
    "href": "unit_tests.html",
    "title": "Unit tests for nodes",
    "section": "",
    "text": "The test should be created in a sub-folder of the node called tests.\nPlease organise the file using these sub-folders:\n\ndata: place here any data file that is needed to create test cases, while it is possible to mock the data. Feel free to add a text file explaing how the data was produced. Mocking data is also possible but it may be not practical for complex datasets.\nresults: create this folder to store your results, i.e. files that would be created by your analysis such as the plots. It should be empty, do not commit your results. To assure this is the case, add a file name .gitinore in the folder with this code:\n\n# Ignore everything in this directory\n*\n\n# But do not ignore this file\n!.gitignore     \n\n\n\nA good starting point, especially starting from scratch, it is to test for the class type, then that the inputs have the right formats and then move to some simple operation or trivial case. Build more complex cases from the simpler ones and exploits your tests to refector the code as needed. Try to test as many reasonable cases as possible, both successfull and not. Remeber to test for exceptions. We also suggest to develop using test drive development tecniques that will ensure a high test coverage and a good code structure. Yes, do not forget to keep refactoring to improve the code.\nYou can find some samples below taken from the cz_parametrisation node, where all objects are tested\nCurrently there is no way to differentiate from tests that require a QPU (i.e. measurments) and those that do not (i.e. analyses). Since the latter are simpler to write, start with those that, in general, are more likely to benefit from unit tests as there is much more logic in the analysis than in the measurment.\nIf you need to test some complex scenario, such as those involving sweep, it is probably easier to start writing code and tests from the lowest level and then compose those objects to hendle the more complex scenario considered.\n\n\n\n\n\ndef test_canCreateCorrectType():\n    c = CZ_Parametrisation_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q15\"])\n    assert isinstance(c, CZ_Parametrisation_Fix_Duration_Node)\n    assert isinstance(c, ParametrizedSweepNode)\nThe suggested very first test is to istantiate the class and make sure it has the correct type(s) following any inheritance.\n\n\n\ndef test_CanGetQubitsFromCouplers():\n    c = CZ_Parametrisation_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q15\"])\n    assert c.all_qubits == [\"q14\", \"q15\"]\n    assert c.couplers == ['q14_q15']\nMake sure all inputs and their manipulations are correctly initialised in the constructor, in this case the qubits are taken from the coupler pair\n\n\n\ndef test_ValidationReturnErrorWithSameQubitCoupler():\n    with pytest.raises(ValueError):\n       CZ_Parametrisation_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q14\"])\nInputs can be incorrect and should always be tested to avoid unexpected behaviour down the line which can be difficult to trace back to the origin. There are infinite number of possible errors, so it is impossible to cover them all, but at least the obvious ones should be considered. In this case a typical typing error with the couple have the same qubit twice.\n\n\n\n@pytest.fixture(autouse=True)\ndef setup_good_data():\n    os.environ[\"DATA_DIR\"] = str(Path(__file__).parent / \"results\")\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    return d14, d15, freqs, amps\nThis is an example of how to load data for testing the analysis of a node, please note that the specifics may change as we are about to change the dataset format at the time of writing this; however, the principle will be the same.\nThis data can then be used in multiple tests, for example:\ndef test_canGetMaxFromQ1(setup_good_data):\n    d14, d15, freqs, amps = setup_good_data\n    c = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    result = c.run_fitting()\n    indexBestFreq = np.where(freqs == result[0])[0]\n    indexBestAmp = np.where(amps == result[1])[0]\n    assert indexBestFreq[0] == 9\n    assert indexBestAmp[0] == 13\n\n\ndef test_canGetMinFromQ2(setup_good_data):\n    d14, d15, freqs, amps = setup_good_data\n    c = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    result = c.run_fitting()\n    indexBestFreq = np.where(freqs == result[0])[0]\n    indexBestAmp = np.where(amps == result[1])[0]\n    assert indexBestFreq[0] == 10\n    assert indexBestAmp[0] == 12\nThese two tests make sure that the return values are correct for the two qubits that are connected by the coupler.\n\n\n\ndef test_canPlotBad(setup_bad_data):\n    matplotlib.use(\"Agg\")\n    d14, d15, freqs, amps = setup_bad_data\n    c14 = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    result = c14.run_fitting()\n\n    figure_path = os.environ[\"DATA_DIR\"] + \"/Frequency_Amplitude_bad_q14.png\"\n    # Remove the file if it already exists\n    if os.path.exists(figure_path):\n        os.remove(figure_path)\n\n    fig, ax = plt.subplots(figsize=(15, 7), num=1)\n    plt.Axes\n    c14.plotter(ax)\n    fig.savefig(figure_path)\n    plt.close()\n\n    assert os.path.exists(figure_path)\n    from PIL import Image\n\n    with Image.open(figure_path) as img:\n        assert img.format == \"PNG\", \"File should be a PNG image\"\n\n    c15 = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    result = c15.run_fitting()\n\n    figure_path = os.environ[\"DATA_DIR\"] + \"/Frequency_Amplitude_bad_q15.png\"\n    # Remove the file if it already exists\n    if os.path.exists(figure_path):\n        os.remove(figure_path)\n\n    fig, ax = plt.subplots(figsize=(15, 7), num=1)\n    plt.Axes\n    c15.plotter(ax)\n    fig.savefig(figure_path)\n    plt.close()\n\n    assert os.path.exists(figure_path)\n    from PIL import Image\n\n    with Image.open(figure_path) as img:\n        assert img.format == \"PNG\", \"File should be a PNG image\"\nThis is an example on how to save files in the “results” sub-folder within the “tests” folder. The code make sure the expected file exists and has the correct format. The created files can be inspected by the developer. A possible extension would be to upload the images in the data folder and check the those produced by the test are identical.\n\n\n\n@pytest.fixture(autouse=True)\ndef setup_bad_data():\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_bad_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    q15Res = q15Ana.run_fitting()\n    return q14Res, q15Res\n\n\ndef test_combineBadResultsReturnNoValidPoint(setup_bad_data):\n    q14Res, q15Res = setup_bad_data\n    c = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n    r = c.are_frequencies_compatible()\n    assert r == False\n    r = c.are_amplitudes_compatible()\n    assert r == False\n    r = c.are_two_qubits_compatible()\n    assert r == False\nIn this example, the data produced is loaded from a file that has data that is not a good working point. Note that there two analyses are run in the setup; the combination is run in the test, so that, if needed in other tests, the data could be modified for specific cases. This is a failure test, making sure that bad inputs are not recognised as good working points.\n\n\n\ndef setup_data():\n    # It should be a single dataset, but we do not have one yet, so we loop over existing files\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    q15Res = q15Ana.run_fitting()\n    c1 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_bad_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs_bad = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps_bad = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(\n        d14, freqs_bad, amps_bad\n    )\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(\n        d15, freqs_bad, amps_bad\n    )\n    q15Res = q15Ana.run_fitting()\n    c2 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    dataset_path = (\n        Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp_2.hdf5\"\n    )\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs_2 = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps_2 = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs_2, amps_2)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs_2, amps_2)\n    q15Res = q15Ana.run_fitting()\n    c3 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    list_of_results = [(c1, 0.1), (c2, 0.2), (c3, 0.3)]\n    return list_of_results, freqs, amps, freqs_2, amps_2\nIn this case, three points are loaded and the analysis is run on each of them. The results are returned for each point and can be used in different tests, for example by removing elements in the list_results array."
  },
  {
    "objectID": "unit_tests.html#folder-structure",
    "href": "unit_tests.html#folder-structure",
    "title": "Unit tests for nodes",
    "section": "",
    "text": "The test should be created in a sub-folder of the node called tests.\nPlease organise the file using these sub-folders:\n\ndata: place here any data file that is needed to create test cases, while it is possible to mock the data. Feel free to add a text file explaing how the data was produced. Mocking data is also possible but it may be not practical for complex datasets.\nresults: create this folder to store your results, i.e. files that would be created by your analysis such as the plots. It should be empty, do not commit your results. To assure this is the case, add a file name .gitinore in the folder with this code:\n\n# Ignore everything in this directory\n*\n\n# But do not ignore this file\n!.gitignore"
  },
  {
    "objectID": "unit_tests.html#general-guideline-and-information",
    "href": "unit_tests.html#general-guideline-and-information",
    "title": "Unit tests for nodes",
    "section": "",
    "text": "A good starting point, especially starting from scratch, it is to test for the class type, then that the inputs have the right formats and then move to some simple operation or trivial case. Build more complex cases from the simpler ones and exploits your tests to refector the code as needed. Try to test as many reasonable cases as possible, both successfull and not. Remeber to test for exceptions. We also suggest to develop using test drive development tecniques that will ensure a high test coverage and a good code structure. Yes, do not forget to keep refactoring to improve the code.\nYou can find some samples below taken from the cz_parametrisation node, where all objects are tested\nCurrently there is no way to differentiate from tests that require a QPU (i.e. measurments) and those that do not (i.e. analyses). Since the latter are simpler to write, start with those that, in general, are more likely to benefit from unit tests as there is much more logic in the analysis than in the measurment.\nIf you need to test some complex scenario, such as those involving sweep, it is probably easier to start writing code and tests from the lowest level and then compose those objects to hendle the more complex scenario considered."
  },
  {
    "objectID": "unit_tests.html#example-tests",
    "href": "unit_tests.html#example-tests",
    "title": "Unit tests for nodes",
    "section": "",
    "text": "def test_canCreateCorrectType():\n    c = CZ_Parametrisation_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q15\"])\n    assert isinstance(c, CZ_Parametrisation_Fix_Duration_Node)\n    assert isinstance(c, ParametrizedSweepNode)\nThe suggested very first test is to istantiate the class and make sure it has the correct type(s) following any inheritance.\n\n\n\ndef test_CanGetQubitsFromCouplers():\n    c = CZ_Parametrisation_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q15\"])\n    assert c.all_qubits == [\"q14\", \"q15\"]\n    assert c.couplers == ['q14_q15']\nMake sure all inputs and their manipulations are correctly initialised in the constructor, in this case the qubits are taken from the coupler pair\n\n\n\ndef test_ValidationReturnErrorWithSameQubitCoupler():\n    with pytest.raises(ValueError):\n       CZ_Parametrisation_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers = [\"q14_q14\"])\nInputs can be incorrect and should always be tested to avoid unexpected behaviour down the line which can be difficult to trace back to the origin. There are infinite number of possible errors, so it is impossible to cover them all, but at least the obvious ones should be considered. In this case a typical typing error with the couple have the same qubit twice.\n\n\n\n@pytest.fixture(autouse=True)\ndef setup_good_data():\n    os.environ[\"DATA_DIR\"] = str(Path(__file__).parent / \"results\")\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    return d14, d15, freqs, amps\nThis is an example of how to load data for testing the analysis of a node, please note that the specifics may change as we are about to change the dataset format at the time of writing this; however, the principle will be the same.\nThis data can then be used in multiple tests, for example:\ndef test_canGetMaxFromQ1(setup_good_data):\n    d14, d15, freqs, amps = setup_good_data\n    c = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    result = c.run_fitting()\n    indexBestFreq = np.where(freqs == result[0])[0]\n    indexBestAmp = np.where(amps == result[1])[0]\n    assert indexBestFreq[0] == 9\n    assert indexBestAmp[0] == 13\n\n\ndef test_canGetMinFromQ2(setup_good_data):\n    d14, d15, freqs, amps = setup_good_data\n    c = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    result = c.run_fitting()\n    indexBestFreq = np.where(freqs == result[0])[0]\n    indexBestAmp = np.where(amps == result[1])[0]\n    assert indexBestFreq[0] == 10\n    assert indexBestAmp[0] == 12\nThese two tests make sure that the return values are correct for the two qubits that are connected by the coupler.\n\n\n\ndef test_canPlotBad(setup_bad_data):\n    matplotlib.use(\"Agg\")\n    d14, d15, freqs, amps = setup_bad_data\n    c14 = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    result = c14.run_fitting()\n\n    figure_path = os.environ[\"DATA_DIR\"] + \"/Frequency_Amplitude_bad_q14.png\"\n    # Remove the file if it already exists\n    if os.path.exists(figure_path):\n        os.remove(figure_path)\n\n    fig, ax = plt.subplots(figsize=(15, 7), num=1)\n    plt.Axes\n    c14.plotter(ax)\n    fig.savefig(figure_path)\n    plt.close()\n\n    assert os.path.exists(figure_path)\n    from PIL import Image\n\n    with Image.open(figure_path) as img:\n        assert img.format == \"PNG\", \"File should be a PNG image\"\n\n    c15 = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    result = c15.run_fitting()\n\n    figure_path = os.environ[\"DATA_DIR\"] + \"/Frequency_Amplitude_bad_q15.png\"\n    # Remove the file if it already exists\n    if os.path.exists(figure_path):\n        os.remove(figure_path)\n\n    fig, ax = plt.subplots(figsize=(15, 7), num=1)\n    plt.Axes\n    c15.plotter(ax)\n    fig.savefig(figure_path)\n    plt.close()\n\n    assert os.path.exists(figure_path)\n    from PIL import Image\n\n    with Image.open(figure_path) as img:\n        assert img.format == \"PNG\", \"File should be a PNG image\"\nThis is an example on how to save files in the “results” sub-folder within the “tests” folder. The code make sure the expected file exists and has the correct format. The created files can be inspected by the developer. A possible extension would be to upload the images in the data folder and check the those produced by the test are identical.\n\n\n\n@pytest.fixture(autouse=True)\ndef setup_bad_data():\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_bad_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    q15Res = q15Ana.run_fitting()\n    return q14Res, q15Res\n\n\ndef test_combineBadResultsReturnNoValidPoint(setup_bad_data):\n    q14Res, q15Res = setup_bad_data\n    c = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n    r = c.are_frequencies_compatible()\n    assert r == False\n    r = c.are_amplitudes_compatible()\n    assert r == False\n    r = c.are_two_qubits_compatible()\n    assert r == False\nIn this example, the data produced is loaded from a file that has data that is not a good working point. Note that there two analyses are run in the setup; the combination is run in the test, so that, if needed in other tests, the data could be modified for specific cases. This is a failure test, making sure that bad inputs are not recognised as good working points.\n\n\n\ndef setup_data():\n    # It should be a single dataset, but we do not have one yet, so we loop over existing files\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    q15Res = q15Ana.run_fitting()\n    c1 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_bad_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs_bad = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps_bad = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(\n        d14, freqs_bad, amps_bad\n    )\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(\n        d15, freqs_bad, amps_bad\n    )\n    q15Res = q15Ana.run_fitting()\n    c2 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    dataset_path = (\n        Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp_2.hdf5\"\n    )\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs_2 = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps_2 = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs_2, amps_2)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs_2, amps_2)\n    q15Res = q15Ana.run_fitting()\n    c3 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    list_of_results = [(c1, 0.1), (c2, 0.2), (c3, 0.3)]\n    return list_of_results, freqs, amps, freqs_2, amps_2\nIn this case, three points are loaded and the analysis is run on each of them. The results are returned for each point and can be used in different tests, for example by removing elements in the list_results array."
  },
  {
    "objectID": "new_node_creation.html",
    "href": "new_node_creation.html",
    "title": "To create a new node:",
    "section": "",
    "text": "in the file tergite_autocalibration/lib/node_factory.py expand the dictionary self.node_implementations with a new entry: The key should be a string of the node name and the value should be the object that contains the implementation details. This object should be imported from either tergite_autocalibration/lib/nodes/qubit_control_nodes.py, tergite_autocalibration/lib/nodes/coupler_nodes.py, tergite_autocalibration/lib/nodes/readout_nodes.py or tergite_autocalibration/lib/nodes/characterization_nodes.py\nIn the file tergite_autocalibration/lib/nodes/graph.py in the list graph_dependencies insert the edges that describe the position of the new node in the Directed Acyclic Graph. There are two entries required (or one entry if the new node is the last on its path):\n\n\n('previous_node','new_node')\n('new_node', 'next_node')\n\n\nIn the tergite_autocalibration/config/device_config.toml set the quantity of interest at nan value\n\n\n\nEach node implementation object should contain a reference to the measurement object, the analysis object, the list of redis fields that the analysis updates and the samplespace of the measurement. For example on the Rabi Rabi Oscillations Node:\nclass Rabi_Oscillations_Node(BaseNode):\n    measurement_obj = Rabi_Oscillations\n    analysis_obj = RabiAnalysis\n\n    def __init__(self, name: str, all_qubits: list[str], **node_dictionary):\n        super().__init__(name, all_qubits, **node_dictionary)\n        self.redis_field = ['rxy:amp180']\n        self.schedule_samplespace = {\n            'mw_amplitudes': {\n                qubit: np.linspace(0.002, 0.80, 101) for qubit in self.all_qubits\n            }\n        }\n\n\nThe measurement_obj is imported from tergite_autocalibration/lib/calibration_schedules/ and contains the class that generates the appropriate measurement schedule. To initialize we require a dicttionary of the extended transmons:\ntransmons: dict[str, ExtendedTransmon]\nIt must contain a method called schedule_function that expects the node.samplespace as input and returns the complete schedule.\n\n\n\nThe analysis_obj is imported from tergite_autocalibration/lib/analysis/ and contains the class that perform the analysis for a single qubit. It must contain a run_fitting method and a plotter method\n\n\n\nNodes are divided in two distict categories:\n\nsimple_sweep: where the Quantify Schedule is compiled only once\nparameterized_sweep: where the node requires multiple iterations and each iteration requires a new recompilation.\n\nFurthermore each node can expect two types of samplespaces:\n\nschedule_samplespace: parameter values to be input to the schedule function\nexternal_samplespace: parameter values for quantities that are not set during a schedule\n\n\n\n\nPlease create a new node in a separate folder, so that is is clearer what the new node is meant to do Add an empty init.py file to the folder, this is needed to mark the folder as part of the packege and allow imports from these folders\nTo keep the code clean, please create sub-folders following this scheme:\n\ntests: create unit tests in here, more on tests in href: unit_tests\nutils: any utility class, such as enum, errors and similar classes should be placed here\n\n\n\n\nPlease add your node to the list of available nodes in this Documentation.\nAdd ny relevant information on how to use your node, dependencies and reference to publication as needed for allowing other to use the code you developed.\nDetails on the implementation on the Node types section."
  },
  {
    "objectID": "new_node_creation.html#node-implementation-object",
    "href": "new_node_creation.html#node-implementation-object",
    "title": "To create a new node:",
    "section": "",
    "text": "Each node implementation object should contain a reference to the measurement object, the analysis object, the list of redis fields that the analysis updates and the samplespace of the measurement. For example on the Rabi Rabi Oscillations Node:\nclass Rabi_Oscillations_Node(BaseNode):\n    measurement_obj = Rabi_Oscillations\n    analysis_obj = RabiAnalysis\n\n    def __init__(self, name: str, all_qubits: list[str], **node_dictionary):\n        super().__init__(name, all_qubits, **node_dictionary)\n        self.redis_field = ['rxy:amp180']\n        self.schedule_samplespace = {\n            'mw_amplitudes': {\n                qubit: np.linspace(0.002, 0.80, 101) for qubit in self.all_qubits\n            }\n        }\n\n\nThe measurement_obj is imported from tergite_autocalibration/lib/calibration_schedules/ and contains the class that generates the appropriate measurement schedule. To initialize we require a dicttionary of the extended transmons:\ntransmons: dict[str, ExtendedTransmon]\nIt must contain a method called schedule_function that expects the node.samplespace as input and returns the complete schedule.\n\n\n\nThe analysis_obj is imported from tergite_autocalibration/lib/analysis/ and contains the class that perform the analysis for a single qubit. It must contain a run_fitting method and a plotter method\n\n\n\nNodes are divided in two distict categories:\n\nsimple_sweep: where the Quantify Schedule is compiled only once\nparameterized_sweep: where the node requires multiple iterations and each iteration requires a new recompilation.\n\nFurthermore each node can expect two types of samplespaces:\n\nschedule_samplespace: parameter values to be input to the schedule function\nexternal_samplespace: parameter values for quantities that are not set during a schedule\n\n\n\n\nPlease create a new node in a separate folder, so that is is clearer what the new node is meant to do Add an empty init.py file to the folder, this is needed to mark the folder as part of the packege and allow imports from these folders\nTo keep the code clean, please create sub-folders following this scheme:\n\ntests: create unit tests in here, more on tests in href: unit_tests\nutils: any utility class, such as enum, errors and similar classes should be placed here\n\n\n\n\nPlease add your node to the list of available nodes in this Documentation.\nAdd ny relevant information on how to use your node, dependencies and reference to publication as needed for allowing other to use the code you developed.\nDetails on the implementation on the Node types section."
  },
  {
    "objectID": "available_nodes.html",
    "href": "available_nodes.html",
    "title": "Available Nodes",
    "section": "",
    "text": "punchout\nresonator_spectroscopy\nresonator_spectroscopy_1\nresonator_spectroscopy_2\nro_frequency_two_state_optimization\nro_frequency_three_state_optimization\nro_amplitude_two_state_optimization\nro_amplitude_three_state_optimization\n\n\n\n\n\nqubit_01_spectroscopy\nqubit_01_spectroscopy_pulsed\nrabi_oscillations\nramsey_correction\nqubit_12_spectroscopy_pulsed\nqubit_12_spectroscopy_multidim\nrabi_oscillations_12\nramsey_correction_12\nadaptive_motzoi_parameter\nn_rabi_oscillations\nstate_discrimination\n\n\n\n\n\ncoupler_spectroscopy\ncoupler_resonator_spectroscopy\n\n\n\n\n\nT1\nT2\nT2_echo\nrandomized_benchmarking\nall_XY"
  },
  {
    "objectID": "available_nodes.html#readout-nodes",
    "href": "available_nodes.html#readout-nodes",
    "title": "Available Nodes",
    "section": "",
    "text": "punchout\nresonator_spectroscopy\nresonator_spectroscopy_1\nresonator_spectroscopy_2\nro_frequency_two_state_optimization\nro_frequency_three_state_optimization\nro_amplitude_two_state_optimization\nro_amplitude_three_state_optimization"
  },
  {
    "objectID": "available_nodes.html#qubit-control-nodes",
    "href": "available_nodes.html#qubit-control-nodes",
    "title": "Available Nodes",
    "section": "",
    "text": "qubit_01_spectroscopy\nqubit_01_spectroscopy_pulsed\nrabi_oscillations\nramsey_correction\nqubit_12_spectroscopy_pulsed\nqubit_12_spectroscopy_multidim\nrabi_oscillations_12\nramsey_correction_12\nadaptive_motzoi_parameter\nn_rabi_oscillations\nstate_discrimination"
  },
  {
    "objectID": "available_nodes.html#coupler-nodes",
    "href": "available_nodes.html#coupler-nodes",
    "title": "Available Nodes",
    "section": "",
    "text": "coupler_spectroscopy\ncoupler_resonator_spectroscopy"
  },
  {
    "objectID": "available_nodes.html#characterization-nodes",
    "href": "available_nodes.html#characterization-nodes",
    "title": "Available Nodes",
    "section": "",
    "text": "T1\nT2\nT2_echo\nrandomized_benchmarking\nall_XY"
  },
  {
    "objectID": "operation.html",
    "href": "operation.html",
    "title": "Tergite Automatic Calibration",
    "section": "",
    "text": "The package ships with a command line interface to solve some common tasks that appear quite often.\nIn the following there are a number of useful commands, but if you want to find out all commands use: acli --help\nTo delete all redis entries: acli node reset -a\nTo reset a particular node: acli node reset -n &lt;nodename&gt;\nFor example to reset the node rabi_oscillations run the command:\nacli node reset -n rabi_oscillations\nTo start a new calibration sequence according to the configuration files:\npython tergite_acl/scripts/calibration_supervisor.py\nor\nacli calibration start"
  },
  {
    "objectID": "operation.html#operation",
    "href": "operation.html#operation",
    "title": "Tergite Automatic Calibration",
    "section": "",
    "text": "The package ships with a command line interface to solve some common tasks that appear quite often.\nIn the following there are a number of useful commands, but if you want to find out all commands use: acli --help\nTo delete all redis entries: acli node reset -a\nTo reset a particular node: acli node reset -n &lt;nodename&gt;\nFor example to reset the node rabi_oscillations run the command:\nacli node reset -n rabi_oscillations\nTo start a new calibration sequence according to the configuration files:\npython tergite_acl/scripts/calibration_supervisor.py\nor\nacli calibration start"
  },
  {
    "objectID": "operation.html#structure",
    "href": "operation.html#structure",
    "title": "Tergite Automatic Calibration",
    "section": "Structure",
    "text": "Structure\nFor each calibration node: compilation -&gt; execution -&gt; post-processing -&gt; redis updating"
  },
  {
    "objectID": "operation.html#data-browsing",
    "href": "operation.html#data-browsing",
    "title": "Tergite Automatic Calibration",
    "section": "Data browsing",
    "text": "Data browsing\nDatasets are stored in data_directory Can be browsed with the dataset browser (coming soon)"
  },
  {
    "objectID": "operation.html#development",
    "href": "operation.html#development",
    "title": "Tergite Automatic Calibration",
    "section": "Development",
    "text": "Development\nWhen submitting contributions, please prepend your commit messages with: fix: for bug fixes feat: for introducing a new feature (e.g. a new measurement node or a new analysis class) chore: for refractoring changes or any change that doesn’t affect the functionality of the code docs: for changes in the README, docstrings etc test: or dev: for testing or development changes (e.g. profiling scripts)"
  },
  {
    "objectID": "getting_started.html",
    "href": "getting_started.html",
    "title": "Getting started",
    "section": "",
    "text": "This project contains an orchistration manager, a collection of callibration schedules and a collection of post-processing & analysis routines. It is tailored for the tune-up of the 25 qubits QPU at Chalmers, QTL. This repository utilizes redis for on memory data storage. As redis operates only on Linux systems, this repo can only work\n\neither on Linux distributions\nor WSL (Windows Subsystem for Linux) environments, installed on a Windows system.\n\nTo install WSL, it is required Windows 10 of version at least 1903.\n\n\n\n\n\ngit clone git@github.com:chalmersnextlabs-quantum/tergite-autocalibration.git\n\n\n\nhttps://redis.io/docs/getting-started/installation/install-redis-on-linux/\n\n\n\nredis-server\n\n\n\nIf for example you want to name your environment tac, you create it as\nconda create --name tac python=3.9\n\n\n\nconda activate tac\n\n\n\nsource activate tac\n\n\n\ncd tergite-autocalibration-lite/\nFrom now on, it is assumed that all commands are executed from the project root directory.\n\n\n\npip install -e .\nHere . is the root directory (i.e. the directory that contains the pyproject.toml file)\n\n\n\n\nBefore the first run of the callibration suite a number of configuration files need to be set up. These files describe our initial knowledge on the system as well as the connectivity with the measurement hardware.\n\n\nAll the settings and paths are contained in the .env file. Since this file contains user specific settings, it is git-ignored, it must be created by the user. For convinience a template file .example.env already exists.\nFirst copy the template file to the .env (this creates the .env file if it doesn’t exist):\ncp dot-env-template .env\nThen edit the newly created .env file according to your system. For example here’s how the user lab-user may complete the .env file:\n# Copy this file to a .env file in the tergite-autocalibration folder on the root level.\n# The .env file is a simple list of keys and values. It is also known as the INI file on MS Windows.\n# Fill in the necessary values.\n\n# DEFAULT_PREFIX is added to logfiles, redis entries and in the data directory\n# Default: cal\nDEFAULT_PREFIX=calibration\n\n# Directory settings\n# ROOT_DIR defines the top-level folder of the tergite-autocalibration-lite folder\n# Default: two levels up from the config\nROOT_DIR='/home/lab-user/github/tergite-acl/'\n\n# DATA_DIR defines where plots are stored\nDATA_DIR='/home/lab-user/github/tergite-acl/data_directory/'\n\n# CONFIG_DIR defines where the configuration is stored\nCONFIG_DIR='/home/lab-user/github/tergite-acl/config_dir/'\n\n\n# Configuration settings\n# It is assumed that all these paths are relative to CONFIG_DIR\n# CLUSTER_CONFIG is what Q-BLOX needs to compile schedules on the hardware\n# It should be a file in json format, there is no default file\nCLUSTER_CONFIG='CLUSTER_CONFIGURATION.json'\n\n# DEVICE_CONFIG contains the initial values for the device configuration\nDEVICE_CONFIG='device_config.toml'\n\n# Configuration variables\n# CLUSTER_IP is the IP address of the instrument cluster to connect with\nCLUSTER_IP='162.0.2.162'\n# SPI_SERIAL_PORT is the port on which the spi rack is connected\nSPI_SERIAL_PORT='/dev/ttyACM0'\n\n# APP_SETTINGS reflect which environment the calibration is to run in.\n# Options\n#  - development\n#  - production\n#  - staging\n#  - test\n# Default: production\n# TODO: currently we are only using the calibration in the development mode\nRUN_MODE=development\n\n# REDIS_PORT is the port which to use when connecting to redis\nREDIS_PORT=6379\n# REDIS_CONNECTION will be automatically created in settings.py\n\n# PLOTTING is a boolean to indicate whether plots should be shown or whether plots should be silent in the background\n# Default: True\nPLOTTING=True"
  },
  {
    "objectID": "getting_started.html#installation",
    "href": "getting_started.html#installation",
    "title": "Getting started",
    "section": "",
    "text": "This project contains an orchistration manager, a collection of callibration schedules and a collection of post-processing & analysis routines. It is tailored for the tune-up of the 25 qubits QPU at Chalmers, QTL. This repository utilizes redis for on memory data storage. As redis operates only on Linux systems, this repo can only work\n\neither on Linux distributions\nor WSL (Windows Subsystem for Linux) environments, installed on a Windows system.\n\nTo install WSL, it is required Windows 10 of version at least 1903."
  },
  {
    "objectID": "getting_started.html#repository-installation",
    "href": "getting_started.html#repository-installation",
    "title": "Getting started",
    "section": "",
    "text": "git clone git@github.com:chalmersnextlabs-quantum/tergite-autocalibration.git\n\n\n\nhttps://redis.io/docs/getting-started/installation/install-redis-on-linux/\n\n\n\nredis-server\n\n\n\nIf for example you want to name your environment tac, you create it as\nconda create --name tac python=3.9\n\n\n\nconda activate tac\n\n\n\nsource activate tac\n\n\n\ncd tergite-autocalibration-lite/\nFrom now on, it is assumed that all commands are executed from the project root directory.\n\n\n\npip install -e .\nHere . is the root directory (i.e. the directory that contains the pyproject.toml file)"
  },
  {
    "objectID": "getting_started.html#setting-up-system-configuration-files",
    "href": "getting_started.html#setting-up-system-configuration-files",
    "title": "Getting started",
    "section": "",
    "text": "Before the first run of the callibration suite a number of configuration files need to be set up. These files describe our initial knowledge on the system as well as the connectivity with the measurement hardware.\n\n\nAll the settings and paths are contained in the .env file. Since this file contains user specific settings, it is git-ignored, it must be created by the user. For convinience a template file .example.env already exists.\nFirst copy the template file to the .env (this creates the .env file if it doesn’t exist):\ncp dot-env-template .env\nThen edit the newly created .env file according to your system. For example here’s how the user lab-user may complete the .env file:\n# Copy this file to a .env file in the tergite-autocalibration folder on the root level.\n# The .env file is a simple list of keys and values. It is also known as the INI file on MS Windows.\n# Fill in the necessary values.\n\n# DEFAULT_PREFIX is added to logfiles, redis entries and in the data directory\n# Default: cal\nDEFAULT_PREFIX=calibration\n\n# Directory settings\n# ROOT_DIR defines the top-level folder of the tergite-autocalibration-lite folder\n# Default: two levels up from the config\nROOT_DIR='/home/lab-user/github/tergite-acl/'\n\n# DATA_DIR defines where plots are stored\nDATA_DIR='/home/lab-user/github/tergite-acl/data_directory/'\n\n# CONFIG_DIR defines where the configuration is stored\nCONFIG_DIR='/home/lab-user/github/tergite-acl/config_dir/'\n\n\n# Configuration settings\n# It is assumed that all these paths are relative to CONFIG_DIR\n# CLUSTER_CONFIG is what Q-BLOX needs to compile schedules on the hardware\n# It should be a file in json format, there is no default file\nCLUSTER_CONFIG='CLUSTER_CONFIGURATION.json'\n\n# DEVICE_CONFIG contains the initial values for the device configuration\nDEVICE_CONFIG='device_config.toml'\n\n# Configuration variables\n# CLUSTER_IP is the IP address of the instrument cluster to connect with\nCLUSTER_IP='162.0.2.162'\n# SPI_SERIAL_PORT is the port on which the spi rack is connected\nSPI_SERIAL_PORT='/dev/ttyACM0'\n\n# APP_SETTINGS reflect which environment the calibration is to run in.\n# Options\n#  - development\n#  - production\n#  - staging\n#  - test\n# Default: production\n# TODO: currently we are only using the calibration in the development mode\nRUN_MODE=development\n\n# REDIS_PORT is the port which to use when connecting to redis\nREDIS_PORT=6379\n# REDIS_CONNECTION will be automatically created in settings.py\n\n# PLOTTING is a boolean to indicate whether plots should be shown or whether plots should be silent in the background\n# Default: True\nPLOTTING=True"
  },
  {
    "objectID": "configuration_files.html",
    "href": "configuration_files.html",
    "title": "Configuration",
    "section": "",
    "text": "To run the autocalibration you will need to configure it with a couple of different configuration files. Doing the configuration with a pre-built configuration package will take about 5-10 minutes. On an unknown experimental device without pre-built files, it probably takes slightly longer.\n\n\nComputer systems generally use CAPITALIZED_GLOBAL_KEYS on a system level e.g. to store global variables. There is a convention to store those variables in a .env file on the root-level of your project and load the variables before running a program. The template for the environmental variables of the tergite-autocalibration can be found in the .example.env file on root-level of the repository. E.g. if you have cloned the repository into /home/user/repos/tergite-autocalibration, then your example template should be located there.\nCopy the template and update values according to the instructions. The template file itself provides instructions on how to update the values.\ncp .example.env .env\nValues that can be set in the environment are e.g. PLOTTING and this variable determines whether plots should be shown.\n\n\n\nAll other configuration files go into the folder that you have specified in the environmental variable CONFIG_DIR. A template for a full configuration package can be found in data/_template/_device_name. Some more pre-built configuration package can be found in data/devices under the respective device names.\nA configuration package consists of the following five files:\n\nCluster configuration\nSPI configuration (optional, only required for two-qubit calibration)\nDevice configuration\nCalibration configuration\nCustom user samplespace configuration (optional, only required if you are sweeping on a very specific range of parameters)\n\nAll example configuration files should come with comments about usage and reasonable initial values.\n\n\nA QBLOX cluster consists of a couple of modules of which each can have multiple input/output options for SMI cables. In the cluster configuration the connection is made between these QBLOX cluster physical ports and clocks to the qubits and couplers of the QPU.\nExample: Definition of a qubit drive for |0&gt; to |1&gt; state)\n{\n   \"backend\": \"quantify_scheduler.backends.qblox_backend.hardware_compile\",\n   \"clusterA\": {\n      \"ref\": \"internal\",\n      \"instrument_type\": \"Cluster\",\n      \"clusterA_module2\": {\n         \"instrument_type\": \"QCM_RF\",\n         \"complex_output_0\": {\n            \"lo_freq\": 3.946e9,\n            \"dc_mixer_offset_I\": 0,\n            \"dc_mixer_offset_Q\": 0,\n            \"portclock_configs\": [\n               {\n                  \"port\": \"q00:mw\",\n                  \"clock\": \"q00.01\",\n                  \"mixer_amp_ratio\": 1,\n                  \"mixer_phase_error_deg\": 0\n               }\n            ]\n         }\n      }\n   }\n}\nThe file in the template package is cluster_configuration.json.\nFIXME: This is to be changed after with the integration of the automatic mixer calibration The cluster configuration relies on some values that come from a mixer calibration. Given a .csv file after a mixer calibration the function utils/hw_generator.py can create instantly the corresponding JSON file.\n\n\n\nWhen working with two-qubit gates, there has to be a current source for the coupler and in the QBLOX stack this is coming from the so called SPI rack. The SPI configuration is mapping the qubits to their respective modules in the SPI rack and can be further used to assign the couplers to groups.\nExample: Definition of a coupler\n[couplers.q11_q12]\nspi_module_no = 1\ndac_name = \"dac0\"\nedge_group = 1\nThe file in the template package is spi_config.toml.\n\n\n\nWhile the previous two configuration files have been used to configure the room temperature instruments, the device configuration defines the initial parameters and characteristics of chip itself. The device configuration is having two main sections – the [device] and the [layout] section. In the [device] section prior knowledge about the device from the VNA are set for the resonator, qubit (drive) and the coupler.\nIt is possible to either address a qubit individually, e.g. the following would set the VNA frequency for qubit q06:\n[device.resonator.q06]\nVNA_frequency = 6832973301.189378\nor for all qubits:\n[device.resonator.all]\nattenuation = 12\nIn the [layout] section the positions of the qubits can be set. This is useful if one would like to e.g. plot the device. Qubits have an x (column) and a y (row) position:\n[layout.resonator.q06]\nposition = { column = 0, row = 0 }\nThe file in the template package is device_config.toml.\n\n\n\nThis file is solely configuring the calibration process itself i.e. everything that is related to running the calibration, its measurements and analyses. In the [general] section there are some settings such as the target node, the qubits and the couplers to calibrate.\nExample: Calibrate qubits q01 and q02 with coupler q01_q02 up until the node cz_calibration.\n[general]\ntarget_node = \"cz_calibration\"\nqubits = [\"q01\", \"q02\"]\ncouplers = [\"q01_q02\"]\nThis is followed by the [initials] section where you can set the initial values for certain calibration nodes. Here, similarly to the device configuration, you can either address a single qubit or all qubits, by using either the qubit identifier e.g. [q06] or the [all] keyword.\nExample: Setting initial values.\n[initials.qubits.all]\nmeasure.acq_delay = 220e-9\n[initials.qubits]\nq06.measure.pulse_amp = 0.03\nBelow, you can define node-specific parameters setting [node_name.scope.property] where scope are the qubits/couplers and the the property is a property known to the node.\nExample: Setting the reset duration for the resonator spectroscopy node.\n[resonator_spectroscopy.all]\nreset.duration =  60e-6\nThe file in the template package is calibration_config.toml.\n\n\n\nIf you want to generate samplespaces with your own custom Python scripts, you can add a custom user samplespace configuration. The file must contain the definition of your samplespace according to the following schema:\nuser_samplespace = {\n    node1_name : {\n            \"settable_of_node1_1\": { 'q01': np.ndarray, 'q02': np.ndarray },\n            \"settable_of_node1_2\": { 'q01': np.ndarray, 'q02': np.ndarray },\n            ...\n        },\n    node2_name : {\n            \"settable_of_node2_1\": { 'q01': np.ndarray, 'q02': np.ndarray },\n            \"settable_of_node2_2\": { 'q01': np.ndarray, 'q02': np.ndarray },\n            ...\n        }\n}\nPlease note: Do not rename the variable user_samplespace, because it cannot be imported otherwise.\nThe file in the template package is user_samplespace.py."
  },
  {
    "objectID": "configuration_files.html#environment-variables",
    "href": "configuration_files.html#environment-variables",
    "title": "Configuration",
    "section": "",
    "text": "Computer systems generally use CAPITALIZED_GLOBAL_KEYS on a system level e.g. to store global variables. There is a convention to store those variables in a .env file on the root-level of your project and load the variables before running a program. The template for the environmental variables of the tergite-autocalibration can be found in the .example.env file on root-level of the repository. E.g. if you have cloned the repository into /home/user/repos/tergite-autocalibration, then your example template should be located there.\nCopy the template and update values according to the instructions. The template file itself provides instructions on how to update the values.\ncp .example.env .env\nValues that can be set in the environment are e.g. PLOTTING and this variable determines whether plots should be shown."
  },
  {
    "objectID": "configuration_files.html#configuration-package",
    "href": "configuration_files.html#configuration-package",
    "title": "Configuration",
    "section": "",
    "text": "All other configuration files go into the folder that you have specified in the environmental variable CONFIG_DIR. A template for a full configuration package can be found in data/_template/_device_name. Some more pre-built configuration package can be found in data/devices under the respective device names.\nA configuration package consists of the following five files:\n\nCluster configuration\nSPI configuration (optional, only required for two-qubit calibration)\nDevice configuration\nCalibration configuration\nCustom user samplespace configuration (optional, only required if you are sweeping on a very specific range of parameters)\n\nAll example configuration files should come with comments about usage and reasonable initial values.\n\n\nA QBLOX cluster consists of a couple of modules of which each can have multiple input/output options for SMI cables. In the cluster configuration the connection is made between these QBLOX cluster physical ports and clocks to the qubits and couplers of the QPU.\nExample: Definition of a qubit drive for |0&gt; to |1&gt; state)\n{\n   \"backend\": \"quantify_scheduler.backends.qblox_backend.hardware_compile\",\n   \"clusterA\": {\n      \"ref\": \"internal\",\n      \"instrument_type\": \"Cluster\",\n      \"clusterA_module2\": {\n         \"instrument_type\": \"QCM_RF\",\n         \"complex_output_0\": {\n            \"lo_freq\": 3.946e9,\n            \"dc_mixer_offset_I\": 0,\n            \"dc_mixer_offset_Q\": 0,\n            \"portclock_configs\": [\n               {\n                  \"port\": \"q00:mw\",\n                  \"clock\": \"q00.01\",\n                  \"mixer_amp_ratio\": 1,\n                  \"mixer_phase_error_deg\": 0\n               }\n            ]\n         }\n      }\n   }\n}\nThe file in the template package is cluster_configuration.json.\nFIXME: This is to be changed after with the integration of the automatic mixer calibration The cluster configuration relies on some values that come from a mixer calibration. Given a .csv file after a mixer calibration the function utils/hw_generator.py can create instantly the corresponding JSON file.\n\n\n\nWhen working with two-qubit gates, there has to be a current source for the coupler and in the QBLOX stack this is coming from the so called SPI rack. The SPI configuration is mapping the qubits to their respective modules in the SPI rack and can be further used to assign the couplers to groups.\nExample: Definition of a coupler\n[couplers.q11_q12]\nspi_module_no = 1\ndac_name = \"dac0\"\nedge_group = 1\nThe file in the template package is spi_config.toml.\n\n\n\nWhile the previous two configuration files have been used to configure the room temperature instruments, the device configuration defines the initial parameters and characteristics of chip itself. The device configuration is having two main sections – the [device] and the [layout] section. In the [device] section prior knowledge about the device from the VNA are set for the resonator, qubit (drive) and the coupler.\nIt is possible to either address a qubit individually, e.g. the following would set the VNA frequency for qubit q06:\n[device.resonator.q06]\nVNA_frequency = 6832973301.189378\nor for all qubits:\n[device.resonator.all]\nattenuation = 12\nIn the [layout] section the positions of the qubits can be set. This is useful if one would like to e.g. plot the device. Qubits have an x (column) and a y (row) position:\n[layout.resonator.q06]\nposition = { column = 0, row = 0 }\nThe file in the template package is device_config.toml.\n\n\n\nThis file is solely configuring the calibration process itself i.e. everything that is related to running the calibration, its measurements and analyses. In the [general] section there are some settings such as the target node, the qubits and the couplers to calibrate.\nExample: Calibrate qubits q01 and q02 with coupler q01_q02 up until the node cz_calibration.\n[general]\ntarget_node = \"cz_calibration\"\nqubits = [\"q01\", \"q02\"]\ncouplers = [\"q01_q02\"]\nThis is followed by the [initials] section where you can set the initial values for certain calibration nodes. Here, similarly to the device configuration, you can either address a single qubit or all qubits, by using either the qubit identifier e.g. [q06] or the [all] keyword.\nExample: Setting initial values.\n[initials.qubits.all]\nmeasure.acq_delay = 220e-9\n[initials.qubits]\nq06.measure.pulse_amp = 0.03\nBelow, you can define node-specific parameters setting [node_name.scope.property] where scope are the qubits/couplers and the the property is a property known to the node.\nExample: Setting the reset duration for the resonator spectroscopy node.\n[resonator_spectroscopy.all]\nreset.duration =  60e-6\nThe file in the template package is calibration_config.toml.\n\n\n\nIf you want to generate samplespaces with your own custom Python scripts, you can add a custom user samplespace configuration. The file must contain the definition of your samplespace according to the following schema:\nuser_samplespace = {\n    node1_name : {\n            \"settable_of_node1_1\": { 'q01': np.ndarray, 'q02': np.ndarray },\n            \"settable_of_node1_2\": { 'q01': np.ndarray, 'q02': np.ndarray },\n            ...\n        },\n    node2_name : {\n            \"settable_of_node2_1\": { 'q01': np.ndarray, 'q02': np.ndarray },\n            \"settable_of_node2_2\": { 'q01': np.ndarray, 'q02': np.ndarray },\n            ...\n        }\n}\nPlease note: Do not rename the variable user_samplespace, because it cannot be imported otherwise.\nThe file in the template package is user_samplespace.py."
  },
  {
    "objectID": "developer_guide.html",
    "href": "developer_guide.html",
    "title": "Introduction",
    "section": "",
    "text": "This and the following sections provide information on how to develop code in tergite-autocalibration.\n\n\nConsider installing Quarto and other packages before you create your coda environemtn to have the path correctly initialised in your environment. This is not necessary but it can simplify operations later, especially using VSCode.\nAfter you install tergite with\npip install -e .\nrun\npip install poetry\npoetry install --with dev,test\nthis will install the additional packages for developing code and running tests\n\n\n\nWe use American English, please set any spell checker to this language.\n\nThe file names should be written using snake_case, with words written in lowercase and separated by underscores.\nClass names should be in PascalCase (where all words are capitalized). Many class do not follow this rule and use CamelCase with underscore, they will be changed.\nMethods should be in snake_case\nVariables should be in snake_case\n\n\n\n\nThere are settings available in the repo for IDEs, recommending extensions and settings. Please discuss with the team before modifying these default settings, they should changed only with the consensus of the team.\n\n\nBlack, quarto and python extensions are recommended. Some settings are recommended too.\n\n\n\n\n\n\nWhen you create or modify a file, make sure that add the following copyright text is present at the top of the file. Remember to add your name and do not delete previus contributors. If you add the statement to a file that does not have one, please check on git the names of contributors.\n# This code is part of Tergite Autocalibration\n#\n# (C) Copyright WRITE YOUR NAME HERE, 2024\n#\n# This code is licensed under the Apache License, Version 2.0. You may\n# obtain a copy of this license in the LICENSE.txt file in the root directory\n# of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.\n#\n# Any modifications or derivative works of this code must retain this\n# copyright notice, and modified files need to carry a notice indicating\n# that they have been altered from the originals.\n\n\n\nUpdate the changelog in the sectionc called [Unreleased]. Please note that there are several sections titled “Added”, “Change” and “Fixed”; place your text in the correct category.\n\n\n\nThe code analyzer used in the project is black, which is installed as part of the dev dependencies. To use black, open a shell and run\nblack .\nPlease make sure to run it before merging the PR."
  },
  {
    "objectID": "developer_guide.html#additional-installations",
    "href": "developer_guide.html#additional-installations",
    "title": "Introduction",
    "section": "",
    "text": "Consider installing Quarto and other packages before you create your coda environemtn to have the path correctly initialised in your environment. This is not necessary but it can simplify operations later, especially using VSCode.\nAfter you install tergite with\npip install -e .\nrun\npip install poetry\npoetry install --with dev,test\nthis will install the additional packages for developing code and running tests"
  },
  {
    "objectID": "developer_guide.html#naming-convention-and-style",
    "href": "developer_guide.html#naming-convention-and-style",
    "title": "Introduction",
    "section": "",
    "text": "We use American English, please set any spell checker to this language.\n\nThe file names should be written using snake_case, with words written in lowercase and separated by underscores.\nClass names should be in PascalCase (where all words are capitalized). Many class do not follow this rule and use CamelCase with underscore, they will be changed.\nMethods should be in snake_case\nVariables should be in snake_case"
  },
  {
    "objectID": "developer_guide.html#ide-preloaded-settings",
    "href": "developer_guide.html#ide-preloaded-settings",
    "title": "Introduction",
    "section": "",
    "text": "There are settings available in the repo for IDEs, recommending extensions and settings. Please discuss with the team before modifying these default settings, they should changed only with the consensus of the team.\n\n\nBlack, quarto and python extensions are recommended. Some settings are recommended too."
  },
  {
    "objectID": "developer_guide.html#things-to-do-before-a-commit",
    "href": "developer_guide.html#things-to-do-before-a-commit",
    "title": "Introduction",
    "section": "",
    "text": "When you create or modify a file, make sure that add the following copyright text is present at the top of the file. Remember to add your name and do not delete previus contributors. If you add the statement to a file that does not have one, please check on git the names of contributors.\n# This code is part of Tergite Autocalibration\n#\n# (C) Copyright WRITE YOUR NAME HERE, 2024\n#\n# This code is licensed under the Apache License, Version 2.0. You may\n# obtain a copy of this license in the LICENSE.txt file in the root directory\n# of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.\n#\n# Any modifications or derivative works of this code must retain this\n# copyright notice, and modified files need to carry a notice indicating\n# that they have been altered from the originals.\n\n\n\nUpdate the changelog in the sectionc called [Unreleased]. Please note that there are several sections titled “Added”, “Change” and “Fixed”; place your text in the correct category.\n\n\n\nThe code analyzer used in the project is black, which is installed as part of the dev dependencies. To use black, open a shell and run\nblack .\nPlease make sure to run it before merging the PR."
  },
  {
    "objectID": "node_types.html",
    "href": "node_types.html",
    "title": "Node types",
    "section": "",
    "text": "Node types\nThe execution of most of the nodes consists of a single schedule compilation, a single measurement and a single post-processing. Although for most of the nodes this workflow suffices, there are exceptions while this workflow can become limiting in more advanced implementations.\nTo allow greater flexibilty in the node implementations the nodes are categorized:\n\nAccording to whether they compile once or multiple times:\n\nnode.type = simple_sweep: if it compiles once\nnode.type = parameterized_sweep: if it compiles multiple times\n\nAccording to whether the sweeping parameters are swept whithin the schedule or not:\n\nThere is only node.schedule_samplespace if the sweeping takes place whithin the schedule\nThere are both node.schedule_samplespace and node.external_samplespace if there are sweeping parameters outside of the scedule. For example the coupler_spectroscopy node sweeps the dc_current outside of the schedule:\n\n\n  class Coupler_Spectroscopy_Node(BaseNode):\n    measurement_obj = Two_Tones_Multidim\n    analysis_obj = CouplerSpectroscopyAnalysis\n\n    def __init__(self, name: str, all_qubits: list[str], couplers, **schedule_keywords):\n        super().__init__(name, all_qubits, **schedule_keywords)\n        self.couplers = couplers\n        self.redis_field = ['parking_current']\n        self.all_qubits = self.coupled_qubits\n\n        self.schedule_samplespace = {\n            'spec_frequencies': {\n                qubit: qubit_samples(qubit) for qubit in self.all_qubits\n            }\n        }\n\n        self.external_samplespace = {\n            'dc_currents': {\n                self.coupler: np.arange(-2.5e-3, 2.5e-3, 500e-6)\n            },\n        }\n\n    def pre_measurement_operation(self, reduced_ext_space):\n        iteration_dict = reduced_ext_space['dc_currents']\n\n        this_iteration_value = list(iteration_dict.values())[0]\n        print(f'{ this_iteration_value = }')\n        self.spi_dac.set_dac_current(self.dac, this_iteration_value)\nBy default every node every node is assigned a node.type attribute at the BaseNode class:\nself.type = simple_sweep\nThis attribute can be overwritten at the implementation of the class of each node. An example of a parameterized_sweep node type is Randomized_Benchmarking as each new iteration requires a the schedule to be recompiled with a different random seed.\nThe tergite_acl/scripts/node_supervisor.py is responcible to distinguish between each node variation.\n\nExamples of nodes requiring an external samplespace:\n\ncoupler_spectroscopy sweeps the dc_current which is set by the SPI rack not the cluster\nT1 sweeps a repetetion index to repeat the measurement many times\nrandomized_benchmarking sweeps different seeds. Although the seed is a schedule parameter, sweeping outside the schedule improves memory utilization."
  }
]